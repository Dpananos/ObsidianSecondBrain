[
  {"id":"abadieWhenShouldYou2022","abstract":"In empirical work it is common to estimate parameters of models and report associated standard errors that account for \"clustering\" of units, where clusters are defined by factors such as geography. Clustering adjustments are typically motivated by the concern that unobserved components of outcomes for units within clusters are correlated. However, this motivation does not provide guidance about questions such as: (i) Why should we adjust standard errors for clustering in some situations but not others? How can we justify the common practice of clustering in observational studies but not randomized experiments, or clustering by state but not by gender? (ii) Why is conventional clustering a potentially conservative \"all-or-nothing\" adjustment, and are there alternative methods that respond to data and are less conservative? (iii) In what settings does the choice of whether and how to cluster make a difference? We address these questions using a framework of sampling and design inference. We argue that clustering can be needed to address sampling issues if sampling follows a two stage process where in the first stage, a subset of clusters are sampled from a population of clusters, and in the second stage, units are sampled from the sampled clusters. Then, clustered standard errors account for the existence of clusters in the population that we do not see in the sample. Clustering can be needed to account for design issues if treatment assignment is correlated with membership in a cluster. We propose new variance estimators to deal with intermediate settings where conventional cluster standard errors are unnecessarily conservative and robust standard errors are too small.","accessed":{"date-parts":[["2025",10,16]]},"author":[{"family":"Abadie","given":"Alberto"},{"family":"Athey","given":"Susan"},{"family":"Imbens","given":"Guido"},{"family":"Wooldridge","given":"Jeffrey"}],"citation-key":"abadieWhenShouldYou2022","DOI":"10.48550/arXiv.1710.02926","issued":{"date-parts":[["2022",9,20]]},"number":"arXiv:1710.02926","publisher":"arXiv","source":"arXiv.org","title":"When Should You Adjust Standard Errors for Clustering?","type":"article","URL":"http://arxiv.org/abs/1710.02926"},
  {"id":"arkhangelskySyntheticDifferenceinDifferences2021","abstract":"We present a new estimator for causal effects with panel data that builds on insights behind the widely used difference-in-differences and synthetic control methods. Relative to these methods we find, both theoretically and empirically, that this \"synthetic difference-in-differences\" estimator has desirable robustness properties, and that it performs well in settings where the conventional estimators are commonly used in practice. We study the asymptotic behavior of the estimator when the systematic part of the outcome model includes latent unit factors interacted with latent time factors, and we present conditions for consistency and asymptotic normality.","accessed":{"date-parts":[["2024",6,28]]},"author":[{"family":"Arkhangelsky","given":"Dmitry"},{"family":"Athey","given":"Susan"},{"family":"Hirshberg","given":"David A."},{"family":"Imbens","given":"Guido W."},{"family":"Wager","given":"Stefan"}],"citation-key":"arkhangelskySyntheticDifferenceinDifferences2021","container-title":"American Economic Review","DOI":"10.1257/aer.20190159","ISSN":"0002-8282","issue":"12","issued":{"date-parts":[["2021",12]]},"language":"en","page":"4088-4118","source":"www.aeaweb.org","title":"Synthetic Difference-in-Differences","type":"article-journal","URL":"https://www.aeaweb.org/articles?id=10.1257/aer.20190159","volume":"111"},
  {"id":"armeroTwoStageBayesianApproach2019","abstract":"Genome-wide association studies (GWAS) aim to assess relationships between single nucleotide polymorphisms (SNPs) and diseases. They are one of the most popular problems in genetics, and have some peculiarities given the large number of SNPs compared to the number of subjects in the study. Individuals might not be independent, especially in animal breeding studies or genetic diseases in isolated populations with highly inbred individuals. We propose a family-based GWAS model in a two-stage approach comprising a dimension reduction and a subsequent model selection. The first stage, in which the genetic relatedness between the subjects is taken into account, selects the promising SNPs. The second stage uses Bayes factors for comparison among all candidate models and a random search strategy for exploring the space of all the regression models in a fully Bayesian approach. A simulation study shows that our approach is superior to Bayesian lasso for model selection in this setting. We also illustrate its performance in a study on Beta-thalassemia disorder in an isolated population from Sardinia. Supplementary Material describing the implementation of the method proposed in this article is available online.","accessed":{"date-parts":[["2022",5,28]]},"author":[{"family":"Armero","given":"Carmen"},{"family":"Cabras","given":"Stefano"},{"family":"Castellanos","given":"María Eugenia"},{"family":"Quirós","given":"Alicia"}],"citation-key":"armeroTwoStageBayesianApproach2019","container-title":"Journal of Computational and Graphical Statistics","DOI":"10.1080/10618600.2018.1483828","ISSN":"1061-8600","issue":"1","issued":{"date-parts":[["2019",1,2]]},"page":"197-204","publisher":"Taylor & Francis","source":"Taylor and Francis+NEJM","title":"Two-Stage Bayesian Approach for GWAS With Known Genealogy","type":"article-journal","URL":"https://doi.org/10.1080/10618600.2018.1483828","volume":"28"},
  {"id":"aronowNonparametricIdentificationNot2021","abstract":"We argue that randomized controlled trials (RCTs) are special even among settings where average treatment effects are identified by a nonparametric unconfoundedness assumption. This claim follows from two results of Robins and Ritov (1997): (1) with at least one continuous covariate control, no estimator of the average treatment effect exists which is uniformly consistent without further assumptions, (2) knowledge of the propensity score yields a uniformly consistent estimator and honest confidence intervals that shrink at parametric rates with increasing sample size, regardless of how complicated the propensity score function is. We emphasize the latter point, and note that successfully-conducted RCTs provide knowledge of the propensity score to the researcher. We discuss modern developments in covariate adjustment for RCTs, noting that statistical models and machine learning methods can be used to improve efficiency while preserving finite sample unbiasedness. We conclude that statistical inference has the potential to be fundamentally more difficult in observational settings than it is in RCTs, even when all confounders are measured.","accessed":{"date-parts":[["2025",2,27]]},"author":[{"family":"Aronow","given":"P. M."},{"family":"Robins","given":"James M."},{"family":"Saarinen","given":"Theo"},{"family":"Sävje","given":"Fredrik"},{"family":"Sekhon","given":"Jasjeet"}],"citation-key":"aronowNonparametricIdentificationNot2021","DOI":"10.48550/arXiv.2108.11342","issued":{"date-parts":[["2021",9,27]]},"number":"arXiv:2108.11342","publisher":"arXiv","source":"arXiv.org","title":"Nonparametric identification is not enough, but randomized controlled trials are","type":"article","URL":"http://arxiv.org/abs/2108.11342"},
  {"id":"atheyEstimatingTreatmentEffects2020","abstract":"Estimating the long-term effects of treatments is of interest in many fields. A common challenge in estimating such treatment effects is that long-term outcomes are unobserved in the time frame needed to make policy decisions. One approach to overcome this missing data problem is to analyze treatments effects on an intermediate outcome, often called a statistical surrogate, if it satisfies the condition that treatment and outcome are independent conditional on the statistical surrogate. The validity of the surrogacy condition is often controversial. Here we exploit that fact that in modern datasets, researchers often observe a large number, possibly hundreds or thousands, of intermediate outcomes, thought to lie on or close to the causal chain between the treatment and the long-term outcome of interest. Even if none of the individual proxies satisfies the statistical surrogacy criterion by itself, using multiple proxies can be useful in causal inference. We focus primarily on a setting with two samples, an experimental sample containing data about the treatment indicator and the surrogates and an observational sample containing information about the surrogates and the primary outcome. We state assumptions under which the average treatment effect be identified and estimated with a high-dimensional vector of proxies that collectively satisfy the surrogacy assumption, and derive the bias from violations of the surrogacy assumption, and show that even if the primary outcome is also observed in the experimental sample, there is still information to be gained from using surrogates.","accessed":{"date-parts":[["2022",6,13]]},"author":[{"family":"Athey","given":"Susan"},{"family":"Chetty","given":"Raj"},{"family":"Imbens","given":"Guido"},{"family":"Kang","given":"Hyunseung"}],"citation-key":"atheyEstimatingTreatmentEffects2020","DOI":"10.48550/arXiv.1603.09326","issued":{"date-parts":[["2020",2,29]]},"number":"arXiv:1603.09326","publisher":"arXiv","source":"arXiv.org","title":"Estimating Treatment Effects using Multiple Surrogates: The Role of the Surrogate Score and the Surrogate Index","title-short":"Estimating Treatment Effects using Multiple Surrogates","type":"article","URL":"http://arxiv.org/abs/1603.09326"},
  {"id":"atheySemiparametricEstimationTreatment2021","abstract":"We develop new semiparametric methods for estimating treatment effects. We focus on a setting where the outcome distributions may be thick tailed, where treatment effects are small, where sample sizes are large and where assignment is completely random. This setting is of particular interest in recent experimentation in tech companies. We propose using parametric models for the treatment effects, as opposed to parametric models for the full outcome distributions. This leads to semiparametric models for the outcome distributions. We derive the semiparametric efficiency bound for this setting, and propose efficient estimators. In the case with a constant treatment effect one of the proposed estimators has an interesting interpretation as a weighted average of quantile treatment effects, with the weights proportional to (minus) the second derivative of the log of the density of the potential outcomes. Our analysis also results in an extension of Huber's model and trimmed mean to include asymmetry and a simplified condition on linear combinations of order statistics, which may be of independent interest.","accessed":{"date-parts":[["2022",5,20]]},"author":[{"family":"Athey","given":"Susan"},{"family":"Bickel","given":"Peter J."},{"family":"Chen","given":"Aiyou"},{"family":"Imbens","given":"Guido W."},{"family":"Pollmann","given":"Michael"}],"citation-key":"atheySemiparametricEstimationTreatment2021","DOI":"10.48550/arXiv.2109.02603","issued":{"date-parts":[["2021",9,6]]},"note":"https://twitter.com/AaronMcDaid/status/1442233632186413061","number":"arXiv:2109.02603","publisher":"arXiv","source":"arXiv.org","title":"Semiparametric Estimation of Treatment Effects in Randomized Experiments","type":"article","URL":"http://arxiv.org/abs/2109.02603"},
  {"id":"atheySemiparametricEstimationTreatment2021a","abstract":"We develop new semiparametric methods for estimating treatment effects. We focus on a setting where the outcome distributions may be thick tailed, where treatment effects are small, where sample sizes are large and where assignment is completely random. This setting is of particular interest in recent experimentation in tech companies. We propose using parametric models for the treatment effects, as opposed to parametric models for the full outcome distributions. This leads to semiparametric models for the outcome distributions. We derive the semiparametric efﬁciency bound for this setting, and propose efﬁcient estimators. In the case with a constant treatment effect one of the proposed estimators has an interesting interpretation as a weighted average of quantile treatment effects, with the weights proportional to (minus) the second derivative of the log of the density of the potential outcomes. Our analysis also results in an extension of Huber’s model and trimmed mean to include asymmetry and a simpliﬁed condition on linear combinations of order statistics, which may be of independent interest.","accessed":{"date-parts":[["2022",5,20]]},"author":[{"family":"Athey","given":"Susan"},{"family":"Bickel","given":"Peter J."},{"family":"Chen","given":"Aiyou"},{"family":"Imbens","given":"Guido W."},{"family":"Pollmann","given":"Michael"}],"citation-key":"atheySemiparametricEstimationTreatment2021a","issued":{"date-parts":[["2021",9,6]]},"language":"en","number":"arXiv:2109.02603","publisher":"arXiv","source":"arXiv.org","title":"Semiparametric Estimation of Treatment Effects in Randomized Experiments","type":"article","URL":"http://arxiv.org/abs/2109.02603"},
  {"id":"AugmentationDecompositionNew","accessed":{"date-parts":[["2024",5,3]]},"citation-key":"AugmentationDecompositionNew","title":"From Augmentation to Decomposition: A New Look at CUPED in 2023","type":"webpage","URL":"https://arxiv.org/html/2312.02935v1"},
  {"id":"baissaWhenBLUENot2020","abstract":"Researchers in political science often estimate linear models of continuous outcomes using least squares. While it is well known that least-squares estimates are sensitive to single, unusual data points, this knowledge has not led to careful practices when using least-squares estimators. Using statistical theory and Monte Carlo simulations, we highlight the importance of using more robust estimators along with variable transformations. We also discuss several approaches to detect, summarize, and communicate the influence of particular data points.","accessed":{"date-parts":[["2023",10,3]]},"author":[{"family":"Baissa","given":"Daniel K."},{"family":"Rainey","given":"Carlisle"}],"citation-key":"baissaWhenBLUENot2020","container-title":"Political Science Research and Methods","DOI":"10.1017/psrm.2018.34","ISSN":"2049-8470, 2049-8489","issue":"1","issued":{"date-parts":[["2020",1]]},"language":"en","page":"136-148","publisher":"Cambridge University Press","source":"Cambridge University Press","title":"When BLUE is not best: non-normal errors and the linear model","title-short":"When BLUE is not best","type":"article-journal","URL":"https://www.cambridge.org/core/journals/political-science-research-and-methods/article/when-blue-is-not-best-nonnormal-errors-and-the-linear-model/199AE23A2FDBFF66FFC1A1F450705E9B","volume":"8"},
  {"id":"BASICPHARMACOKINETICSPHARMACODYNAMICS","citation-key":"BASICPHARMACOKINETICSPHARMACODYNAMICS","language":"en","page":"588","source":"Zotero","title":"BASIC PHARMACOKINETICS AND PHARMACODYNAMICS","type":"article-journal"},
  {"id":"bauerNONMEMTutorialPart2019","abstract":"In this second tutorial on NONMEM, the examples of typical pharmacokinetic/pharmacodynamic modeling problems that occur in the pharmaceutical field will be presented, which the reader can use as a template for his or her own modeling endeavors. Each of the problems presented is challenging in some way, and the logic behind setting up each problem is discussed. Logical concepts of the problem itself as well as the technical aspect of how to set it up in NONMEM are described and demonstrated. The concepts behind the various estimation algorithms will first be described to allow the user a better understanding of how to use them.","accessed":{"date-parts":[["2022",5,18]]},"author":[{"family":"Bauer","given":"Robert J."}],"citation-key":"bauerNONMEMTutorialPart2019","container-title":"CPT: Pharmacometrics & Systems Pharmacology","DOI":"10.1002/psp4.12422","ISSN":"2163-8306","issue":"8","issued":{"date-parts":[["2019"]]},"language":"en","page":"538-556","source":"Wiley Online Library","title":"NONMEM Tutorial Part II: Estimation Methods and Advanced Examples","title-short":"NONMEM Tutorial Part II","type":"article-journal","URL":"https://onlinelibrary.wiley.com/doi/abs/10.1002/psp4.12422","volume":"8"},
  {"id":"bermanFalseDiscoveryTesting2022","abstract":"We investigate what fraction of all significant results in website A/B testing is actually null effects (i.e., the false discovery rate (FDR)). Our data consist of 4,964 effects from 2,766 experiments conducted on a commercial A/B testing platform. Using three different methods, we find that the FDR ranges between 28% and 37% for tests conducted at 10% significance and between 18% and 25% for tests at 5% significance (two sided). These high FDRs stem mostly from the high fraction of true null effects, about 70%, rather than from low power. Using our estimates, we also assess the potential of various A/B test designs to reduce the FDR. The two main implications are that decision makers should expect one in five interventions achieving significance at 5% confidence to be ineffective when deployed in the field and that analysts should consider using two-stage designs with multiple variations rather than basic A/B tests.\n\nThis paper was accepted by Eric Anderson, marketing.","accessed":{"date-parts":[["2022",10,12]]},"author":[{"family":"Berman","given":"Ron"},{"family":"Van den Bulte","given":"Christophe"}],"citation-key":"bermanFalseDiscoveryTesting2022","container-title":"Management Science","DOI":"10.1287/mnsc.2021.4207","ISSN":"0025-1909","issue":"9","issued":{"date-parts":[["2022",9]]},"page":"6762-6782","publisher":"INFORMS","source":"pubsonline.informs.org (Atypon)","title":"False Discovery in A/B Testing","type":"article-journal","URL":"https://pubsonline.informs.org/doi/10.1287/mnsc.2021.4207","volume":"68"},
  {"id":"betancourtConceptualIntroductionHamiltonian2018","abstract":"Hamiltonian Monte Carlo has proven a remarkable empirical success, but only recently have we begun to develop a rigorous understanding of why it performs so well on difficult problems and how it is best applied in practice. Unfortunately, that understanding is confined within the mathematics of differential geometry which has limited its dissemination, especially to the applied communities for which it is particularly important. In this review I provide a comprehensive conceptual account of these theoretical foundations, focusing on developing a principled intuition behind the method and its optimal implementations rather of any exhaustive rigor. Whether a practitioner or a statistician, the dedicated reader will acquire a solid grasp of how Hamiltonian Monte Carlo works, when it succeeds, and, perhaps most importantly, when it fails.","accessed":{"date-parts":[["2022",5,18]]},"author":[{"family":"Betancourt","given":"Michael"}],"citation-key":"betancourtConceptualIntroductionHamiltonian2018","DOI":"10.48550/arXiv.1701.02434","issued":{"date-parts":[["2018",7,15]]},"publisher":"arXiv","source":"arXiv.org","title":"A Conceptual Introduction to Hamiltonian Monte Carlo","type":"article","URL":"http://arxiv.org/abs/1701.02434"},
  {"id":"betancourtGeometricFoundationsHamiltonian2014","abstract":"Although Hamiltonian Monte Carlo has proven an empirical success, the lack of a rigorous theoretical understanding of the algorithm has in many ways impeded both principled developments of the method and use of the algorithm in practice. In this paper we develop the formal foundations of the algorithm through the construction of measures on smooth manifolds, and demonstrate how the theory naturally identifies efficient implementations and motivates promising generalizations.","accessed":{"date-parts":[["2022",5,18]]},"author":[{"family":"Betancourt","given":"M. J."},{"family":"Byrne","given":"Simon"},{"family":"Livingstone","given":"Samuel"},{"family":"Girolami","given":"Mark"}],"citation-key":"betancourtGeometricFoundationsHamiltonian2014","issued":{"date-parts":[["2014",10,19]]},"publisher":"arXiv","source":"arXiv.org","title":"The Geometric Foundations of Hamiltonian Monte Carlo","type":"article","URL":"http://arxiv.org/abs/1410.5110"},
  {"id":"BiasReductionMaximum","abstract":"David Firth, Bias Reduction of Maximum Likelihood Estimates, Biometrika, Vol. 80, No. 1 (Mar., 1993), pp. 27-38","accessed":{"date-parts":[["2023",9,20]]},"citation-key":"BiasReductionMaximum","title":"Bias Reduction of Maximum Likelihood Estimates on JSTOR","type":"webpage","URL":"https://www-jstor-org.proxy1.lib.uwo.ca/stable/2336755"},
  {"id":"BiostatisticalMethodsAssessment","accessed":{"date-parts":[["2022",6,16]]},"citation-key":"BiostatisticalMethodsAssessment","language":"en-us","title":"Biostatistical Methods: The Assessment of Relative Risks, 2nd Edition | Wiley","title-short":"Biostatistical Methods","type":"book","URL":"https://www.wiley.com/en-us/Biostatistical+Methods%3A+The+Assessment+of+Relative+Risks%2C+2nd+Edition-p-9780470508220"},
  {"id":"blandComparisonsBaselineRandomised2011","abstract":"In randomised trials, rather than comparing randomised groups directly some researchers carry out a significance test comparing a baseline with a final measurement separately in each group.","accessed":{"date-parts":[["2023",8,4]]},"author":[{"family":"Bland","given":"J. Martin"},{"family":"Altman","given":"Douglas G."}],"citation-key":"blandComparisonsBaselineRandomised2011","container-title":"Trials","container-title-short":"Trials","DOI":"10.1186/1745-6215-12-264","ISSN":"1745-6215","issue":"1","issued":{"date-parts":[["2011",12,22]]},"page":"264","source":"BioMed Central","title":"Comparisons against baseline within randomised groups are often used and can be highly misleading","type":"article-journal","URL":"https://doi.org/10.1186/1745-6215-12-264","volume":"12"},
  {"id":"bojinovDesignAnalysisSwitchback2023","abstract":"Switchback experiments, where a firm sequentially exposes an experimental unit to random treatments, are among the most prevalent designs used in the technology sector, with applications ranging from ride-hailing platforms to online marketplaces. Although practitioners have widely adopted this technique, the derivation of the optimal design has been elusive, hindering practitioners from drawing valid causal conclusions with enough statistical power. We address this limitation by deriving the optimal design of switchback experiments under a range of different assumptions on the order of the carryover effect—the length of time a treatment persists in impacting the outcome. We cast the optimal experimental design problem as a minimax discrete optimization problem, identify the worst-case adversarial strategy, establish structural results, and solve the reduced problem via a continuous relaxation. For switchback experiments conducted under the optimal design, we provide two approaches for performing inference. The first provides exact randomization-based p-values, and the second uses a new finite population central limit theorem to conduct conservative hypothesis tests and build confidence intervals. We further provide theoretical results when the order of the carryover effect is misspecified and provide a data-driven procedure to identify the order of the carryover effect. We conduct extensive simulations to study the numerical performance and empirical properties of our results and conclude with practical suggestions. This paper was accepted by George Shanthikumar, big data analytics. Funding: The authors thank the Massachusetts Institute of Technology (MIT)-IBM partnership in Artificial Intelligence and the MIT Data Science Laboratory for support. Supplemental Material: Data and the online appendix are available at https://doi.org/10.1287/mnsc.2022.4583.","accessed":{"date-parts":[["2024",10,26]]},"author":[{"family":"Bojinov","given":"Iavor"},{"family":"Simchi-Levi","given":"David"},{"family":"Zhao","given":"Jinglong"}],"citation-key":"bojinovDesignAnalysisSwitchback2023","container-title":"Management Science","DOI":"10.1287/mnsc.2022.4583","ISSN":"0025-1909","issue":"7","issued":{"date-parts":[["2023",7]]},"page":"3759-3777","publisher":"INFORMS","source":"pubsonline.informs.org (Atypon)","title":"Design and Analysis of Switchback Experiments","type":"article-journal","URL":"https://pubsonline.informs.org/doi/abs/10.1287/mnsc.2022.4583","volume":"69"},
  {"id":"brookhartVariableSelectionPropensity2006","abstract":"Despite the growing popularity of propensity score (PS) methods in epidemiology, relatively little has been written in the epidemiologic literature about the problem of variable selection in PS models. The authors present the results of two simulation studies designed to help epidemiologists gain insight into the variable selection problem in a PS analysis. The simulation studies illustrate how the choice of variables that are included in a PS model can affect the bias, variance and mean-squared error of an estimated exposure effect. The results suggest that variables that are unrelated to the exposure but related to the outcome should always be included in a PS model. The inclusion of these variables will increase the precision of the estimated exposure effect without increasing bias. In contrast, including variables that are related to the exposure but not the outcome will decrease the precision of the estimated exposure effect without decreasing bias. In small studies, the inclusion of variables that are strongly related to the exposure but only weakly related to the outcome can be detrimental to an estimate in a mean-squared error sense. The addition of these variables removes only a small amount of bias but can strongly decrease the precision of the estimated exposure effect. These simulation studies and other analytical results suggest that standard model building tools designed to create good predictive models of the exposure will not necessarily lead to optimal PS models, particularly in the setting of small samples.","accessed":{"date-parts":[["2024",8,21]]},"author":[{"family":"Brookhart","given":"M. Alan"},{"family":"Schneeweiss","given":"Sebastian"},{"family":"Rothman","given":"Kenneth J."},{"family":"Glynn","given":"Robert J."},{"family":"Avorn","given":"Jerry"},{"family":"Stürmer","given":"Til"}],"citation-key":"brookhartVariableSelectionPropensity2006","container-title":"American journal of epidemiology","container-title-short":"Am J Epidemiol","DOI":"10.1093/aje/kwj149","ISSN":"0002-9262","issue":"12","issued":{"date-parts":[["2006",6,15]]},"page":"1149-1156","PMCID":"PMC1513192","PMID":"16624967","source":"PubMed Central","title":"Variable selection for propensity score models.","type":"article-journal","URL":"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1513192/","volume":"163"},
  {"id":"brooksPopulationPharmacokineticModelling2016","abstract":"Abstract This review summarises the available data on the population pharmacokinetics of tacrolimus and use ofMaximum A Posteriori(MAP) Bayesian estimation to predict tacrolimus exposure and subsequent drug dosage requirements in solid organ transplant recipients. A literature search was conducted which identified 56 studies that assessed the population pharmacokinetics of tacrolimus based on non-linear mixed effects modelling and 14 studies that assessed the predictive performance of MAP Bayesian estimation of tacrolimus area under the plasma concentration–time curve (AUC) from time zero to the end of the dosing interval. Studies were most commonly undertaken in adult kidney transplant recipients and investigated the immediate-release formulation. The pharmacokinetics of tacrolimus were described using one- and two-compartment disposition models with first-order elimination in 61 and 39 % of population pharmacokinetic studies, respectively. Variability in tacrolimus whole blood apparent clearance amongst transplant recipients was most commonly related to cytochrome P450 (CYP)3A5genotype (rs776746), patient haematocrit, patient weight, post-operative day and hepatic function (aspartate aminotransferase). Bias, as calculated using estimation of the mean predictive error (MPE) or mean percentage predictive error (MPPE) associated with prediction of the tacrolimus AUC, ranged from −15 to 9.95 %. Imprecision, as calculated through estimation of the root mean squared error (RMSE) or mean absolute prediction error (MAPE), was generally much poorer overall, ranging from 0.81 to 40.r2values ranged from 0.27 to 0.99 %. Of the Bayesian forecasting strategies that used two or more tacrolimus concentrations, 71 % showed bias of 10 % or less; however, only 39 % showed imprecision of 10 % or less. The combination of sampling times at 0, 1 and 3 h post-dose consistently showed bias and imprecision values of less than 15 %. No studies to date have examined how closely MAP Bayesian dosage predictions of tacrolimus actually achieve target AUC by comparing dosage prediction from one occasion with a future measured AUC. Further research involving larger prospective studies including more diverse transplant groups and the extended-release formulation of tacrolimus is needed. Several questions require further examination, including the following. Do Bayesian forecasting methods currently use the most appropriate population pharmacokinetic models and optimal sampling times for dosage prediction? Does Bayesian forecasting perform well when applied to make dosage predictions on a subsequent occasion? How can Bayesian forecasting be simplified for use in the clinical setting? And, are patient outcomes improved with dosage prediction based on Bayesian forecasting compared with trough concentration monitoring?","accessed":{"date-parts":[["2022",5,19]]},"author":[{"family":"Brooks","given":"Emily"},{"family":"Tett","given":"Susan E."},{"family":"Isbel","given":"Nicole M."},{"family":"Staatz","given":"Christine E."}],"citation-key":"brooksPopulationPharmacokineticModelling2016","container-title":"Clinical Pharmacokinetics","container-title-short":"Clinical Pharmacokinetics","DOI":"10.1007/s40262-016-0396-1","ISSN":"0312-5963","issue":"11","issued":{"date-parts":[["2016"]]},"page":"1295-1335","publisher":"Springer International Publishing","source":"Scholars Portal Journals","title":"Population Pharmacokinetic Modelling and Bayesian Estimation of Tacrolimus Exposure: Is this Clinically Useful for Dosage Prediction Yet?","title-short":"Population Pharmacokinetic Modelling and Bayesian Estimation of Tacrolimus Exposure","type":"article-journal","URL":"https://journals.scholarsportal.info/details/03125963/v55i0011/1295_ppmabecufdpy.xml","volume":"55"},
  {"id":"byonPopulationPharmacokineticsPharmacodynamics2017","abstract":"Apixaban is approved for treatment of venous thromboembolism (VTE) and prevention of recurrence. Population pharmacokinetics, pharmacokinetics–pharmacodynamics (anti-FXa activity), and exposure–response (binary bleeding and thromboembolic endpoints) of apixaban in VTE treatment subjects were characterized using data from phase I–III studies. Apixaban pharmacokinetics were adequately characterized by a two-compartment model with first-order absorption and elimination. Age, sex, and Asian race had less than 25% impact on exposure, while subjects with severe renal impairment were predicted to have 56% higher exposure than the reference subject (60-year-old non-Asian male weighing 85 kg with creatinine clearance of 100 mL/min). The relationship between apixaban concentration and anti-FXa activity was described by a linear model with a slope estimate of 0.0159 IU/ng. The number of subjects with either a bleeding or thromboembolic event was small, and no statistically significant relationship between apixaban exposure and clinical endpoints could be discerned with a logistic regression analysis.","accessed":{"date-parts":[["2022",5,18]]},"author":[{"family":"Byon","given":"W"},{"family":"Sweeney","given":"K"},{"family":"Frost","given":"C"},{"family":"Boyd","given":"Ra"}],"citation-key":"byonPopulationPharmacokineticsPharmacodynamics2017","container-title":"CPT: Pharmacometrics & Systems Pharmacology","DOI":"10.1002/psp4.12184","ISSN":"2163-8306","issue":"5","issued":{"date-parts":[["2017"]]},"language":"en","page":"340-349","source":"Wiley Online Library","title":"Population Pharmacokinetics, Pharmacodynamics, and Exploratory Exposure–Response Analyses of Apixaban in Subjects Treated for Venous Thromboembolism","type":"article-journal","URL":"https://onlinelibrary.wiley.com/doi/abs/10.1002/psp4.12184","volume":"6"},
  {"id":"callawayDifferenceinDifferencesMultipleTime2021","abstract":"In this article, we consider identification, estimation, and inference procedures for treatment effect parameters using Difference-in-Differences (DiD) with (i) multiple time periods, (ii) variation in treatment timing, and (iii) when the “parallel trends assumption” holds potentially only after conditioning on observed covariates. We show that a family of causal effect parameters are identified in staggered DiD setups, even if differences in observed characteristics create non-parallel outcome dynamics between groups. Our identification results allow one to use outcome regression, inverse probability weighting, or doubly-robust estimands. We also propose different aggregation schemes that can be used to highlight treatment effect heterogeneity across different dimensions as well as to summarize the overall effect of participating in the treatment. We establish the asymptotic properties of the proposed estimators and prove the validity of a computationally convenient bootstrap procedure to conduct asymptotically valid simultaneous (instead of pointwise) inference. Finally, we illustrate the relevance of our proposed tools by analyzing the effect of the minimum wage on teen employment from 2001–2007. Open-source software is available for implementing the proposed methods.","accessed":{"date-parts":[["2023",9,6]]},"author":[{"family":"Callaway","given":"Brantly"},{"family":"Sant’Anna","given":"Pedro H. C."}],"citation-key":"callawayDifferenceinDifferencesMultipleTime2021","collection-title":"Themed Issue: Treatment Effect 1","container-title":"Journal of Econometrics","container-title-short":"Journal of Econometrics","DOI":"10.1016/j.jeconom.2020.12.001","ISSN":"0304-4076","issue":"2","issued":{"date-parts":[["2021",12,1]]},"page":"200-230","source":"ScienceDirect","title":"Difference-in-Differences with multiple time periods","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S0304407620303948","volume":"225"},
  {"id":"carlinUsesAbusesRegression2025","abstract":"Regression methods dominate the practice of biostatistical analysis, but biostatistical training emphasizes the details of regression models and methods ahead of the purposes for which such modeling might be useful. More broadly, statistics is widely understood to provide a body of techniques for “modeling data,” underpinned by what we describe as the “true model myth”: that the task of the statistician/data analyst is to build a model that closely approximates the true data generating process. By way of our own historical examples and a brief review of mainstream clinical research journals, we describe how this perspective has led to a range of problems in the application of regression methods, including misguided “adjustment” for covariates, misinterpretation of regression coefficients and the widespread fitting of regression models without a clear purpose. We then outline a new approach to the teaching and application of biostatistical methods, which situates them within a framework that first requires clear definition of the substantive research question at hand, within one of three categories: descriptive, predictive, or causal. Within this approach, the development and application of (multivariable) regression models, as well as other advanced biostatistical methods, should proceed differently according to the type of question. Regression methods will no doubt remain central to statistical practice as they provide a powerful tool for representing variation in a response or outcome variable as a function of “input” variables, but their conceptualization and usage should follow from the purpose at hand.","accessed":{"date-parts":[["2025",7,4]]},"author":[{"family":"Carlin","given":"John B."},{"family":"Moreno-Betancur","given":"Margarita"}],"citation-key":"carlinUsesAbusesRegression2025","container-title":"Statistics in Medicine","DOI":"10.1002/sim.10244","ISSN":"1097-0258","issue":"13-14","issued":{"date-parts":[["2025"]]},"language":"en","license":"© 2025 The Author(s). Statistics in Medicine published by John Wiley & Sons Ltd.","page":"e10244","source":"Wiley Online Library","title":"On the Uses and Abuses of Regression Models: A Call for Reform of Statistical Practice and Teaching","title-short":"On the Uses and Abuses of Regression Models","type":"article-journal","URL":"https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.10244","volume":"44"},
  {"id":"chakrabortyDynamicTreatmentRegimes2014","abstract":"A dynamic treatment regime consists of a sequence of decision rules, one per stage of intervention, that dictate how to individualize treatments to patients, based on evolving treatment and covariate history. These regimes are particularly useful for managing chronic disorders and fit well into the larger paradigm of personalized medicine. They provide one way to operationalize a clinical decision support system. Statistics plays a key role in the construction of evidence-based dynamic treatment regimes—informing the best study design as well as efficient estimation and valid inference. Owing to the many novel methodological challenges this area offers, it has been growing in popularity among statisticians in recent years. In this article, we review the key developments in this exciting field of research. In particular, we discuss the sequential multiple assignment randomized trial designs, estimation techniques like Q-learning and marginal structural models, and several inference techniques designed to address the associated nonstandard asymptotics. We reference software whenever available. We also outline some important future directions.","accessed":{"date-parts":[["2022",5,21]]},"author":[{"family":"Chakraborty","given":"Bibhas"},{"family":"Murphy","given":"Susan A."}],"citation-key":"chakrabortyDynamicTreatmentRegimes2014","container-title":"Annual Review of Statistics and Its Application","container-title-short":"Annual Review of Statistics and Its Application","DOI":"10.1146/annurev-statistics-022513-115553","ISSN":"2326-8298","issued":{"date-parts":[["2014"]]},"page":"447-464","publisher":"Annual Reviews","source":"Scholars Portal Journals","title":"Dynamic Treatment Regimes","type":"article-journal","URL":"https://journals.scholarsportal.info/details/23268298/v1inone/447_dtr.xml","volume":"1"},
  {"id":"chenLoglikeIdentifiedATEs2023","abstract":"Researchers frequently estimate the average treatment effect (ATE) in logs, which has the desirable property that its units approximate percentages. When the outcome takes on zero values, researchers often use alternative transformations (e.g., $\\log(1+Y)$, $\\mathrm{arcsinh}(Y)$) that behave like $\\log(Y)$ for large values of $Y$, and interpret the units as percentages. In this paper, we show that ATEs for transformations other than $\\log(Y)$ cannot be interpreted as percentages, at least if one imposes the requirement that a percentage does not depend on the original scaling of the outcome (e.g. dollars versus cents). We first show that if $m(y)$ is a function that behaves like $\\log(y)$ for large values of $y$ and the treatment affects the probability that $Y=0$, then the ATE for $m(Y)$ can be made arbitrarily large or small in magnitude by re-scaling the units of $Y$. More generally, we show that any parameter of the form $\\theta_g = E[g(Y(1),Y(0)) ]$ that is well-defined with zero-valued outcomes cannot be both scale-invariant and point-identified. Thus, researchers must either consider parameters outside of this class, or give up on either scale-invariance or point-identification. We conclude by outlining a variety of options available to empirical researchers dealing with zero-valued outcomes, including (i) estimating ATEs for normalized outcomes, (ii) explicitly calibrating the value placed on the extensive versus intensive margins, or (iii) estimating separate effects for the intensive and extensive margins.","accessed":{"date-parts":[["2023",1,6]]},"author":[{"family":"Chen","given":"Jiafeng"},{"family":"Roth","given":"Jonathan"}],"citation-key":"chenLoglikeIdentifiedATEs2023","DOI":"10.48550/arXiv.2212.06080","issued":{"date-parts":[["2023",1,2]]},"number":"arXiv:2212.06080","publisher":"arXiv","source":"arXiv.org","title":"Log-like? Identified ATEs defined with zero-valued outcomes are (arbitrarily) scale-dependent","title-short":"Log-like?","type":"article","URL":"http://arxiv.org/abs/2212.06080"},
  {"id":"chenPersonalizedDoseFinding2016","abstract":"In dose-finding clinical trials, it is becoming increasingly important to account for individual-level heterogeneity while searching for optimal doses to ensure an optimal individualized dose rule (IDR) maximizes the expected beneficial clinical outcome for each individual. In this article, we advocate a randomized trial design where candidate dose levels assigned to study subjects are randomly chosen from a continuous distribution within a safe range. To estimate the optimal IDR using such data, we propose an outcome weighted learning method based on a nonconvex loss function, which can be solved efficiently using a difference of convex functions algorithm. The consistency and convergence rate for the estimated IDR are derived, and its small-sample performance is evaluated via simulation studies. We demonstrate that the proposed method outperforms competing approaches. Finally, we illustrate this method using data from a cohort study for warfarin (an anti-thrombotic drug) dosing. Supplementary materials for this article are available online.","accessed":{"date-parts":[["2022",5,19]]},"author":[{"family":"Chen","given":"Guanhua"},{"family":"Zeng","given":"Donglin"},{"family":"Kosorok","given":"Michael R."}],"citation-key":"chenPersonalizedDoseFinding2016","container-title":"Journal of the American Statistical Association","DOI":"10.1080/01621459.2016.1148611","ISSN":"0162-1459","issue":"516","issued":{"date-parts":[["2016",10,1]]},"page":"1509-1521","PMID":"28255189","publisher":"Taylor & Francis","source":"Taylor and Francis+NEJM","title":"Personalized Dose Finding Using Outcome Weighted Learning","type":"article-journal","URL":"https://doi.org/10.1080/01621459.2016.1148611","volume":"111"},
  {"id":"choudhuryDynamicTreatmentEffect2015","abstract":"We present a method for estimating the empirical dynamic treatment effect (DTE) curves from tumor growth delay (TGD) studies. This improves on current common methods of TGD analysis, such as T/C ratio and doubling times, by providing a more detailed treatment effect and overcomes their lack of reproducibility. The methodology doesn't presuppose any prior form for the treatment effect dynamics and is shown to give consistent estimates with missing data. The method is illustrated by application to real data from TGD studies involving three types of therapy. Firstly, we demonstrate that radiotherapy induces a sharp peak in inhibition in a FaDu model. The height, duration and timing of the peak increase linearly with radiation dose. Second, we demonstrate that a combination of temozolomide and an experimental therapy in a glioma PDX model yields an effect, similar to an additive version of the DTE curves for the mono-therapies, except that there is a 30 day delay in peak inhibition. In the third study, we consider the DTE of anti-angiogenic therapy in glioma. We show that resulting DTE curves are flat. We discuss how features of the DTE curves should be interpreted and potentially used to improve therapy.","accessed":{"date-parts":[["2022",5,19]]},"author":[{"family":"Choudhury","given":"Kingshuk Roy"},{"family":"Keir","given":"Stephen T."},{"family":"Ashcraft","given":"Kathleen A."},{"family":"Boss","given":"Mary-Keara"},{"family":"Dewhirst","given":"Mark W."}],"citation-key":"choudhuryDynamicTreatmentEffect2015","container-title":"Oncotarget","container-title-short":"Oncotarget","ISSN":"1949-2553","issue":"16","issued":{"date-parts":[["2015",5,15]]},"page":"14656-14668","PMCID":"PMC4546495","PMID":"25986925","source":"PubMed Central","title":"Dynamic treatment effect (DTE) curves reveal the mode of action for standard and experimental cancer therapies","type":"article-journal","URL":"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4546495/","volume":"6"},
  {"id":"christodoulouSystematicReviewShows2019","abstract":"Objectives\nThe objective of this study was to compare performance of logistic regression (LR) with machine learning (ML) for clinical prediction modeling in the literature.\nStudy Design and Setting\nWe conducted a Medline literature search (1/2016 to 8/2017) and extracted comparisons between LR and ML models for binary outcomes.\nResults\nWe included 71 of 927 studies. The median sample size was 1,250 (range 72–3,994,872), with 19 predictors considered (range 5–563) and eight events per predictor (range 0.3–6,697). The most common ML methods were classification trees, random forests, artificial neural networks, and support vector machines. In 48 (68%) studies, we observed potential bias in the validation procedures. Sixty-four (90%) studies used the area under the receiver operating characteristic curve (AUC) to assess discrimination. Calibration was not addressed in 56 (79%) studies. We identified 282 comparisons between an LR and ML model (AUC range, 0.52–0.99). For 145 comparisons at low risk of bias, the difference in logit(AUC) between LR and ML was 0.00 (95% confidence interval, −0.18 to 0.18). For 137 comparisons at high risk of bias, logit(AUC) was 0.34 (0.20–0.47) higher for ML.\nConclusion\nWe found no evidence of superior performance of ML over LR. Improvements in methodology and reporting are needed for studies that compare modeling algorithms.","accessed":{"date-parts":[["2024",4,25]]},"author":[{"family":"Christodoulou","given":"Evangelia"},{"family":"Ma","given":"Jie"},{"family":"Collins","given":"Gary S."},{"family":"Steyerberg","given":"Ewout W."},{"family":"Verbakel","given":"Jan Y."},{"family":"Van Calster","given":"Ben"}],"citation-key":"christodoulouSystematicReviewShows2019","container-title":"Journal of Clinical Epidemiology","container-title-short":"Journal of Clinical Epidemiology","DOI":"10.1016/j.jclinepi.2019.02.004","ISSN":"0895-4356","issued":{"date-parts":[["2019",6,1]]},"page":"12-22","source":"ScienceDirect","title":"A systematic review shows no performance benefit of machine learning over logistic regression for clinical prediction models","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S0895435618310813","volume":"110"},
  {"id":"cinelliCrashCourseGood2022","abstract":"Many students of statistics and econometrics express frustration with the way a problem known as “bad control” is treated in the traditional literature. The issue arises when the addition of a variable to a regression equation produces an unintended discrepancy between the regression coefficient and the effect that the coefficient is intended to represent. Avoiding such discrepancies presents a challenge to all analysts in the data intensive sciences. This note describes graphical tools for understanding, visualizing, and resolving the problem through a series of illustrative examples. By making this “crash course” accessible to instructors and practitioners, we hope to avail these tools to a broader community of scientists concerned with the causal interpretation of regression models.","accessed":{"date-parts":[["2023",6,27]]},"author":[{"family":"Cinelli","given":"Carlos"},{"family":"Forney","given":"Andrew"},{"family":"Pearl","given":"Judea"}],"citation-key":"cinelliCrashCourseGood2022","container-title":"Sociological Methods & Research","DOI":"10.1177/00491241221099552","ISSN":"0049-1241","issued":{"date-parts":[["2022",5,20]]},"language":"en","page":"00491241221099552","publisher":"SAGE Publications Inc","source":"SAGE Journals","title":"A Crash Course in Good and Bad Controls","type":"article-journal","URL":"https://doi.org/10.1177/00491241221099552"},
  {"id":"cirincionePopulationPharmacokineticsApixaban2018","abstract":"This analysis describes the population pharmacokinetics (PPK) of apixaban in nonvalvular atrial fibrillation (NVAF) subjects, and quantifies the impact of intrinsic and extrinsic factors on exposure. The PPK model was developed using data from phase I–III studies. Apixaban exposure was characterized by a two-compartment PPK model with first-order absorption and elimination. Predictive covariates on apparent clearance included age, sex, Asian race, renal function, and concomitant strong/moderate cytochrome P450 (CYP)3A4/P-glycoprotein (P-gp) inhibitors. Individual covariate effects generally resulted in < 25% change in apixaban exposure vs. the reference NVAF subject (non-Asian, male, aged 65 years, weighing 70 kg without concomitant CYP3A4/P-gp inhibitors), except for severe renal impairment, which resulted in 55% higher exposure than the reference subject. The dose-reduction algorithm resulted in a 27% lower median exposure, with a large overlap between the 2.5-mg and 5-mg groups. The impact of Asian race on apixaban exposure was < 15% and not considered clinically significant.","accessed":{"date-parts":[["2022",5,28]]},"author":[{"family":"Cirincione","given":"Brenda"},{"family":"Kowalski","given":"Kenneth"},{"family":"Nielsen","given":"Jace"},{"family":"Roy","given":"Amit"},{"family":"Thanneer","given":"Neelima"},{"family":"Byon","given":"Wonkyung"},{"family":"Boyd","given":"Rebecca"},{"family":"Wang","given":"Xiaoli"},{"family":"Leil","given":"Tarek"},{"family":"LaCreta","given":"Frank"},{"family":"Ueno","given":"Takayo"},{"family":"Oishi","given":"Masayo"},{"family":"Frost","given":"Charles"}],"citation-key":"cirincionePopulationPharmacokineticsApixaban2018","container-title":"CPT: Pharmacometrics & Systems Pharmacology","DOI":"10.1002/psp4.12347","ISSN":"2163-8306","issue":"11","issued":{"date-parts":[["2018"]]},"language":"en","page":"728-738","source":"Wiley Online Library","title":"Population Pharmacokinetics of Apixaban in Subjects With Nonvalvular Atrial Fibrillation","type":"article-journal","URL":"http://onlinelibrary.wiley.com/doi/abs/10.1002/psp4.12347","volume":"7"},
  {"id":"cliftDevelopmentInternalexternalValidation2023","abstract":"Objective To develop a clinically useful model that estimates the 10 year risk of breast cancer related mortality in women (self-reported female sex) with breast cancer of any stage, comparing results from regression and machine learning approaches.\nDesign Population based cohort study.\nSetting QResearch primary care database in England, with individual level linkage to the national cancer registry, Hospital Episodes Statistics, and national mortality registers.\nParticipants 141 765 women aged 20 years and older with a diagnosis of invasive breast cancer between 1 January 2000 and 31 December 2020.\nMain outcome measures Four model building strategies comprising two regression (Cox proportional hazards and competing risks regression) and two machine learning (XGBoost and an artificial neural network) approaches. Internal-external cross validation was used for model evaluation. Random effects meta-analysis that pooled estimates of discrimination and calibration metrics, calibration plots, and decision curve analysis were used to assess model performance, transportability, and clinical utility.\nResults During a median 4.16 years (interquartile range 1.76-8.26) of follow-up, 21 688 breast cancer related deaths and 11 454 deaths from other causes occurred. Restricting to 10 years maximum follow-up from breast cancer diagnosis, 20 367 breast cancer related deaths occurred during a total of 688 564.81 person years. The crude breast cancer mortality rate was 295.79 per 10 000 person years (95% confidence interval 291.75 to 299.88). Predictors varied for each regression model, but both Cox and competing risks models included age at diagnosis, body mass index, smoking status, route to diagnosis, hormone receptor status, cancer stage, and grade of breast cancer. The Cox model’s random effects meta-analysis pooled estimate for Harrell’s C index was the highest of any model at 0.858 (95% confidence interval 0.853 to 0.864, and 95% prediction interval 0.843 to 0.873). It appeared acceptably calibrated on calibration plots. The competing risks regression model had good discrimination: pooled Harrell’s C index 0.849 (0.839 to 0.859, and 0.821 to 0.876, and evidence of systematic miscalibration on summary metrics was lacking. The machine learning models had acceptable discrimination overall (Harrell’s C index: XGBoost 0.821 (0.813 to 0.828, and 0.805 to 0.837); neural network 0.847 (0.835 to 0.858, and 0.816 to 0.878)), but had more complex patterns of miscalibration and more variable regional and stage specific performance. Decision curve analysis suggested that the Cox and competing risks regression models tested may have higher clinical utility than the two machine learning approaches.\nConclusion In women with breast cancer of any stage, using the predictors available in this dataset, regression based methods had better and more consistent performance compared with machine learning approaches and may be worthy of further evaluation for potential clinical use, such as for stratified follow-up.","accessed":{"date-parts":[["2024",4,25]]},"author":[{"family":"Clift","given":"Ash Kieran"},{"family":"Dodwell","given":"David"},{"family":"Lord","given":"Simon"},{"family":"Petrou","given":"Stavros"},{"family":"Brady","given":"Michael"},{"family":"Collins","given":"Gary S."},{"family":"Hippisley-Cox","given":"Julia"}],"citation-key":"cliftDevelopmentInternalexternalValidation2023","container-title":"BMJ","container-title-short":"BMJ","DOI":"10.1136/bmj-2022-073800","ISSN":"1756-1833","issued":{"date-parts":[["2023",5,10]]},"language":"en","license":"© Author(s) (or their employer(s)) 2019. Re-use permitted under CC BY. No commercial re-use. See rights and permissions. Published by BMJ.. http://creativecommons.org/licenses/by/4.0/This is an Open Access article distributed in accordance with the terms of the Creative Commons Attribution (CC BY 4.0) license, which permits others to distribute, remix, adapt and build upon this work, for commercial use, provided the original work is properly cited. See: http://creativecommons.org/licenses/by/4.0/.","page":"e073800","PMID":"37164379","publisher":"British Medical Journal Publishing Group","section":"Research","source":"www.bmj.com","title":"Development and internal-external validation of statistical and machine learning models for breast cancer prognostication: cohort study","title-short":"Development and internal-external validation of statistical and machine learning models for breast cancer prognostication","type":"article-journal","URL":"https://www.bmj.com/content/381/bmj-2022-073800","volume":"381"},
  {"id":"cliftPredicting10yearBreast2023","accessed":{"date-parts":[["2024",4,25]]},"author":[{"family":"Clift","given":"Ash Kieran"},{"family":"Collins","given":"Gary S."},{"family":"Lord","given":"Simon"},{"family":"Petrou","given":"Stavros"},{"family":"Dodwell","given":"David"},{"family":"Brady","given":"Michael"},{"family":"Hippisley-Cox","given":"Julia"}],"citation-key":"cliftPredicting10yearBreast2023","container-title":"The Lancet Digital Health","container-title-short":"The Lancet Digital Health","DOI":"10.1016/S2589-7500(23)00113-9","ISSN":"2589-7500","issue":"9","issued":{"date-parts":[["2023",9,1]]},"language":"English","page":"e571-e581","PMID":"37625895","publisher":"Elsevier","source":"www.thelancet.com","title":"Predicting 10-year breast cancer mortality risk in the general female population in England: a model development and validation study","title-short":"Predicting 10-year breast cancer mortality risk in the general female population in England","type":"article-journal","URL":"https://www.thelancet.com/journals/landig/article/PIIS2589-7500(23)00113-9/fulltext","volume":"5"},
  {"id":"collinsTRIPOD+AIStatementUpdated2024","abstract":"<p>The TRIPOD (Transparent Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis) statement was published in 2015 to provide the minimum reporting recommendations for studies developing or evaluating the performance of a prediction model. Methodological advances in the field of prediction have since included the widespread use of artificial intelligence (AI) powered by machine learning methods to develop prediction models. An update to the TRIPOD statement is thus needed. TRIPOD+AI provides harmonised guidance for reporting prediction model studies, irrespective of whether regression modelling or machine learning methods have been used. The new checklist supersedes the TRIPOD 2015 checklist, which should no longer be used. This article describes the development of TRIPOD+AI and presents the expanded 27 item checklist with more detailed explanation of each reporting recommendation, and the TRIPOD+AI for Abstracts checklist. TRIPOD+AI aims to promote the complete, accurate, and transparent reporting of studies that develop a prediction model or evaluate its performance. Complete reporting will facilitate study appraisal, model evaluation, and model implementation.</p>","accessed":{"date-parts":[["2024",4,25]]},"author":[{"family":"Collins","given":"Gary S."},{"family":"Moons","given":"Karel G. M."},{"family":"Dhiman","given":"Paula"},{"family":"Riley","given":"Richard D."},{"family":"Beam","given":"Andrew L."},{"family":"Calster","given":"Ben Van"},{"family":"Ghassemi","given":"Marzyeh"},{"family":"Liu","given":"Xiaoxuan"},{"family":"Reitsma","given":"Johannes B."},{"family":"Smeden","given":"Maarten","dropping-particle":"van"},{"family":"Boulesteix","given":"Anne-Laure"},{"family":"Camaradou","given":"Jennifer Catherine"},{"family":"Celi","given":"Leo Anthony"},{"family":"Denaxas","given":"Spiros"},{"family":"Denniston","given":"Alastair K."},{"family":"Glocker","given":"Ben"},{"family":"Golub","given":"Robert M."},{"family":"Harvey","given":"Hugh"},{"family":"Heinze","given":"Georg"},{"family":"Hoffman","given":"Michael M."},{"family":"Kengne","given":"André Pascal"},{"family":"Lam","given":"Emily"},{"family":"Lee","given":"Naomi"},{"family":"Loder","given":"Elizabeth W."},{"family":"Maier-Hein","given":"Lena"},{"family":"Mateen","given":"Bilal A."},{"family":"McCradden","given":"Melissa D."},{"family":"Oakden-Rayner","given":"Lauren"},{"family":"Ordish","given":"Johan"},{"family":"Parnell","given":"Richard"},{"family":"Rose","given":"Sherri"},{"family":"Singh","given":"Karandeep"},{"family":"Wynants","given":"Laure"},{"family":"Logullo","given":"Patricia"}],"citation-key":"collinsTRIPOD+AIStatementUpdated2024","container-title":"BMJ","container-title-short":"BMJ","DOI":"10.1136/bmj-2023-078378","ISSN":"1756-1833","issued":{"date-parts":[["2024",4,16]]},"language":"en","license":"© Author(s) (or their employer(s)) 2019. Re-use permitted under CC BY. No commercial re-use. See rights and permissions. Published by BMJ.. http://creativecommons.org/licenses/by/4.0/This is an Open Access article distributed in accordance with the terms of the Creative Commons Attribution (CC BY 4.0) license, which permits others to distribute, remix, adapt and build upon this work, for commercial use, provided the original work is properly cited. See: http://creativecommons.org/licenses/by/4.0/.","page":"e078378","PMID":"38626948","publisher":"British Medical Journal Publishing Group","section":"Research Methods &amp; Reporting","source":"www.bmj.com","title":"TRIPOD+AI statement: updated guidance for reporting clinical prediction models that use regression or machine learning methods","title-short":"TRIPOD+AI statement","type":"article-journal","URL":"https://www.bmj.com/content/385/bmj-2023-078378","volume":"385"},
  {"id":"danielMakingApplesOranges2021","abstract":"We revisit the well‐known but often misunderstood issue of (non)collapsibility of effect measures in regression models for binary and time‐to‐event outcomes. We describe an existing simple but largely ignored procedure for marginalizing estimates of conditional odds ratios and propose a similar procedure for marginalizing estimates of conditional hazard ratios (allowing for right censoring), demonstrating its performance in simulation studies and in a reanalysis of data from a small randomized trial in primary biliary cirrhosis patients. In addition, we aim to provide an educational summary of issues surrounding (non)collapsibility from a causal inference perspective and to promote the idea that the words conditional and adjusted (likewise marginal and unadjusted) should not be used interchangeably.","accessed":{"date-parts":[["2023",4,26]]},"author":[{"family":"Daniel","given":"Rhian"},{"family":"Zhang","given":"Jingjing"},{"family":"Farewell","given":"Daniel"}],"citation-key":"danielMakingApplesOranges2021","container-title":"Biometrical Journal. Biometrische Zeitschrift","container-title-short":"Biom J","DOI":"10.1002/bimj.201900297","ISSN":"0323-3847","issue":"3","issued":{"date-parts":[["2021",3]]},"page":"528-557","PMCID":"PMC7986756","PMID":"33314251","source":"PubMed Central","title":"Making apples from oranges: Comparing noncollapsible effect estimators and their standard errors after adjustment for different covariate sets","title-short":"Making apples from oranges","type":"article-journal","URL":"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7986756/","volume":"63"},
  {"id":"dengApplyingDeltaMethod2018","abstract":"During the last decade, the information technology industry has adopted a data-driven culture, relying on online metrics to measure and monitor business performance. Under the setting of big data, the majority of such metrics approximately follow normal distributions, opening up potential opportunities to model them directly without extra model assumptions and solve big data problems via closed-form formulas using distributed algorithms at a fraction of the cost of simulation-based procedures like bootstrap. However, certain attributes of the metrics, such as their corresponding data generating processes and aggregation levels, pose numerous challenges for constructing trustworthy estimation and inference procedures. Motivated by four real-life examples in metric development and analytics for large-scale A/B testing, we provide a practical guide to applying the Delta method, one of the most important tools from the classic statistics literature, to address the aforementioned challenges. We emphasize the central role of the Delta method in metric analytics by highlighting both its classic and novel applications.","accessed":{"date-parts":[["2025",10,24]]},"author":[{"family":"Deng","given":"Alex"},{"family":"Knoblich","given":"Ulf"},{"family":"Lu","given":"Jiannan"}],"citation-key":"dengApplyingDeltaMethod2018","DOI":"10.48550/arXiv.1803.06336","issued":{"date-parts":[["2018",9,12]]},"number":"arXiv:1803.06336","publisher":"arXiv","source":"arXiv.org","title":"Applying the Delta method in metric analytics: A practical guide with novel ideas","title-short":"Applying the Delta method in metric analytics","type":"article","URL":"http://arxiv.org/abs/1803.06336","version":"4"},
  {"id":"dengEquivalenceDeltaMethod2021","abstract":"It often happens that the same problem presents itself to different communities and the solutions proposed or adopted by those communities are different. We take the case of the variance estimation of the population average treatment effect in cluster-randomized experiments. The econometrics literature promotes the cluster-robust variance estimator (Athey and Imbens, 2017), which can be dated back to the study of linear regression with clustered residuals (Liang and Zeger, 1986). The A/B testing or online experimentation literature promotes the delta method (Kohavi et al., 2010, Deng et al., 2017, 2018), which tackles the variance estimation of the ATE estimator directly using large sample theory. The two methods are seemly different as the former begins with a regression setting at the individual unit level and the latter is semi-parametric with only i.i.d. assumptions on the clusters. Both methods are widely used in practice. It begs the question for their connection and comparison. In this paper we prove they are equivalent and in the canonical implementation they should give exactly the same result.","accessed":{"date-parts":[["2025",10,24]]},"author":[{"family":"Deng","given":"Alex"},{"family":"Lu","given":"Jiannan"},{"family":"Qin","given":"Wen"}],"citation-key":"dengEquivalenceDeltaMethod2021","DOI":"10.48550/arXiv.2105.14705","issued":{"date-parts":[["2021",5,31]]},"number":"arXiv:2105.14705","publisher":"arXiv","source":"arXiv.org","title":"The equivalence of the Delta method and the cluster-robust variance estimator for the analysis of clustered randomized experiments","type":"article","URL":"http://arxiv.org/abs/2105.14705"},
  {"id":"dengImprovingSensitivityOnline2013","abstract":"Online controlled experiments are at the heart of making data-driven decisions at a diverse set of companies, including Amazon, eBay, Facebook, Google, Microsoft, Yahoo, and Zynga. Small diﬀerences in key metrics, on the order of fractions of a percent, may have very signiﬁcant business implications. At Bing it is not uncommon to see experiments that impact annual revenue by millions of dollars, even tens of millions of dollars, either positively or negatively. With thousands of experiments being run annually, improving the sensitivity of experiments allows for more precise assessment of value, or equivalently running the experiments on smaller populations (supporting more experiments) or for shorter durations (improving the feedback cycle and agility). We propose an approach (CUPED) that utilizes data from the pre-experiment period to reduce metric variability and hence achieve better sensitivity. This technique is applicable to a wide variety of key business metrics, and it is practical and easy to implement. The results on Bing’s experimentation system are very successful: we can reduce variance by about 50%, eﬀectively achieving the same statistical power with only half of the users, or half the duration.","accessed":{"date-parts":[["2022",5,20]]},"author":[{"family":"Deng","given":"Alex"},{"family":"Xu","given":"Ya"},{"family":"Kohavi","given":"Ron"},{"family":"Walker","given":"Toby"}],"citation-key":"dengImprovingSensitivityOnline2013","container-title":"Proceedings of the sixth ACM international conference on Web search and data mining - WSDM '13","DOI":"10.1145/2433396.2433413","event-place":"Rome, Italy","event-title":"the sixth ACM international conference","ISBN":"978-1-4503-1869-3","issued":{"date-parts":[["2013"]]},"language":"en","page":"123","publisher":"ACM Press","publisher-place":"Rome, Italy","source":"DOI.org (Crossref)","title":"Improving the sensitivity of online controlled experiments by utilizing pre-experiment data","type":"paper-conference","URL":"http://dl.acm.org/citation.cfm?doid=2433396.2433413"},
  {"id":"dengZeroHeroExploiting2023","abstract":"In online experiments where the intervention is only exposed, or \"triggered\", for a small subset of the population, it is critical to use variance reduction techniques to estimate treatment effects with sufficient precision to inform business decisions. Trigger-dilute analysis is often used in these situations, and reduces the sampling variance of overall intent-to-treat (ITT) effects by an order of magnitude equal to the inverse of the triggering rate; for example, a triggering rate of $5\\%$ corresponds to roughly a $20x$ reduction in variance. To apply trigger-dilute analysis, one needs to know experimental subjects' triggering counterfactual statuses, i.e., the counterfactual behavior of subjects under both treatment and control conditions. In this paper, we propose an unbiased ITT estimator with reduced variance applicable for experiments where the triggering counterfactual status is only observed in the treatment group. Our method is based on the efficiency augmentation idea of CUPED and draws upon identification frameworks from the principal stratification and instrumental variables literature. The unbiasedness of our estimation approach relies on a testable assumption that the augmentation term used for covariate adjustment equals zero in expectation. Unlike traditional covariate adjustment or principal score modeling approaches, our estimator can incorporate both pre-experiment and in-experiment observations. We demonstrate through a real-world experiment and simulations that our estimator can remain unbiased and achieve precision improvements as large as if triggering status were fully observed, and in some cases can even outperform trigger-dilute analysis.","accessed":{"date-parts":[["2023",2,13]]},"author":[{"family":"Deng","given":"Alex"},{"family":"Yuan","given":"Lo-Hua"},{"family":"Kanai","given":"Naoya"},{"family":"Salama-Manteau","given":"Alexandre"}],"citation-key":"dengZeroHeroExploiting2023","DOI":"10.48550/arXiv.2112.13299","issued":{"date-parts":[["2023",1,31]]},"number":"arXiv:2112.13299","publisher":"arXiv","source":"arXiv.org","title":"Zero to Hero: Exploiting Null Effects to Achieve Variance Reduction in Experiments with One-sided Triggering","title-short":"Zero to Hero","type":"article","URL":"http://arxiv.org/abs/2112.13299"},
  {"id":"doudchenkoBalancingRegressionDifferenceInDifferences2016","abstract":"In a seminal paper Abadie et al (2010) develop the synthetic control procedure for estimating the effect of a treatment, in the presence of a single treated unit and a number of control units, with pre-treatment outcomes observed for all units. The method constructs a set of weights such that covariates and pre-treatment outcomes of the treated unit are approximately matched by a weighted average of control units. The weights are restricted to be nonnegative and sum to one, which allows the procedure to obtain the weights even when the number of lagged outcomes is modest relative to the number of control units, a setting that is not uncommon in applications. In the current paper we propose a more general class of synthetic control estimators that allows researchers to relax some of the restrictions in the ADH method. We allow the weights to be negative, do not necessarily restrict the sum of the weights, and allow for a permanent additive difference between the treated unit and the controls, similar to difference-in-difference procedures. The weights directly minimize the distance between the lagged outcomes for the treated and the control units, using regularization methods to deal with a potentially large number of possible control units.","accessed":{"date-parts":[["2024",7,2]]},"author":[{"family":"Doudchenko","given":"Nikolay"},{"family":"Imbens","given":"Guido W."}],"citation-key":"doudchenkoBalancingRegressionDifferenceInDifferences2016","collection-title":"Working Paper Series","DOI":"10.3386/w22791","genre":"Working Paper","issued":{"date-parts":[["2016",10]]},"number":"22791","publisher":"National Bureau of Economic Research","source":"National Bureau of Economic Research","title":"Balancing, Regression, Difference-In-Differences and Synthetic Control Methods: A Synthesis","title-short":"Balancing, Regression, Difference-In-Differences and Synthetic Control Methods","type":"article","URL":"https://www.nber.org/papers/w22791"},
  {"id":"dubeyScalableInterpretabilityPolynomials2022","abstract":"Generalized Additive Models (GAMs) have quickly become the leading choice for fully-interpretable machine learning. However, unlike uninterpretable methods such as DNNs, they lack expressive power and easy scalability, and are hence not a feasible alternative for real-world tasks. We present a new class of GAMs that use tensor rank decompositions of polynomials to learn powerful, {\\em fully-interpretable} models. Our approach, titled Scalable Polynomial Additive Models (SPAM) is effortlessly scalable and models {\\em all} higher-order feature interactions without a combinatorial parameter explosion. SPAM outperforms all current interpretable approaches, and matches DNN/XGBoost performance on a series of real-world benchmarks with up to hundreds of thousands of features. We demonstrate by human subject evaluations that SPAMs are demonstrably more interpretable in practice, and are hence an effortless replacement for DNNs for creating interpretable and high-performance systems suitable for large-scale machine learning. Source code is available at https://github.com/facebookresearch/nbm-spam.","accessed":{"date-parts":[["2022",6,14]]},"author":[{"family":"Dubey","given":"Abhimanyu"},{"family":"Radenovic","given":"Filip"},{"family":"Mahajan","given":"Dhruv"}],"citation-key":"dubeyScalableInterpretabilityPolynomials2022","DOI":"10.48550/arXiv.2205.14108","issued":{"date-parts":[["2022",6,8]]},"number":"arXiv:2205.14108","publisher":"arXiv","source":"arXiv.org","title":"Scalable Interpretability via Polynomials","type":"article","URL":"http://arxiv.org/abs/2205.14108"},
  {"id":"EfficiencyStudyEstimators","accessed":{"date-parts":[["2023",9,4]]},"citation-key":"EfficiencyStudyEstimators","title":"Efficiency Study of Estimators for a Treatment Effect in a Pretest–Posttest Trial | Scholars Portal Journals","type":"webpage","URL":"https://journals-scholarsportal-info.proxy1.lib.uwo.ca/details/00031305/v55i0004/314_esoefateiapt.xml"},
  {"id":"efronBetterBootstrapConfidence1987","abstract":"We consider the problem of setting approximate confidence intervals for a single parameter θ in a multiparameter family. The standard approximate intervals based on maximum likelihood theory, θ̂ ± σ̂z<sup>(α)</sup>, can be quite misleading. In practice, tricks based on transformations, bias corrections, and so forth, are often used to improve their accuracy. The bootstrap confidence intervals discussed in this article automatically incorporate such tricks without requiring the statistician to think them through for each new application, at the price of a considerable increase in computational effort. The new intervals incorporate an improvement over previously suggested methods, which results in second-order correctness in a wide variety of problems. In addition to parametric families, bootstrap intervals are also developed for nonparametric situations.","accessed":{"date-parts":[["2023",8,12]]},"author":[{"family":"Efron","given":"Bradley"}],"citation-key":"efronBetterBootstrapConfidence1987","container-title":"Journal of the American Statistical Association","DOI":"10.2307/2289144","ISSN":"0162-1459","issue":"397","issued":{"date-parts":[["1987"]]},"page":"171-185","publisher":"[American Statistical Association, Taylor & Francis, Ltd.]","source":"JSTOR","title":"Better Bootstrap Confidence Intervals","type":"article-journal","URL":"https://www.jstor.org/stable/2289144","volume":"82"},
  {"id":"faderHowProjectCustomer2007","abstract":"At the heart of any contractual or subscription‐oriented business model is the notion of the retention rate. An important managerial task is to take a series of past retention numbers for a given group of customers and project them into the future to make more accurate predictions about customer tenure, lifetime value, and so on. As an alternative to common “curve‐fitting” regression models, we develop and demonstrate a probability model with a well‐grounded “story” for the churn process. We show that our basic model (known as a “shifted‐beta‐geometric”) can be implemented in a simple Microsoft Excel spreadsheet and provides remarkably accurate forecasts and other useful diagnostics about customer retention.We provide a detailed appendix covering the implementation details and offer additional pointers to other related models.","accessed":{"date-parts":[["2023",11,20]]},"author":[{"family":"Fader","given":"Peter S."},{"family":"Hardie","given":"Bruce G. S."}],"citation-key":"faderHowProjectCustomer2007","container-title":"Journal of Interactive Marketing","container-title-short":"Journal of Interactive Marketing","DOI":"10.1002/dir.20074","ISSN":"1094-9968","issue":"1","issued":{"date-parts":[["2007"]]},"page":"76-90","publisher":"Wiley Subscription Services, Inc., A Wiley Company","source":"Scholars Portal Journals","title":"How to project customer retention","type":"article-journal","URL":"https://journals.scholarsportal.info/details/10949968/v21i0001/76_htpcr.xml","volume":"21"},
  {"id":"feitTestRollProfitMaximizing2019","abstract":"Marketers often use A/B testing as a tool to compare marketing treatments in a test stage and then deploy the better-performing treatment to the remainder of the consumer population. While these tests have traditionally been analyzed using hypothesis testing, we re-frame them as an explicit trade-off between the opportunity cost of the test (where some customers receive a sub-optimal treatment) and the potential losses associated with deploying a sub-optimal treatment to the remainder of the population. We derive a closed-form expression for the profit-maximizing test size and show that it is substantially smaller than typically recommended for a hypothesis test, particularly when the response is noisy or when the total population is small. The common practice of using small holdout groups can be rationalized by asymmetric priors. The proposed test design achieves nearly the same expected regret as the flexible, yet harder-to-implement multi-armed bandit under a wide range of conditions. We demonstrate the benefits of the method in three different marketing contexts -- website design, display advertising and catalog tests -- in which we estimate priors from past data. In all three cases, the optimal sample sizes are substantially smaller than for a traditional hypothesis test, resulting in higher profit.","accessed":{"date-parts":[["2022",5,30]]},"author":[{"family":"Feit","given":"Elea McDonnell"},{"family":"Berman","given":"Ron"}],"citation-key":"feitTestRollProfitMaximizing2019","container-title":"Marketing Science","container-title-short":"Marketing Science","DOI":"10.1287/mksc.2019.1194","ISSN":"0732-2399, 1526-548X","issue":"6","issued":{"date-parts":[["2019",11]]},"page":"1038-1058","source":"arXiv.org","title":"Test & Roll: Profit-Maximizing A/B Tests","title-short":"Test & Roll","type":"article-journal","URL":"http://arxiv.org/abs/1811.00457","volume":"38"},
  {"id":"fikselIntroductionConductingInterim","abstract":"In confirmatory (phase III) clinical trials, the goal is to test whether an experimental treatment is superior to a standard of care or placebo treatment. In designing the trial, health authorities require that type I error is strictly controlled–that is, if the experimental treatment is truly not better than the...","accessed":{"date-parts":[["2022",7,6]]},"author":[{"family":"Fiksel","given":"Jacob"}],"citation-key":"fikselIntroductionConductingInterim","language":"en","title":"Introduction to Conducting Interim Analyses Using Alpha Spending","type":"webpage","URL":"http://jfiksel.github.io/2021-02-03-alpha_spending_explained/"},
  {"id":"freedmanRandomizationDoesNot2008","abstract":"The logit model is often used to analyze experimental data. However, randomization does not justify the model, so the usual estimators can be inconsistent. A consistent estimator is proposed. Neyman’s non-parametric setup is used as a benchmark. In this setup, each subject has two potential responses, one if treated and the other if untreated; only one of the two responses can be observed. Beside the mathematics, there are simulation results, a brief review of the literature, and some recommendations for practice.","accessed":{"date-parts":[["2025",3,9]]},"author":[{"family":"Freedman","given":"David A."}],"citation-key":"freedmanRandomizationDoesNot2008","container-title":"Statistical Science","DOI":"10.1214/08-STS262","ISSN":"0883-4237, 2168-8745","issue":"2","issued":{"date-parts":[["2008",5]]},"page":"237-249","publisher":"Institute of Mathematical Statistics","source":"Project Euclid","title":"Randomization Does Not Justify Logistic Regression","type":"article-journal","URL":"https://projecteuclid.org/journals/statistical-science/volume-23/issue-2/Randomization-Does-Not-Justify-Logistic-Regression/10.1214/08-STS262.full","volume":"23"},
  {"id":"garthwaiteConfidenceIntervalsRandomization1996","abstract":"An efficient method of constructing confidence intervals that correspond to randomization tests is described. The method is based on the Robbins-Monro search process and conducts a separate search for each endpoint of the confidence interval. Each search requires only slightly more permutations than the number needed for a randomization test. Diagnostics to monitor searches are suggested and the method is illustrated with examples.","accessed":{"date-parts":[["2024",12,18]]},"author":[{"family":"Garthwaite","given":"Paul H."}],"citation-key":"garthwaiteConfidenceIntervalsRandomization1996","container-title":"Biometrics","DOI":"10.2307/2532852","ISSN":"0006-341X","issue":"4","issued":{"date-parts":[["1996"]]},"page":"1387-1393","publisher":"[Wiley, International Biometric Society]","source":"JSTOR","title":"Confidence Intervals from Randomization Tests","type":"article-journal","URL":"https://www.jstor.org/stable/2532852","volume":"52"},
  {"id":"gelmanBayesianWorkflow2020","abstract":"The Bayesian approach to data analysis provides a powerful way to handle uncertainty in all observations, model parameters, and model structure using probability theory. Probabilistic programming languages make it easier to specify and fit Bayesian models, but this still leaves us with many options regarding constructing, evaluating, and using these models, along with many remaining challenges in computation. Using Bayesian inference to solve real-world problems requires not only statistical skills, subject matter knowledge, and programming, but also awareness of the decisions made in the process of data analysis. All of these aspects can be understood as part of a tangled workflow of applied Bayesian statistics. Beyond inference, the workflow also includes iterative model building, model checking, validation and troubleshooting of computational problems, model understanding, and model comparison. We review all these aspects of workflow in the context of several examples, keeping in mind that in practice we will be fitting many models for any given problem, even if only a subset of them will ultimately be relevant for our conclusions.","accessed":{"date-parts":[["2022",6,20]]},"author":[{"family":"Gelman","given":"Andrew"},{"family":"Vehtari","given":"Aki"},{"family":"Simpson","given":"Daniel"},{"family":"Margossian","given":"Charles C."},{"family":"Carpenter","given":"Bob"},{"family":"Yao","given":"Yuling"},{"family":"Kennedy","given":"Lauren"},{"family":"Gabry","given":"Jonah"},{"family":"Bürkner","given":"Paul-Christian"},{"family":"Modrák","given":"Martin"}],"citation-key":"gelmanBayesianWorkflow2020","issued":{"date-parts":[["2020",11,3]]},"number":"arXiv:2011.01808","publisher":"arXiv","source":"arXiv.org","title":"Bayesian Workflow","type":"article","URL":"http://arxiv.org/abs/2011.01808"},
  {"id":"gelmanHowStatisticalChallenges2024","abstract":"Given the well-known problems of replicability, how is it that researchers at respected institutions continue to publish and publicize studies that are fatally flawed in the sense of not providing evidence to support their strong claims? We argue that two general problems are (a) difficulties of analyzing data with multilevel structure and (b) misinterpretation of the literature. We demonstrate with the example of a recently published claim that altering patients’ subjective perception of time can have a notable effect on physical healing. We discuss ways of avoiding or at least reducing such problems, including comparing final results with simpler analyses, moving away from shot-in-the-dark phenomenological studies, and more carefully examining previous published claims. Making incorrect choices in multilevel modeling is just one way that things can go wrong, but this example also provides a window into more general problems with complicated designs, cutting-edge statistical methods, and the connections between substantive theory, experimental design, data collection, and replication.","accessed":{"date-parts":[["2025",2,26]]},"author":[{"family":"Gelman","given":"Andrew"},{"family":"Brown","given":"Nicholas J. L."}],"citation-key":"gelmanHowStatisticalChallenges2024","container-title":"Advances in Methods and Practices in Psychological Science","DOI":"10.1177/25152459241276398","ISSN":"2515-2459","issue":"4","issued":{"date-parts":[["2024",10,1]]},"language":"en","page":"25152459241276398","publisher":"SAGE Publications Inc","source":"SAGE Journals","title":"How Statistical Challenges and Misreadings of the Literature Combine to Produce Unreplicable Science: An Example From Psychology","title-short":"How Statistical Challenges and Misreadings of the Literature Combine to Produce Unreplicable Science","type":"article-journal","URL":"https://doi.org/10.1177/25152459241276398","volume":"7"},
  {"id":"gelmanInterrogatingCargoCult2025","abstract":"Over the past fifty years, the term “cargo cult” has been used to describe the actions of scientists who appear to follow forms of scientific inquiry but without the understanding and self-criticism that are essential to real scientific progress. The term has served a useful role by providing a short and catchy label to something that is otherwise difficult to explain. However, the term is also fraught with historical and cultural baggage and, in our opinion, encourages crossing of a subtle line between criticizing discipline methodological norms and criticizing the individuals currently carrying out those norms as part of a complex and context dependent social process. We find that carefully interrogating the term itself holds some important lessons for improvement in the science reform movement.","accessed":{"date-parts":[["2025",7,13]]},"author":[{"family":"Gelman","given":"Andrew"},{"family":"Higgs","given":"Megan"}],"citation-key":"gelmanInterrogatingCargoCult2025","container-title":"Theory and Society","container-title-short":"Theory and Society","DOI":"10.1007/s11186-025-09614-6","ISSN":"0304-2421","issue":"2","issued":{"date-parts":[["2025"]]},"page":"197-207","publisher":"Springer Netherlands","source":"Scholars Portal Journals","title":"Interrogating the “cargo cult science” metaphor","type":"article-journal","URL":"https://journals.scholarsportal.info/details/03042421/v54i0002/197_itcsm.xml","volume":"54"},
  {"id":"gibertDevelopmentBayesianEstimation2022","abstract":"Apixaban is a direct oral anticoagulant (DOAC). Many studies have shown that it shows high pharmacokinetic interindividual and intraindividual variability (IIV). The risk of hemorrhage is a major concern for patients treated with apixaban to undergo an operation or an invasive procedure. Due to this large pharmacokinetic variability, the current recommendations concerning the optimal duration of apixaban discontinuation before a high-bleeding risk procedure concern the general population and not a specific patient. The aim of this study was (1) to investigate by simulation the distribution of decay time of apixaban concentration and (2) to develop and validate an easy-to-use web tool to estimate the individual decay time of apixaban in a “real-life” situation. A systematic review of the literature was conducted to select the relevant pharmacokinetic models for the creation of the web tool. For each model, pharmacokinetic profiles were simulated and the time to reach concentrations below the threshold of 30 ng/ml (T30) was calculated. One of the selected models was chosen to perform a Bayesian estimation and predict the optimal duration of apixaban discontinuation before a high-bleeding risk procedure. All these results were concatenated into the PrevBleed application developed with the R Shiny package.","accessed":{"date-parts":[["2022",5,28]]},"author":[{"family":"Gibert","given":"Audrick"},{"family":"Lanoiselée","given":"Julien"},{"family":"Janisset","given":"Luc"},{"family":"Pernod","given":"Gilles"},{"family":"Ollier","given":"Edouard"},{"family":"Delavenne","given":"Xavier"}],"citation-key":"gibertDevelopmentBayesianEstimation2022","container-title":"Fundamental & Clinical Pharmacology","DOI":"10.1111/fcp.12770","ISSN":"1472-8206","issue":"n/a","issued":{"date-parts":[["2022",2,22]]},"language":"en","source":"Wiley Online Library","title":"Development of a Bayesian estimation tool to determine the optimal duration of apixaban discontinuation before a high-bleeding risk procedure","type":"article-journal","URL":"http://onlinelibrary.wiley.com/doi/abs/10.1111/fcp.12770","volume":"n/a"},
  {"id":"greiferMatchingMethodsConfounder2021","abstract":"Propensity score weighting and outcome regression are popular ways to adjust for observed confounders in epidemiologic research. Here, we provide an introduction to matching methods, which serve the same purpose but can offer advantages in robustness and performance. A key difference between matching and weighting methods is that matching methods do not directly rely on the propensity score and so are less sensitive to its misspecification or to the presence of extreme values. Matching methods offer many options for customization, which allow a researcher to incorporate substantive knowledge and carefully manage bias/variance trade-offs in estimating the effects of nonrandomized exposures. We review these options and their implications, provide guidance for their use, and compare matching methods with weighting methods. Because of their potential advantages over other methods, matching methods should have their place in an epidemiologist’s methodological toolbox.","accessed":{"date-parts":[["2024",3,31]]},"author":[{"family":"Greifer","given":"Noah"},{"family":"Stuart","given":"Elizabeth A"}],"citation-key":"greiferMatchingMethodsConfounder2021","container-title":"Epidemiologic Reviews","container-title-short":"Epidemiologic Reviews","DOI":"10.1093/epirev/mxab003","ISSN":"1478-6729","issue":"1","issued":{"date-parts":[["2021",12,30]]},"page":"118-129","source":"Silverchair","title":"Matching Methods for Confounder Adjustment: An Addition to the Epidemiologist’s Toolbox","title-short":"Matching Methods for Confounder Adjustment","type":"article-journal","URL":"https://doi.org/10.1093/epirev/mxab003","volume":"43"},
  {"id":"gulilatAssociationSexStroke2022","abstract":"Background\nEvidence from clinical trials suggests a differential effect of sex on the effectiveness and safety of direct oral anticoagulants (DOACs) for stroke prophylaxis in atrial fibrillation (AF).\nMethods\nThis population-based cohort study examined the independent effect of sex on hemorrhage and ischemic stroke in 23,884 patients (55% females; age ≥ 66 years) with AF starting apixaban or rivaroxaban treatment in Ontario, Canada. Patients were followed for 90 days after their DOAC prescription. Using female sex as the exposure of interest, differences in baseline characteristics were balanced between sexes using inverse probability weights based on propensity scores. Applying weighted modified Poisson regression, risk ratios (RRs) were estimated for major hemorrhage, ischemic stroke/systemic embolism/transient ischemic attack (hereafter stroke), myocardial infarction, and all-cause mortality, with males as a reference.\nResults\nFemales were older, had higher predicted stroke risk (based on CHADS2 score), and had fewer comorbidities than did males. Males had a higher prevalence of coronary artery disease, diabetes, and cancer, and similar predicted bleeding risk (based on HAS-BLED score). After weighting, baseline characteristics were well balanced. The 90-day risks for hemorrhage (RR 0.96; 95% confidence interval [CI] 0.80-1.15; P = 0.69) and stroke (RR 1.01; 95% CI 0.86-1.19; P = 0.94) were similar between sexes, which remained true when assessing each DOAC separately by dosing regimen. Compared to males, females had a lower risk for myocardial infarction (RR 0.66; 95% CI 0.52-0.84; P = 0.0008), and for all-cause mortality (RR 0.76; 95% CI 0.67-0.87; P < 0.0001).\nConclusions\nOur findings do not suggest an association of sex with the 90-day risk of hemorrhage or ischemic stroke in older AF patients prescribed apixaban or rivaroxaban.\nRésumé\nContexte\nLes données probantes issues des essais cliniques donnent à penser que l’efficacité et l’innocuité des anticoagulants oraux directs (AOD) utilisés pour la prophylaxie des accidents vasculaires cérébraux (AVC) dans un contexte de fibrillation auriculaire (FA) varient selon le sexe du patient.\nMéthodologie\nCette étude de cohorte populationnelle a examiné l’effet indépendant du sexe sur l’hémorragie et l’AVC ischémique chez 23 884 patients (55 % de femmes; âge ≥ 66 ans) atteints de FA ayant amorcé un traitement par l’apixaban ou le rivaroxaban en Ontario (Canada). Les patients ont été suivis pendant 90 jours après avoir reçu une ordonnance d’AOD. Le sexe féminin constituant l’exposition d’intérêt, les différences quant aux caractéristiques initiales ont été réparties de façon équilibrée entre les sexes au moyen d’une pondération par probabilité inverse reposant sur le score de propension. La régression de Poisson modifiée avec pondération a servi à estimer les rapports de risques (RR) d’hémorragie majeure, d’AVC ischémique/d’embolie systémique/d’accident ischémique transitoire (ci-après AVC), d’infarctus du myocarde et de mortalité toutes causes confondues, les hommes formant la population de référence.\nRésultats\nLes femmes étaient plus âgées, présentaient un risque prévu d’AVC plus élevé (d’après le score CHADS2) et présentaient moins de maladies concomitantes que les hommes. Les hommes présentaient une prévalence plus élevée de coronaropathie, de diabète et de cancer, et un risque prévu d’hémorragie similaire (compte tenu du score HAS-BLED). Après la pondération, la répartition des caractéristiques initiales des patients était bien équilibrée. Le risque d’hémorragie (RR : 0,96; intervalle de confiance [IC] à 95 % : 0,80-1,15; P = 0,69) et d’AVC (RR : 1,01; IC à 95 % : 0,86-1,19; P = 0,94) sur 90 jours était similaire pour les deux sexes, et il en était de même lorsque chaque AOD a été évalué séparément en fonction du schéma posologique. Par rapport aux hommes, les femmes présentaient un risque plus faible d’infarctus du myocarde (RR : 0,66; IC à 95 % : 0,52-0,84; P = 0,0008) et de mortalité toutes causes confondues (RR : 0,76; IC à 95 % : 0,67-0,87; P < 0,0001).\nConclusions\nNos résultats ne laissent présumer aucune association entre le sexe et le risque d’hémorragie ou d’AVC ischémique sur une période de 90 jours chez les patients âgés atteints de FA à qui l’apixaban ou le rivaroxaban sont prescrits.","accessed":{"date-parts":[["2022",5,28]]},"author":[{"family":"Gulilat","given":"Markus"},{"family":"Jandoc","given":"Racquel"},{"family":"Jeyakumar","given":"Nivethika"},{"family":"McArthur","given":"Eric"},{"family":"Garg","given":"Amit X."},{"family":"Kim","given":"Richard B."},{"family":"Tirona","given":"Rommel G."},{"family":"Schwarz","given":"Ute I."}],"citation-key":"gulilatAssociationSexStroke2022","container-title":"CJC Open","container-title-short":"CJC Open","DOI":"10.1016/j.cjco.2021.09.002","ISSN":"2589-790X","issue":"1","issued":{"date-parts":[["2022",1,1]]},"language":"en","page":"56-64","source":"ScienceDirect","title":"Association of Sex With Stroke and Bleeding Risk of Apixaban and Rivaroxaban in Elderly Atrial Fibrillation Patients Using Propensity Score Weights","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S2589790X21002353","volume":"4"},
  {"id":"gulilatDrugInteractionsPharmacogenetic2020","abstract":"Factor Xa-inhibitor apixaban is an oral anticoagulant prescribed in atrial fibrillation (AF) for stroke prevention. Its pharmacokinetic profile is known to be affected by cytochrome P450 (CYP)3A metabolism, while it is also a substrate of the efflux transporters ATP-binding cassette (ABC)B1 (P-glycoprotein) and ABCG2 (breast cancer resistance protein, BCRP). In this study, we assessed the impact of interacting medication and pharmacogenetic variation to better explain apixaban concentration differences among 358 Caucasian AF patients. Genotyping (ABCG2, ABCB1, CYP3A4*22, CYP3A5*3) was performed by TaqMan assays, and apixaban quantified by mass spectrometry. The typical patient was on average 77.2 years old, 85.5 kg, and had a serum creatinine of 103.1 µmol/L. Concomitant amiodarone, an antiarrhythmic agent and moderate CYP3A/ABCB1 inhibitor, the impaired-function variant ABCG2 c.421C > A, and sex predicted higher apixaban concentrations when controlling for age, weight and serum creatinine (multivariate regression; R2 = 0.34). Our findings suggest that amiodarone and ABCG2 genotype contribute to interpatient apixaban variability beyond known clinical factors.","accessed":{"date-parts":[["2022",5,28]]},"author":[{"family":"Gulilat","given":"Markus"},{"family":"Keller","given":"Denise"},{"family":"Linton","given":"Bradley"},{"family":"Pananos","given":"A. Demetri"},{"family":"Lizotte","given":"Daniel"},{"family":"Dresser","given":"George K."},{"family":"Alfonsi","given":"Jeffrey"},{"family":"Tirona","given":"Rommel G."},{"family":"Kim","given":"Richard B."},{"family":"Schwarz","given":"Ute I."}],"citation-key":"gulilatDrugInteractionsPharmacogenetic2020","container-title":"Journal of Thrombosis and Thrombolysis","container-title-short":"J Thromb Thrombolysis","DOI":"10.1007/s11239-019-01962-2","ISSN":"1573-742X","issue":"2","issued":{"date-parts":[["2020",2,1]]},"language":"en","page":"294-303","source":"Springer Link","title":"Drug interactions and pharmacogenetic factors contribute to variation in apixaban concentration in atrial fibrillation patients in routine care","type":"article-journal","URL":"https://doi.org/10.1007/s11239-019-01962-2","volume":"49"},
  {"id":"guptaDevelopmentValidationISARIC2021","accessed":{"date-parts":[["2024",4,25]]},"author":[{"family":"Gupta","given":"Rishi K."},{"family":"Harrison","given":"Ewen M."},{"family":"Ho","given":"Antonia"},{"family":"Docherty","given":"Annemarie B."},{"family":"Knight","given":"Stephen R."},{"family":"Smeden","given":"Maarten","dropping-particle":"van"},{"family":"Abubakar","given":"Ibrahim"},{"family":"Lipman","given":"Marc"},{"family":"Quartagno","given":"Matteo"},{"family":"Pius","given":"Riinu"},{"family":"Buchan","given":"Iain"},{"family":"Carson","given":"Gail"},{"family":"Drake","given":"Thomas M."},{"family":"Dunning","given":"Jake"},{"family":"Fairfield","given":"Cameron J."},{"family":"Gamble","given":"Carrol"},{"family":"Green","given":"Christopher A."},{"family":"Halpin","given":"Sophie"},{"family":"Hardwick","given":"Hayley E."},{"family":"Holden","given":"Karl A."},{"family":"Horby","given":"Peter W."},{"family":"Jackson","given":"Clare"},{"family":"Mclean","given":"Kenneth A."},{"family":"Merson","given":"Laura"},{"family":"Nguyen-Van-Tam","given":"Jonathan S."},{"family":"Norman","given":"Lisa"},{"family":"Olliaro","given":"Piero L."},{"family":"Pritchard","given":"Mark G."},{"family":"Russell","given":"Clark D."},{"family":"Scott-Brown","given":"James"},{"family":"Shaw","given":"Catherine A."},{"family":"Sheikh","given":"Aziz"},{"family":"Solomon","given":"Tom"},{"family":"Sudlow","given":"Cathie"},{"family":"Swann","given":"Olivia V."},{"family":"Turtle","given":"Lance"},{"family":"Openshaw","given":"Peter J. M."},{"family":"Baillie","given":"J. Kenneth"},{"family":"Semple","given":"Malcolm G."},{"family":"Noursadeghi","given":"Mahdad"},{"family":"Baillie","given":"J. Kenneth"},{"family":"Semple","given":"Malcolm G."},{"family":"Openshaw","given":"Peter JM"},{"family":"Carson","given":"Gail"},{"family":"Alex","given":"Beatrice"},{"family":"Bach","given":"Benjamin"},{"family":"Barclay","given":"Wendy S."},{"family":"Bogaert","given":"Debby"},{"family":"Chand","given":"Meera"},{"family":"Cooke","given":"Graham S."},{"family":"Docherty","given":"Annemarie B."},{"family":"Dunning","given":"Jake"},{"family":"Filipe","given":"Ana da Silva"},{"family":"Fletcher","given":"Tom"},{"family":"Green","given":"Christopher A."},{"family":"Harrison","given":"Ewen M."},{"family":"Hiscox","given":"Julian A."},{"family":"Ho","given":"Antonia Ying Wai"},{"family":"Horby","given":"Peter W."},{"family":"Ijaz","given":"Samreen"},{"family":"Khoo","given":"Saye"},{"family":"Klenerman","given":"Paul"},{"family":"Law","given":"Andrew"},{"family":"Lim","given":"Wei Shen"},{"family":"Mentzer","given":"Alexander J."},{"family":"Merson","given":"Laura"},{"family":"Meynert","given":"Alison M."},{"family":"Noursadeghi","given":"Mahdad"},{"family":"Moore","given":"Shona C."},{"family":"Palmarini","given":"Massimo"},{"family":"Paxton","given":"William A."},{"family":"Pollakis","given":"Georgios"},{"family":"Price","given":"Nicholas"},{"family":"Rambaut","given":"Andrew"},{"family":"Robertson","given":"David L."},{"family":"Russell","given":"Clark D."},{"family":"Sancho-Shimizu","given":"Vanessa"},{"family":"Scott","given":"Janet T."},{"family":"Silva","given":"Thushan","dropping-particle":"de"},{"family":"Sigfrid","given":"Louise"},{"family":"Solomon","given":"Tom"},{"family":"Sriskandan","given":"Shiranee"},{"family":"Stuart","given":"David"},{"family":"Summers","given":"Charlotte"},{"family":"Tedder","given":"Richard S."},{"family":"Thomson","given":"Emma C."},{"family":"Thompson","given":"AA Roger"},{"family":"Thwaites","given":"Ryan S."},{"family":"Turtle","given":"Lance CW"},{"family":"Zambon","given":"Maria"},{"family":"Hardwick","given":"Hayley"},{"family":"Donohue","given":"Chloe"},{"family":"Lyons","given":"Ruth"},{"family":"Griffiths","given":"Fiona"},{"family":"Oosthuyzen","given":"Wilna"},{"family":"Norman","given":"Lisa"},{"family":"Pius","given":"Riinu"},{"family":"Drake","given":"Tom M."},{"family":"Fairfield","given":"Cameron J."},{"family":"Knight","given":"Stephen"},{"family":"Mclean","given":"Kenneth A."},{"family":"Murphy","given":"Derek"},{"family":"Shaw","given":"Catherine A."},{"family":"Dalton","given":"Jo"},{"family":"Lee","given":"James"},{"family":"Plotkin","given":"Daniel"},{"family":"Girvan","given":"Michelle"},{"family":"Mullaney","given":"Scott"},{"family":"Petersen","given":"Claire"},{"family":"Saviciute","given":"Egle"},{"family":"Roberts","given":"Stephanie"},{"family":"Harrison","given":"Janet"},{"family":"Marsh","given":"Laura"},{"family":"Connor","given":"Marie"},{"family":"Halpin","given":"Sophie"},{"family":"Jackson","given":"Clare"},{"family":"Gamble","given":"Carrol"},{"family":"Leeming","given":"Gary"},{"family":"Law","given":"Andrew"},{"family":"Wham","given":"Murray"},{"family":"Clohisey","given":"Sara"},{"family":"Hendry","given":"Ross"},{"family":"Scott-Brown","given":"James"},{"family":"Greenhalf","given":"William"},{"family":"Shaw","given":"Victoria"},{"family":"McDonald","given":"Sarah"},{"family":"Keating","given":"Seán"},{"family":"Ahmed","given":"Katie A."},{"family":"Armstrong","given":"Jane A."},{"family":"Ashworth","given":"Milton"},{"family":"Asiimwe","given":"Innocent G."},{"family":"Bakshi","given":"Siddharth"},{"family":"Barlow","given":"Samantha L."},{"family":"Booth","given":"Laura"},{"family":"Brennan","given":"Benjamin"},{"family":"Bullock","given":"Katie"},{"family":"Catterall","given":"Benjamin WA"},{"family":"Clark","given":"Jordan J."},{"family":"Clarke","given":"Emily A."},{"family":"Cole","given":"Sarah"},{"family":"Cooper","given":"Louise"},{"family":"Cox","given":"Helen"},{"family":"Davis","given":"Christopher"},{"family":"Dincarslan","given":"Oslem"},{"family":"Dunn","given":"Chris"},{"family":"Dyer","given":"Philip"},{"family":"Elliott","given":"Angela"},{"family":"Evans","given":"Anthony"},{"family":"Finch","given":"Lorna"},{"family":"Fisher","given":"Lewis WS"},{"family":"Foster","given":"Terry"},{"family":"Garcia-Dorival","given":"Isabel"},{"family":"Greenhalf","given":"Willliam"},{"family":"Gunning","given":"Philip"},{"family":"Hartley","given":"Catherine"},{"family":"Ho","given":"Antonia"},{"family":"Jensen","given":"Rebecca L."},{"family":"Jones","given":"Christopher B."},{"family":"Jones","given":"Trevor R."},{"family":"Khandaker","given":"Shadia"},{"family":"King","given":"Katharine"},{"family":"Kiy","given":"Robyn T."},{"family":"Koukorava","given":"Chrysa"},{"family":"Lake","given":"Annette"},{"family":"Lant","given":"Suzannah"},{"family":"Latawiec","given":"Diane"},{"family":"Lavelle-Langham","given":"L."},{"family":"Lefteri","given":"Daniella"},{"family":"Lett","given":"Lauren"},{"family":"Livoti","given":"Lucia A."},{"family":"Mancini","given":"Maria"},{"family":"McDonald","given":"Sarah"},{"family":"McEvoy","given":"Laurence"},{"family":"McLauchlan","given":"John"},{"family":"Metelmann","given":"Soeren"},{"family":"Miah","given":"Nahida S."},{"family":"Middleton","given":"Joanna"},{"family":"Mitchell","given":"Joyce"},{"family":"Moore","given":"Shona C."},{"family":"Murphy","given":"Ellen G."},{"family":"Penrice-Randal","given":"Rebekah"},{"family":"Pilgrim","given":"Jack"},{"family":"Prince","given":"Tessa"},{"family":"Reynolds","given":"Will"},{"family":"Ridley","given":"P. Matthew"},{"family":"Sales","given":"Debby"},{"family":"Shaw","given":"Victoria E."},{"family":"Shears","given":"Rebecca K."},{"family":"Small","given":"Benjamin"},{"family":"Subramaniam","given":"Krishanthi S."},{"family":"Szemiel","given":"Agnieska"},{"family":"Taggart","given":"Aislynn"},{"family":"Tanianis-Hughes","given":"Jolanta"},{"family":"Thomas","given":"Jordan"},{"family":"Trochu","given":"Erwan"},{"family":"Tonder","given":"Libby","dropping-particle":"van"},{"family":"Wilcock","given":"Eve"},{"family":"Zhang","given":"J. Eunice"},{"family":"Adeniji","given":"Kayode"},{"family":"Agranoff","given":"Daniel"},{"family":"Agwuh","given":"Ken"},{"family":"Ail","given":"Dhiraj"},{"family":"Alegria","given":"Ana"},{"family":"Angus","given":"Brian"},{"family":"Ashish","given":"Abdul"},{"family":"Atkinson","given":"Dougal"},{"family":"Bari","given":"Shahedal"},{"family":"Barlow","given":"Gavin"},{"family":"Barnass","given":"Stella"},{"family":"Barrett","given":"Nicholas"},{"family":"Bassford","given":"Christopher"},{"family":"Baxter","given":"David"},{"family":"Beadsworth","given":"Michael"},{"family":"Bernatoniene","given":"Jolanta"},{"family":"Berridge","given":"John"},{"family":"Best","given":"Nicola"},{"family":"Bothma","given":"Pieter"},{"family":"Brealey","given":"David"},{"family":"Brittain-Long","given":"Robin"},{"family":"Bulteel","given":"Naomi"},{"family":"Burden","given":"Tom"},{"family":"Burtenshaw","given":"Andrew"},{"family":"Caruth","given":"Vikki"},{"family":"Chadwick","given":"David"},{"family":"Chambler","given":"Duncan"},{"family":"Chee","given":"Nigel"},{"family":"Child","given":"Jenny"},{"family":"Chukkambotla","given":"Srikanth"},{"family":"Clark","given":"Tom"},{"family":"Collini","given":"Paul"},{"family":"Cosgrove","given":"Catherine"},{"family":"Cupitt","given":"Jason"},{"family":"Cutino-Moguel","given":"Maria-Teresa"},{"family":"Dark","given":"Paul"},{"family":"Dawson","given":"Chris"},{"family":"Dervisevic","given":"Samir"},{"family":"Donnison","given":"Phil"},{"family":"Douthwaite","given":"Sam"},{"family":"DuRand","given":"Ingrid"},{"family":"Dushianthan","given":"Ahilanadan"},{"family":"Dyer","given":"Tristan"},{"family":"Evans","given":"Cariad"},{"family":"Eziefula","given":"Chi"},{"family":"Fegan","given":"Chrisopher"},{"family":"Finn","given":"Adam"},{"family":"Fullerton","given":"Duncan"},{"family":"Garg","given":"Sanjeev"},{"family":"Garg","given":"Atul"},{"family":"Gkrania-Klotsas","given":"Effrossyni"},{"family":"Godden","given":"Jo"},{"family":"Goldsmith","given":"Arthur"},{"family":"Graham","given":"Clive"},{"family":"Hardy","given":"Elaine"},{"family":"Hartshorn","given":"Stuart"},{"family":"Harvey","given":"Daniel"},{"family":"Havalda","given":"Peter"},{"family":"Hawcutt","given":"Daniel B."},{"family":"Hobrok","given":"Maria"},{"family":"Hodgson","given":"Luke"},{"family":"Hormis","given":"Anil"},{"family":"Jacobs","given":"Michael"},{"family":"Jain","given":"Susan"},{"family":"Jennings","given":"Paul"},{"family":"Kaliappan","given":"Agilan"},{"family":"Kasipandian","given":"Vidya"},{"family":"Kegg","given":"Stephen"},{"family":"Kelsey","given":"Michael"},{"family":"Kendall","given":"Jason"},{"family":"Kerrison","given":"Caroline"},{"family":"Kerslake","given":"Ian"},{"family":"Koch","given":"Oliver"},{"family":"Koduri","given":"Gouri"},{"family":"Koshy","given":"George"},{"family":"Laha","given":"Shondipon"},{"family":"Laird","given":"Steven"},{"family":"Larkin","given":"Susan"},{"family":"Leiner","given":"Tamas"},{"family":"Lillie","given":"Patrick"},{"family":"Limb","given":"James"},{"family":"Linnett","given":"Vanessa"},{"family":"Little","given":"Jeff"},{"family":"MacMahon","given":"Michael"},{"family":"MacNaughton","given":"Emily"},{"family":"Mankregod","given":"Ravish"},{"family":"Masson","given":"Huw"},{"family":"Matovu","given":"Elijah"},{"family":"McCullough","given":"Katherine"},{"family":"McEwen","given":"Ruth"},{"family":"Meda","given":"Manjula"},{"family":"Mills","given":"Gary"},{"family":"Minton","given":"Jane"},{"family":"Mirfenderesky","given":"Mariyam"},{"family":"Mohandas","given":"Kavya"},{"family":"Mok","given":"Quen"},{"family":"Moon","given":"James"},{"family":"Moore","given":"Elinoor"},{"family":"Morgan","given":"Patrick"},{"family":"Morris","given":"Craig"},{"family":"Mortimore","given":"Katherine"},{"family":"Moses","given":"Samuel"},{"family":"Mpenge","given":"Mbiye"},{"family":"Mulla","given":"Rohinton"},{"family":"Murphy","given":"Michael"},{"family":"Nagel","given":"Megan"},{"family":"Nagarajan","given":"Thapas"},{"family":"Nelson","given":"Mark"},{"family":"Otahal","given":"Igor"},{"family":"Pais","given":"Mark"},{"family":"Panchatsharam","given":"Selva"},{"family":"Paraiso","given":"Hassan"},{"family":"Patel","given":"Brij"},{"family":"Pattison","given":"Natalie"},{"family":"Pepperell","given":"Justin"},{"family":"Peters","given":"Mark"},{"family":"Phull","given":"Mandeep"},{"family":"Pintus","given":"Stefania"},{"family":"Pooni","given":"Jagtur Singh"},{"family":"Post","given":"Frank"},{"family":"Price","given":"David"},{"family":"Prout","given":"Rachel"},{"family":"Rae","given":"Nikolas"},{"family":"Reschreiter","given":"Henrik"},{"family":"Reynolds","given":"Tim"},{"family":"Richardson","given":"Neil"},{"family":"Roberts","given":"Mark"},{"family":"Roberts","given":"Devender"},{"family":"Rose","given":"Alistair"},{"family":"Rousseau","given":"Guy"},{"family":"Ryan","given":"Brendan"},{"family":"Saluja","given":"Taranprit"},{"family":"Shah","given":"Aarti"},{"family":"Shanmuga","given":"Prad"},{"family":"Sharma","given":"Anil"},{"family":"Shawcross","given":"Anna"},{"family":"Sizer","given":"Jeremy"},{"family":"Shankar-Hari","given":"Manu"},{"family":"Smith","given":"Richard"},{"family":"Snelson","given":"Catherine"},{"family":"Spittle","given":"Nick"},{"family":"Staines","given":"Nikki"},{"family":"Stambach","given":"Tom"},{"family":"Stewart","given":"Richard"},{"family":"Subudhi","given":"Pradeep"},{"family":"Szakmany","given":"Tamas"},{"family":"Tatham","given":"Kate"},{"family":"Thomas","given":"Jo"},{"family":"Thompson","given":"Chris"},{"family":"Thompson","given":"Robert"},{"family":"Tridente","given":"Ascanio"},{"family":"Tupper-Carey","given":"Darell"},{"family":"Twagira","given":"Mary"},{"family":"Ustianowski","given":"Andrew"},{"family":"Vallotton","given":"Nick"},{"family":"Vincent-Smith","given":"Lisa"},{"family":"Visuvanathan","given":"Shico"},{"family":"Vuylsteke","given":"Alan"},{"family":"Waddy","given":"Sam"},{"family":"Wake","given":"Rachel"},{"family":"Walden","given":"Andrew"},{"family":"Welters","given":"Ingeborg"},{"family":"Whitehouse","given":"Tony"},{"family":"Whittaker","given":"Paul"},{"family":"Whittington","given":"Ashley"},{"family":"Wijesinghe","given":"Meme"},{"family":"Williams","given":"Martin"},{"family":"Wilson","given":"Lawrence"},{"family":"Wilson","given":"Sarah"},{"family":"Winchester","given":"Stephen"},{"family":"Wiselka","given":"Martin"},{"family":"Wolverson","given":"Adam"},{"family":"Wooton","given":"Daniel G."},{"family":"Workman","given":"Andrew"},{"family":"Yates","given":"Bryan"},{"family":"Young","given":"Peter"}],"citation-key":"guptaDevelopmentValidationISARIC2021","container-title":"The Lancet Respiratory Medicine","container-title-short":"The Lancet Respiratory Medicine","DOI":"10.1016/S2213-2600(20)30559-2","ISSN":"2213-2600, 2213-2619","issue":"4","issued":{"date-parts":[["2021",4,1]]},"language":"English","page":"349-359","PMID":"33444539","publisher":"Elsevier","source":"www.thelancet.com","title":"Development and validation of the ISARIC 4C Deterioration model for adults hospitalised with COVID-19: a prospective cohort study","title-short":"Development and validation of the ISARIC 4C Deterioration model for adults hospitalised with COVID-19","type":"article-journal","URL":"https://www.thelancet.com/journals/lanres/article/PIIS2213-2600(20)30559-2/fulltext","volume":"9"},
  {"id":"hazlettUnderstandingAvoidingWeights2024","abstract":"Researchers in many fields endeavor to estimate treatment effects by regressing outcome data (Y) on a treatment (D) and observed confounders (X). Even absent unobserved confounding, the regression coefficient on the treatment reports a weighted average of strata-specific treatment effects (Angrist, 1998). Where heterogeneous treatment effects cannot be ruled out, the resulting coefficient is thus not generally equal to the average treatment effect (ATE), and is unlikely to be the quantity of direct scientific or policy interest. The difference between the coefficient and the ATE has led researchers to propose various interpretational, bounding, and diagnostic aids (Humphreys, 2009; Aronow and Samii, 2016; Sloczynski, 2022; Chattopadhyay and Zubizarreta, 2023). We note that the linear regression of Y on D and X can be misspecified when the treatment effect is heterogeneous in X. The \"weights of regression\", for which we provide a new (more general) expression, simply characterize how the OLS coefficient will depart from the ATE under the misspecification resulting from unmodeled treatment effect heterogeneity. Consequently, a natural alternative to suffering these weights is to address the misspecification that gives rise to them. For investigators committed to linear approaches, we propose relying on the slightly weaker assumption that the potential outcomes are linear in X. Numerous well-known estimators are unbiased for the ATE under this assumption, namely regression-imputation/g-computation/T-learner, regression with an interaction of the treatment and covariates (Lin, 2013), and balancing weights. Any of these approaches avoid the apparent weighting problem of the misspecified linear regression, at an efficiency cost that will be small when there are few covariates relative to sample size. We demonstrate these lessons using simulations in observational and experimental settings.","accessed":{"date-parts":[["2024",8,22]]},"author":[{"family":"Hazlett","given":"Chad"},{"family":"Shinkre","given":"Tanvi"}],"citation-key":"hazlettUnderstandingAvoidingWeights2024","DOI":"10.48550/arXiv.2403.03299","issued":{"date-parts":[["2024",3,5]]},"number":"arXiv:2403.03299","publisher":"arXiv","source":"arXiv.org","title":"Understanding and avoiding the \"weights of regression\": Heterogeneous effects, misspecification, and longstanding solutions","title-short":"Understanding and avoiding the \"weights of regression\"","type":"article","URL":"http://arxiv.org/abs/2403.03299"},
  {"id":"heapsEnforcingStationarityPrior2022","abstract":"Stationarity is a very common assumption in time series analysis. A vector autoregressive process is stationary if and only if the roots of its characteristic equation lie outside the unit circle, constraining the autoregressive coefficient matrices to lie in the stationary region. However, the stationary region has a highly complex geometry which impedes specification of a prior distribution. In this work, an unconstrained reparameterization of a stationary vector autoregression is presented. The new parameters are partial autocorrelation matrices, which are interpretable, and can be transformed bijectively to the space of unconstrained square matrices through a simple mapping of their singular values. This transformation preserves various structural forms of the partial autocorrelation matrices and readily facilitates specification of a prior. Properties of this prior are described along with an important special case which is exchangeable with respect to the order of the elements in the observation vector. Posterior inference and computation are described and implemented using Hamiltonian Monte Carlo via Stan. The prior and inferential procedures are illustrated with an application to a macroeconomic time series which highlights the benefits of enforcing stationarity and encouraging shrinkage towards a sensible parametric structure. Supplementary materials for this article are available online.","accessed":{"date-parts":[["2022",5,23]]},"author":[{"family":"Heaps","given":"Sarah E."}],"citation-key":"heapsEnforcingStationarityPrior2022","container-title":"Journal of Computational and Graphical Statistics","DOI":"10.1080/10618600.2022.2079648","ISSN":"1061-8600","issue":"ja","issued":{"date-parts":[["2022",5,23]]},"page":"1-24","source":"Taylor and Francis+NEJM","title":"Enforcing stationarity through the prior in vector autoregressions","type":"article-journal","URL":"https://doi.org/10.1080/10618600.2022.2079648","volume":"0"},
  {"id":"heathDeterminingBayesianPredictive2020","abstract":"Background/Aims\nNon-inferiority trials investigate whether a novel intervention, which typically has other benefits (i.e., cheaper or safer), has similar clinical effectiveness to currently available treatments. In situations where interim evidence in a non-inferiority trial suggests that the novel treatment is truly inferior, ethical concerns with continuing randomisation to the “inferior” intervention are raised. Thus, if interim data indicate that concluding non-inferiority at the end of the trial is unlikely, stopping for futility should be considered. To date, limited examples are available to guide the development of stopping rules for non-inferiority trials.\nMethods\nWe used a Bayesian predictive power approach to develop a stopping rule for futility for a trial collecting binary outcomes. We evaluated the frequentist operating characteristics of the stopping rule to ensure control of the Type I and Type II error. Our case study is the Intranasal Ketamine for Procedural Sedation trial (INK trial), a non-inferiority trial designed to assess the sedative properties of ketamine administered using two alternative routes.\nResults\nWe considered implementing our stopping rule after the INK trial enrols 140 patients out of 560. The trial would be stopped if 12 more patients experience a failure on the novel treatment compared to standard care. This trial has a type I error rate of 2.2% and a power of 80%.\nConclusions\nStopping for futility in non-inferiority trials reduces exposure to ineffective treatments and preserves resources for alternative research questions. Futility stopping rules based on Bayesian predictive power are easy to implement and align with trial aims.\nTrial registration\nClinicalTrials.gov NCT02828566 July 11, 2016.","accessed":{"date-parts":[["2023",12,16]]},"author":[{"family":"Heath","given":"Anna"},{"family":"Offringa","given":"Martin"},{"family":"Pechlivanoglou","given":"Petros"},{"family":"Rios","given":"Juan David"},{"family":"Klassen","given":"Terry P."},{"family":"Poonai","given":"Naveen"},{"family":"Pullenayegum","given":"Eleanor"}],"citation-key":"heathDeterminingBayesianPredictive2020","container-title":"Contemporary Clinical Trials Communications","container-title-short":"Contemporary Clinical Trials Communications","DOI":"10.1016/j.conctc.2020.100561","ISSN":"2451-8654","issued":{"date-parts":[["2020",6,1]]},"page":"100561","source":"ScienceDirect","title":"Determining a Bayesian predictive power stopping rule for futility in a non-inferiority trial with binary outcomes","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S2451865420300454","volume":"18"},
  {"id":"hendersonRegretRegressionOptimalDynamic2010","abstract":"We consider optimal dynamic treatment regime determination in practice. Model building, checking, and comparison have had little or no attention so far in this literature. Motivated by an application on optimal dosage of anticoagulants, we propose a modeling and estimation strategy that incorporates the regret functions of Murphy (2003, Journal of the Royal Statistical Society, Series B 65, 331–366) into a regression model for observed responses. Estimation is quick and diagnostics are available, meaning a variety of candidate models can be compared. The method is illustrated using simulation and the anticoagulation application.","accessed":{"date-parts":[["2022",5,21]]},"author":[{"family":"Henderson","given":"Robin"},{"family":"Ansell","given":"Phil"},{"family":"Alshibani","given":"Deyadeen"}],"citation-key":"hendersonRegretRegressionOptimalDynamic2010","container-title":"Biometrics","DOI":"10.1111/j.1541-0420.2009.01368.x","ISSN":"1541-0420","issue":"4","issued":{"date-parts":[["2010"]]},"language":"en","page":"1192-1201","source":"Wiley Online Library","title":"Regret-Regression for Optimal Dynamic Treatment Regimes","type":"article-journal","URL":"http://onlinelibrary.wiley.com/doi/abs/10.1111/j.1541-0420.2009.01368.x","volume":"66"},
  {"id":"huaPersonalizedDynamicTreatment2021","abstract":"Accurate models of clinical actions and their impacts on disease progression are critical for estimating personalized optimal dynamic treatment regimes (DTRs) in medical/health research, especially in managing chronic conditions. Traditional statistical methods for DTRs usually focus on estimating the optimal treatment or dosage at each given medical intervention, but overlook the important question of \"when this intervention should happen.\" We fill this gap by developing a two-step Bayesian approach to optimize clinical decisions with timing. In the first step, we build a generative model for a sequence of medical interventions-which are discrete events in continuous time-with a marked temporal point process (MTPP) where the mark is the assigned treatment or dosage. Then this clinical action model is embedded into a Bayesian joint framework where the other components model clinical observations including longitudinal medical measurements and time-to-event data conditional on treatment histories. In the second step, we propose a policy gradient method to learn the personalized optimal clinical decision that maximizes the patient survival by interacting the MTPP with the model on clinical observations while accounting for uncertainties in clinical observations learned from the posterior inference of the Bayesian joint model in the first step. A signature application of the proposed approach is to schedule follow-up visitations and assign a dosage at each visitation for patients after kidney transplantation. We evaluate our approach with comparison to alternative methods on both simulated and real-world datasets. In our experiments, the personalized decisions made by the proposed method are clinically useful: they are interpretable and successfully help improve patient survival.","accessed":{"date-parts":[["2022",5,19]]},"author":[{"family":"Hua","given":"William"},{"family":"Mei","given":"Hongyuan"},{"family":"Zohar","given":"Sarah"},{"family":"Giral","given":"Magali"},{"family":"Xu","given":"Yanxun"}],"citation-key":"huaPersonalizedDynamicTreatment2021","issued":{"date-parts":[["2021",2,18]]},"number":"arXiv:2007.04155","publisher":"arXiv","source":"arXiv.org","title":"Personalized Dynamic Treatment Regimes in Continuous Time: A Bayesian Approach for Optimizing Clinical Decisions with Timing","title-short":"Personalized Dynamic Treatment Regimes in Continuous Time","type":"article","URL":"http://arxiv.org/abs/2007.04155"},
  {"id":"Inbox677Demetripananoszapiercom","accessed":{"date-parts":[["2023",6,30]]},"citation-key":"Inbox677Demetripananoszapiercom","title":"Inbox (677) - demetri.pananos@zapier.com - Zapier Mail","type":"webpage","URL":"https://mail.google.com/mail/u/0/#inbox"},
  {"id":"ironsCausallySoundPriors2024","abstract":"We introduce the BREASE framework for the Bayesian analysis of randomized controlled trials with a binary treatment and a binary outcome. Approaching the problem from a causal inference perspective, we propose parameterizing the likelihood in terms of the baselinerisk, efficacy, and adverse side effects of the treatment, along with a flexible, yet intuitive and tractable jointly independent beta prior distribution on these parameters, which we show to be a generalization of the Dirichlet prior for the joint distribution of potential outcomes. Our approach has a number of desirable characteristics when compared to current mainstream alternatives: (i) it naturally induces prior dependence between expected outcomes in the treatment and control groups; (ii) as the baseline risk, efficacy and risk of adverse side effects are quantities commonly present in the clinicians' vocabulary, the hyperparameters of the prior are directly interpretable, thus facilitating the elicitation of prior knowledge and sensitivity analysis; and (iii) we provide analytical formulae for the marginal likelihood, Bayes factor, and other posterior quantities, as well as an exact posterior sampling algorithm and an accurate and fast data-augmented Gibbs sampler in cases where traditional MCMC fails. Empirical examples demonstrate the utility of our methods for estimation, hypothesis testing, and sensitivity analysis of treatment effects.","accessed":{"date-parts":[["2025",1,16]]},"author":[{"family":"Irons","given":"Nicholas J."},{"family":"Cinelli","given":"Carlos"}],"citation-key":"ironsCausallySoundPriors2024","DOI":"10.48550/arXiv.2308.13713","issued":{"date-parts":[["2024",11,25]]},"number":"arXiv:2308.13713","publisher":"arXiv","source":"arXiv.org","title":"Causally Sound Priors for Binary Experiments","type":"article","URL":"http://arxiv.org/abs/2308.13713"},
  {"id":"johariExperimentalDesignTwoSided2021","abstract":"We develop an analytical framework to study experimental design in two-sided marketplaces. Many of these experiments exhibit interference, where an intervention applied to one market participant influences the behavior of another participant. This interference leads to biased estimates of the treatment effect of the intervention. We develop a stochastic market model and associated mean field limit to capture dynamics in such experiments, and use our model to investigate how the performance of different designs and estimators is affected by marketplace interference effects. Platforms typically use two common experimental designs: demand-side (\"customer\") randomization (CR) and supply-side (\"listing\") randomization (LR), along with their associated estimators. We show that good experimental design depends on market balance: in highly demand-constrained markets, CR is unbiased, while LR is biased; conversely, in highly supply-constrained markets, LR is unbiased, while CR is biased. We also introduce and study a novel experimental design based on two-sided randomization (TSR) where both customers and listings are randomized to treatment and control. We show that appropriate choices of TSR designs can be unbiased in both extremes of market balance, while yielding relatively low bias in intermediate regimes of market balance.","accessed":{"date-parts":[["2024",5,5]]},"author":[{"family":"Johari","given":"Ramesh"},{"family":"Li","given":"Hannah"},{"family":"Liskovich","given":"Inessa"},{"family":"Weintraub","given":"Gabriel"}],"citation-key":"johariExperimentalDesignTwoSided2021","DOI":"10.48550/arXiv.2002.05670","issued":{"date-parts":[["2021",9,26]]},"number":"arXiv:2002.05670","publisher":"arXiv","source":"arXiv.org","title":"Experimental Design in Two-Sided Platforms: An Analysis of Bias","title-short":"Experimental Design in Two-Sided Platforms","type":"article","URL":"http://arxiv.org/abs/2002.05670"},
  {"id":"kahanImproperAnalysisTrials2012","abstract":"Many clinical trials restrict randomisation using stratified blocks or minimisation to balance prognostic factors across treatment groups. It is widely acknowledged in the statistical literature that the subsequent analysis should reflect the design of the study, and any stratification or minimisation variables should be adjusted for in the analysis. However, a review of recent general medical literature showed only 14 of 41 eligible studies reported adjusting their primary analysis for stratification or minimisation variables. We show that balancing treatment groups using stratification leads to correlation between the treatment groups. If this correlation is ignored and an unadjusted analysis is performed, standard errors for the treatment effect will be biased upwards, resulting in 95% confidence intervals that are too wide, type I error rates that are too low and a reduction in power. Conversely, an adjusted analysis will give valid inference. We explore the extent of this issue using simulation for continuous, binary and time-to-event outcomes where treatment is allocated using stratified block randomisation or minimisation. Copyright © 2011 John Wiley & Sons, Ltd.","accessed":{"date-parts":[["2022",5,18]]},"author":[{"family":"Kahan","given":"Brennan C."},{"family":"Morris","given":"Tim P."}],"citation-key":"kahanImproperAnalysisTrials2012","container-title":"Statistics in Medicine","DOI":"10.1002/sim.4431","ISSN":"1097-0258","issue":"4","issued":{"date-parts":[["2012"]]},"language":"en","page":"328-340","source":"Wiley Online Library","title":"Improper analysis of trials randomised using stratified blocks or minimisation","type":"article-journal","URL":"https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.4431","volume":"31"},
  {"id":"KbSafari_compatibilityZotero","accessed":{"date-parts":[["2024",10,29]]},"citation-key":"KbSafari_compatibilityZotero","title":"kb:safari_compatibility [Zotero Documentation]","type":"webpage","URL":"https://www.zotero.org/support/kb/safari_compatibility"},
  {"id":"KbSafari_compatibilityZoteroa","accessed":{"date-parts":[["2024",10,29]]},"citation-key":"KbSafari_compatibilityZoteroa","title":"kb:safari_compatibility [Zotero Documentation]","type":"webpage","URL":"https://www.zotero.org/support/kb/safari_compatibility"},
  {"id":"kohaviFalsePositivesTests2024","abstract":"A/B tests, or online controlled experiments, are used heavily in the software industry to evaluate implementations of ideas, as the paradigm is the gold standard in science for establishing causality: the changes introduced in the treatment caused the changes to the metrics of interest with high probability. What distinguishes software experiments, or A/B tests, from experiments in many other domains is the scale (e.g., over 100 experiment treatments may launch on a given workday in large companies) and the effect sizes that matter to the business are small (e.g., a 3% improvement to conversion rate from a single experiment is a cause for celebration). The humbling reality is that most experiments fail to improve key metrics, and success rates of only about 10-20% are most common. With low success rates, the industry standard alpha threshold of 0.05 implies a high probability of false positives. We begin with motivation about why false positives are expensive in many software domains. We then offer several approaches to estimate the true success rate of experiments, given the observed \"win\" rate (statistically significant positive improvements), and show examples from Expedia and Optimizely. We offer a modified procedure for experimentation, based in sequential group testing, that selectively extends experiments to reduce false positives, increase power, at a small increase to runtime. We conclude with a discussion of the difference between ideas and experiments in practice, terms that are often incorrectly used interchangeably.","accessed":{"date-parts":[["2025",3,16]]},"author":[{"family":"Kohavi","given":"Ron"},{"family":"Chen","given":"Nanyu"}],"citation-key":"kohaviFalsePositivesTests2024","collection-title":"KDD '24","container-title":"Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining","DOI":"10.1145/3637528.3671631","event-place":"New York, NY, USA","ISBN":"979-8-4007-0490-1","issued":{"date-parts":[["2024",8,24]]},"page":"5240–5250","publisher":"Association for Computing Machinery","publisher-place":"New York, NY, USA","source":"ACM Digital Library","title":"False Positives in A/B Tests","type":"paper-conference","URL":"https://doi.org/10.1145/3637528.3671631"},
  {"id":"laberTreebasedMethodsIndividualized2015","abstract":"Individualized treatment rules recommend treatments on the basis of individual patient characteristics. A high-quality treatment rule can produce better patient outcomes, lower costs and less treatment burden. If a treatment rule learned from data is to be used to inform clinical practice or provide scientific insight, it is crucial that it be interpretable; clinicians may be unwilling to implement models they do not understand, and black-box models may not be useful for guiding future research. The canonical example of an interpretable prediction model is a decision tree. We propose a method for estimating an optimal individualized treatment rule within the class of rules that are representable as decision trees. The class of rules we consider is interpretable but expressive. A novel feature of this problem is that the learning task is unsupervised, as the optimal treatment for each patient is unknown and must be estimated. The proposed method applies to both categorical and continuous treatments and produces favourable marginal mean outcomes in simulation experiments. We illustrate it using data from a study of major depressive disorder.","accessed":{"date-parts":[["2022",5,19]]},"author":[{"family":"Laber","given":"E. B."},{"family":"Zhao","given":"Y. Q."}],"citation-key":"laberTreebasedMethodsIndividualized2015","container-title":"Biometrika","container-title-short":"Biometrika","DOI":"10.1093/biomet/asv028","ISSN":"0006-3444","issue":"3","issued":{"date-parts":[["2015",9,1]]},"page":"501-514","source":"Silverchair","title":"Tree-based methods for individualized treatment regimes","type":"article-journal","URL":"https://doi.org/10.1093/biomet/asv028","volume":"102"},
  {"id":"larsenStatisticalChallengesOnline2022","abstract":"The rise of internet-based services and products in the late 1990's brought about an unprecedented opportunity for online businesses to engage in large scale data-driven decision making. Over the past two decades, organizations such as Airbnb, Alibaba, Amazon, Baidu, Booking.com, Alphabet's Google, LinkedIn, Lyft, Meta's Facebook, Microsoft, Netflix, Twitter, Uber, and Yandex have invested tremendous resources in online controlled experiments (OCEs) to assess the impact of innovation on their customers and businesses. Running OCEs at scale has presented a host of challenges requiring solutions from many domains. In this paper we review challenges that require new statistical methodologies to address them. In particular, we discuss the practice and culture of online experimentation, as well as its statistics literature, placing the current methodologies within their relevant statistical lineages and providing illustrative examples of OCE applications. Our goal is to raise academic statisticians' awareness of these new research opportunities to increase collaboration between academia and the online industry.","accessed":{"date-parts":[["2022",12,30]]},"author":[{"family":"Larsen","given":"Nicholas"},{"family":"Stallrich","given":"Jonathan"},{"family":"Sengupta","given":"Srijan"},{"family":"Deng","given":"Alex"},{"family":"Kohavi","given":"Ron"},{"family":"Stevens","given":"Nathaniel"}],"citation-key":"larsenStatisticalChallengesOnline2022","DOI":"10.48550/arXiv.2212.11366","issued":{"date-parts":[["2022",12,21]]},"number":"arXiv:2212.11366","source":"arXiv.org","title":"Statistical Challenges in Online Controlled Experiments: A Review of A/B Testing Methodology","title-short":"Statistical Challenges in Online Controlled Experiments","type":"article","URL":"http://arxiv.org/abs/2212.11366"},
  {"id":"lesaffreDesignAnalysisSplitmouth2009","abstract":"The split-mouth design is a popular design in oral health research. In the most common split-mouth study, each of two treatments are randomly assigned to either the right or left halves of the dentition. The attractiveness of the design is that it removes a lot of inter-individual variability from the estimates of the treatment effect. However, already about 20 years ago the pitfalls of the design have been reported in the oral health literature. Yet, many clinicians are not aware of the potential problems with the split-mouth design. Further, it is our experience that most statisticians are not even aware of the existence of this design. Since most of the critical remarks appeared in the oral health literature, we argue that it is necessary to introduce the split-mouth design to a statistical audience, so that both clinicians and statisticians clearly understand the advantages, limitations, statistical considerations, and implications of its use in clinical trials and advise them on its use in practice. Copyright © 2009 John Wiley & Sons, Ltd.","accessed":{"date-parts":[["2023",3,13]]},"author":[{"family":"Lesaffre","given":"Emmanuel"},{"family":"Philstrom","given":"Bruce"},{"family":"Needleman","given":"Ian"},{"family":"Worthington","given":"Helen"}],"citation-key":"lesaffreDesignAnalysisSplitmouth2009","container-title":"Statistics in Medicine","container-title-short":"Statistics in Medicine","DOI":"10.1002/sim.3634","ISSN":"0277-6715","issue":"28","issued":{"date-parts":[["2009"]]},"page":"3470-3482","publisher":"John Wiley & Sons, Ltd.","source":"Scholars Portal Journals","title":"The design and analysis of split-mouth studies: What statisticians and clinicians should know","title-short":"The design and analysis of split-mouth studies","type":"article-journal","URL":"https://journals.scholarsportal.info/details/02776715/v28i0028/3470_tdaaoswsacsk.xml","volume":"28"},
  {"id":"leskoFrameworkDescriptiveEpidemiology2022","abstract":"Abstract\n            In this paper, we propose a framework for thinking through the design and conduct of descriptive epidemiologic studies. A well-defined descriptive question aims to quantify and characterize some feature of the health of a population and must clearly state: 1) the target population, characterized by person and place, and anchored in time; 2) the outcome, event, or health state or characteristic; and 3) the measure of occurrence that will be used to summarize the outcome (e.g., incidence, prevalence, average time to event, etc.). Additionally, 4) any auxiliary variables will be prespecified and their roles as stratification factors (to characterize the outcome distribution) or nuisance variables (to be standardized over) will be stated. We illustrate application of this framework to describe the prevalence of viral suppression on December 31, 2019, among people living with human immunodeficiency virus (HIV) who had been linked to HIV care in the United States. Application of this framework highlights biases that may arise from missing data, especially 1) differences between the target population and the analytical sample; 2) measurement error; 3) competing events, late entries, loss to follow-up, and inappropriate interpretation of the chosen measure of outcome occurrence; and 4) inappropriate adjustment.","accessed":{"date-parts":[["2025",7,5]]},"author":[{"family":"Lesko","given":"Catherine R"},{"family":"Fox","given":"Matthew P"},{"family":"Edwards","given":"Jessie K"}],"citation-key":"leskoFrameworkDescriptiveEpidemiology2022","container-title":"American Journal of Epidemiology","DOI":"10.1093/aje/kwac115","ISSN":"0002-9262, 1476-6256","issue":"12","issued":{"date-parts":[["2022",11,19]]},"language":"en","license":"https://academic.oup.com/journals/pages/open_access/funder_policies/chorus/standard_publication_model","page":"2063-2070","source":"DOI.org (Crossref)","title":"A Framework for Descriptive Epidemiology","type":"article-journal","URL":"https://academic.oup.com/aje/article/191/12/2063/6623869","volume":"191"},
  {"id":"liGNetRecurrentNetwork2021","abstract":"Counterfactual prediction is a fundamental task in decision-making. This paper introduces G-Net, a sequential deep learning framework for counterfactual  prediction under dynamic time-varying treatment strategies in complex longitudinal settings. G-Net is based upon g-computation, a causal inference method for estimating effects of general dynamic treatment strategies. Past g-computation implementations have mostly been built using classical regression models. G-Net instead adopts a recurrent neural network framework to capture complex temporal and nonlinear dependencies in the data. To our knowledge, G-Net is the first g-computation based deep sequential modeling framework that provides estimates of treatment effects under \\em{dynamic} and \\em{time-varying} treatment strategies. We evaluate G-Net using simulated longitudinal data from two sources: CVSim, a mechanistic model of the cardiovascular system, and a pharmacokinetic simulation of tumor growth. G-Net outperforms both classical and state-of-the-art counterfactual prediction models in these settings.","accessed":{"date-parts":[["2022",5,19]]},"author":[{"family":"Li","given":"Rui"},{"family":"Hu","given":"Stephanie"},{"family":"Lu","given":"Mingyu"},{"family":"Utsumi","given":"Yuria"},{"family":"Chakraborty","given":"Prithwish"},{"family":"Sow","given":"Daby M."},{"family":"Madan","given":"Piyush"},{"family":"Li","given":"Jun"},{"family":"Ghalwash","given":"Mohamed"},{"family":"Shahn","given":"Zach"},{"family":"Lehman","given":"Li-wei"}],"citation-key":"liGNetRecurrentNetwork2021","container-title":"Proceedings of Machine Learning for Health","event-title":"Machine Learning for Health","ISSN":"2640-3498","issued":{"date-parts":[["2021",11,28]]},"language":"en","page":"282-299","publisher":"PMLR","source":"proceedings.mlr.press","title":"G-Net: a Recurrent Network Approach to G-Computation for Counterfactual Prediction Under a Dynamic Treatment Regime","title-short":"G-Net","type":"paper-conference","URL":"https://proceedings.mlr.press/v158/li21a.html"},
  {"id":"lindonRapidRegressionDetection2022","abstract":"The practice of continuous deployment has enabled companies to reduce time-to-market by increasing the rate at which software can be deployed. However, deploying more frequently bears the risk that occasionally defective changes are released. For Internet companies, this has the potential to degrade the user experience and increase user abandonment. Therefore, quality control gates are an important component of the software delivery process. These are used to build confidence in the reliability of a release or change. Towards this end, a common approach is to perform a canary test to evaluate new software under production workloads. Detecting defects as early as possible is necessary to reduce exposure and to provide immediate feedback to the developer. We present a statistical framework for rapidly detecting regressions in software deployments. Our approach is based on sequential tests of stochastic order and of equality in distribution. This enables canary tests to be continuously monitored, permitting regressions to be rapidly detected while strictly controlling the false detection probability throughout. The utility of this approach is demonstrated based on two case studies at Netflix.","accessed":{"date-parts":[["2025",9,18]]},"author":[{"family":"Lindon","given":"Michael"},{"family":"Sanden","given":"Chris"},{"family":"Shirikian","given":"Vaché"}],"citation-key":"lindonRapidRegressionDetection2022","container-title":"Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining","DOI":"10.1145/3534678.3539099","issued":{"date-parts":[["2022",8,14]]},"page":"3336-3346","source":"arXiv.org","title":"Rapid Regression Detection in Software Deployments through Sequential Testing","type":"paper-conference","URL":"http://arxiv.org/abs/2205.14762"},
  {"id":"liUtilityApproachIndividualized2020","abstract":"In many settings, including oncology, increasing the dose of treatment results in both increased efficacy and toxicity. With the increasing availability of validated biomarkers and prediction models, there is the potential for individualized dosing based on patient specific factors. We consider the setting where there is an existing dataset of patients treated with heterogenous doses and including binary efficacy and toxicity outcomes and patient factors such as clinical features and biomarkers. The goal is to analyze the data to estimate an optimal dose for each (future) patient based on their clinical features and biomarkers. We propose an optimal individualized dose finding rule by maximizing utility functions for individual patients while limiting the rate of toxicity. The utility is defined as a weighted combination of efficacy and toxicity probabilities. This approach maximizes overall efficacy at a prespecified constraint on overall toxicity. We model the binary efficacy and toxicity outcomes using logistic regression with dose, biomarkers and dose–biomarker interactions. To incorporate the large number of potential parameters, we use the LASSO method. We additionally constrain the dose effect to be non-negative for both efficacy and toxicity for all patients. Simulation studies show that the utility approach combined with any of the modeling methods can improve efficacy without increasing toxicity relative to fixed dosing. The proposed methods are illustrated using a dataset of patients with lung cancer treated with radiation therapy.","accessed":{"date-parts":[["2022",5,19]]},"author":[{"family":"Li","given":"Pin"},{"family":"Taylor","given":"Jeremy M.G."},{"family":"Kong","given":"Spring"},{"family":"Jolly","given":"Shruti"},{"family":"Schipper","given":"Matthew J."}],"citation-key":"liUtilityApproachIndividualized2020","container-title":"Biometrical Journal","DOI":"10.1002/bimj.201900030","ISSN":"1521-4036","issue":"2","issued":{"date-parts":[["2020"]]},"language":"en","page":"386-397","source":"Wiley Online Library","title":"A utility approach to individualized optimal dose selection using biomarkers","type":"article-journal","URL":"http://onlinelibrary.wiley.com/doi/abs/10.1002/bimj.201900030","volume":"62"},
  {"id":"lumleyImportanceNormalityAssumption2002","abstract":"It is widely but incorrectly believed that the t-test and linear regression are valid only for Normally distributed outcomes. The t-test and linear regression compare the mean of an outcome variable for different subjects. While these are valid even in very small samples if the outcome variable is Normally distributed, their major usefulness comes from the fact that in large samples they are valid for any distribution. We demonstrate this validity by simulation in extremely non-Normal data. We discuss situations in which in other methods such as the Wilcoxon rank sum test and ordinal logistic regression (proportional odds model) have been recommended, and conclude that the t-test and linear regression often provide a convenient and practical alternative. The major limitation on the t-test and linear regression for inference about associations is not a distributional one, but whether detecting and estimating a difference in the mean of the outcome answers the scientific question at hand.","accessed":{"date-parts":[["2024",2,2]]},"author":[{"family":"Lumley","given":"Thomas"},{"family":"Diehr","given":"Paula"},{"family":"Emerson","given":"Scott"},{"family":"Chen","given":"Lu"}],"citation-key":"lumleyImportanceNormalityAssumption2002","container-title":"Annual Review of Public Health","DOI":"10.1146/annurev.publhealth.23.100901.140546","issue":"1","issued":{"date-parts":[["2002"]]},"page":"151-169","PMID":"11910059","source":"Annual Reviews","title":"The Importance of the Normality Assumption in Large Public Health Data Sets","type":"article-journal","URL":"https://doi.org/10.1146/annurev.publhealth.23.100901.140546","volume":"23"},
  {"id":"maharScopingReviewStudies2021","abstract":"Dynamic treatment regimens (DTRs) formalise the multi-stage and dynamic decision problems that clinicians often face when treating chronic or progressive medical conditions. Compared to randomised controlled trials, using observational data to optimise DTRs may allow a wider range of treatments to be evaluated at a lower cost. This review aimed to provide an overview of how DTRs are optimised with observational data in practice.","accessed":{"date-parts":[["2022",5,21]]},"author":[{"family":"Mahar","given":"Robert K."},{"family":"McGuinness","given":"Myra B."},{"family":"Chakraborty","given":"Bibhas"},{"family":"Carlin","given":"John B."},{"family":"IJzerman","given":"Maarten J."},{"family":"Simpson","given":"Julie A."}],"citation-key":"maharScopingReviewStudies2021","container-title":"BMC Medical Research Methodology","container-title-short":"BMC Medical Research Methodology","DOI":"10.1186/s12874-021-01211-2","ISSN":"1471-2288","issue":"1","issued":{"date-parts":[["2021",2,22]]},"page":"39","source":"BioMed Central","title":"A scoping review of studies using observational data to optimise dynamic treatment regimens","type":"article-journal","URL":"https://doi.org/10.1186/s12874-021-01211-2","volume":"21"},
  {"id":"mckennaPrecisionMedicineImprecise2018","abstract":"Medical oncology is in need of a mathematical modeling toolkit that can leverage clinically-available measurements to optimize treatment selection and schedules for patients. Just as the therapeutic choice has been optimized to match tumor genetics, the delivery of those therapeutics should be optimized based on patient-specific pharmacokinetic/pharmacodynamic properties. Under the current approach to treatment response planning and assessment, there does not exist an efficient method to consolidate biomarker changes into a holistic understanding of treatment response. While the majority of research on chemotherapies focus on cellular and genetic mechanisms of resistance, there are numerous patient-specific and tumor-specific measures that contribute to treatment response. New approaches that consolidate multimodal information into actionable data are needed. Mathematical modeling offers a solution to this problem. In this perspective, we first focus on the particular case of breast cancer to highlight how mathematical models have shaped the current approaches to treatment. Then we compare chemotherapy to radiation therapy. Finally, we identify opportunities to improve chemotherapy treatments using the model of radiation therapy. We posit that mathematical models can improve the application of anticancer therapeutics in the era of precision medicine. By highlighting a number of historical examples of the contributions of mathematical models to cancer therapy, we hope that this contribution serves to engage investigators who may not have previously considered how mathematical modeling can provide real insights into breast cancer therapy.","accessed":{"date-parts":[["2022",5,19]]},"author":[{"family":"McKenna","given":"Matthew T."},{"family":"Weis","given":"Jared A."},{"family":"Brock","given":"Amy"},{"family":"Quaranta","given":"Vito"},{"family":"Yankeelov","given":"Thomas E."}],"citation-key":"mckennaPrecisionMedicineImprecise2018","container-title":"Translational Oncology","container-title-short":"Translational Oncology","DOI":"10.1016/j.tranon.2018.03.009","ISSN":"1936-5233","issue":"3","issued":{"date-parts":[["2018",6,1]]},"language":"en","page":"732-742","source":"ScienceDirect","title":"Precision Medicine with Imprecise Therapy: Computational Modeling for Chemotherapy in Breast Cancer","title-short":"Precision Medicine with Imprecise Therapy","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S193652331830113X","volume":"11"},
  {"id":"mcneishUnnecessaryUbiquityHierarchical2017","abstract":"In psychology and the behavioral sciences generally, the use of the hierarchical linear model (HLM) and its extensions for discrete outcomes are popular methods for modeling clustered data. HLM and its discrete outcome extensions, however, are certainly not the only methods available to model clustered data. Although other methods exist and are widely implemented in other disciplines, it seems that psychologists have yet to consider these methods in substantive studies. This article compares and contrasts HLM with alternative methods including generalized estimating equations and cluster-robust standard errors. These alternative methods do not model random effects and thus make a smaller number of assumptions and are interpreted identically to single-level methods with the benefit that estimates are adjusted to reflect clustering of observations. Situations where these alternative methods may be advantageous are discussed including research questions where random effects are and are not required, when random effects can change the interpretation of regression coefficients, challenges of modeling with random effects with discrete outcomes, and examples of published psychology articles that use HLM that may have benefitted from using alternative methods. Illustrative examples are provided and discussed to demonstrate the advantages of the alternative methods and also when HLM would be the preferred method. (PsycINFO Database Record (c) 2017 APA, all rights reserved) (Source: journal abstract)","accessed":{"date-parts":[["2023",9,20]]},"author":[{"family":"McNeish","given":"Daniel"},{"family":"Stapleton","given":"Laura M."},{"family":"Link to external site","given":"this","dropping-particle":"link will open in a new window"},{"family":"Silverman","given":"Rebecca D."}],"citation-key":"mcneishUnnecessaryUbiquityHierarchical2017","container-title":"Psychological Methods","DOI":"10.1037/met0000078","ISSN":"1082-989X","issue":"1","issued":{"date-parts":[["2017",3]]},"language":"English","license":"© 2016, American Psychological Association","page":"114-140","source":"ProQuest","title":"On the unnecessary ubiquity of hierarchical linear modeling","type":"article-journal","URL":"https://www.proquest.com/docview/1786797662/abstract/8546E81E13004EC8PQ/1","volume":"22"},
  {"id":"Monolix2018R1User","accessed":{"date-parts":[["2022",6,4]]},"citation-key":"Monolix2018R1User","title":"Monolix 2018R1 User guide","type":"webpage","URL":"https://monolix.lixoft.com/single-page/"},
  {"id":"moodieDemystifyingOptimalDynamic2007","abstract":"A dynamic regime is a function that takes treatment and covariate history and baseline covariates as inputs and returns a decision to be made. Murphy (2003, Journal of the Royal Statistical Society, Series B65, 331–366) and Robins (2004, Proceedings of the Second Seattle Symposium on Biostatistics, 189–326) have proposed models and developed semiparametric methods for making inference about the optimal regime in a multi-interval trial that provide clear advantages over traditional parametric approaches. We show that Murphy's model is a special case of Robins's and that the methods are closely related but not equivalent. Interesting features of the methods are highlighted using the Multicenter AIDS Cohort Study and through simulation.","accessed":{"date-parts":[["2022",5,21]]},"author":[{"family":"Moodie","given":"Erica E. M."},{"family":"Richardson","given":"Thomas S."},{"family":"Stephens","given":"David A."}],"citation-key":"moodieDemystifyingOptimalDynamic2007","container-title":"Biometrics","DOI":"10.1111/j.1541-0420.2006.00686.x","ISSN":"1541-0420","issue":"2","issued":{"date-parts":[["2007"]]},"language":"en","page":"447-455","source":"Wiley Online Library","title":"Demystifying Optimal Dynamic Treatment Regimes","type":"article-journal","URL":"http://onlinelibrary.wiley.com/doi/abs/10.1111/j.1541-0420.2006.00686.x","volume":"63"},
  {"id":"morrisPlanningMethodCovariate2022","abstract":"It has long been advised to account for baseline covariates in the analysis of confirmatory randomised trials, with the main statistical justifications being that this increases power and, when a randomisation scheme balanced covariates, permits a valid estimate of experimental error. There are various methods available to account for covariates but it is not clear how to choose among them.","accessed":{"date-parts":[["2022",5,18]]},"author":[{"family":"Morris","given":"Tim P."},{"family":"Walker","given":"A. Sarah"},{"family":"Williamson","given":"Elizabeth J."},{"family":"White","given":"Ian R."}],"citation-key":"morrisPlanningMethodCovariate2022","container-title":"Trials","container-title-short":"Trials","DOI":"10.1186/s13063-022-06097-z","ISSN":"1745-6215","issue":"1","issued":{"date-parts":[["2022",4,18]]},"page":"328","source":"BioMed Central","title":"Planning a method for covariate adjustment in individually randomised trials: a practical guide","title-short":"Planning a method for covariate adjustment in individually randomised trials","type":"article-journal","URL":"https://doi.org/10.1186/s13063-022-06097-z","volume":"23"},
  {"id":"morsePersonalizedMedicineDream2015","abstract":"Personalized medicine is an emerging field with a goal of applying genomic information as a predictor of disease risk as well as individualization of drug therapy. For optimization of drug therapy, significant progress has been made in the past decade in linking genetic variation in genes associated with drug disposition to prediction of drug response and adverse reactions. For most drugs in clinical use, the interplay of many factors, including genetics, demographics, drug–drug interactions, disease states and the environment, result in the interindividual variability observed during drug therapy. Broadly speaking, such determinants of drug response are mediated through modulation of drug concentrations reflective of pharmacokinetic factors, as well as drug targets, often referred to as pharmacodynamics. It is clear that for personalized medicine to become clinically meaningful, genomic as well as clinical and environmental influences must be considered together. We show, for a number of drugs in clinical use, that genomics-guided treatment options not only are becoming feasible but are also on the cusp of showing superiority in terms of clinical outcomes as well as cost-benefit. One of the most widely studied drugs with regard to genomics-guided dosing options is the oral anticoagulant, warfarin. Genetic polymorphisms in the gene encoding cytochrome P450 2C9 (CYP2C9) and those in the target gene responsible for the warfarin anticoagulant effect, vitamin K epoxide reductase (VKORC1), account for much of the variability in the warfarin maintenance dose; however, routine genotyping in warfarin therapy remains controversial. We will outline the importance of understanding all of the variables that mediate warfarin response as the prerequisite to successful utilization of genotype-guided warfarin therapy. Similarly, HMG Co-A reductase inhibitors, commonly known as statins, also display wide interindividual variability in plasma concentration, response and toxicity due in part to polymorphisms in transporter genes, including SLCO1B1 and ABCG2. Genetic factors are also important considerations in treatment with other therapeutic agents discussed, including clopidogrel and tamoxifen. Implementation of personalized medicine-based treatment options for these and other drugs, the pharmacokinetics or pharmacodynamics of which are impacted by functional genetic variations, will require overcoming a number of challenges, including cost, turnaround time, and demonstration of clinical benefit, as well as better training of health care professionals about genomics in general, and pharmacogenomics in particular.","accessed":{"date-parts":[["2022",6,10]]},"author":[{"family":"Morse","given":"Bridget L."},{"family":"Kim","given":"Richard B."}],"citation-key":"morsePersonalizedMedicineDream2015","container-title":"Critical Reviews in Clinical Laboratory Sciences","DOI":"10.3109/10408363.2014.950407","ISSN":"1040-8363","issue":"1","issued":{"date-parts":[["2015",1,2]]},"page":"1-11","PMID":"25181036","publisher":"Taylor & Francis","source":"Taylor and Francis+NEJM","title":"Is personalized medicine a dream or a reality?","type":"article-journal","URL":"https://doi.org/10.3109/10408363.2014.950407","volume":"52"},
  {"id":"mouldBasicConceptsPopulation2013","abstract":"Population pharmacokinetic models are used to describe the time course of drug exposure in patients and to investigate sources of variability in patient exposure. They can be used to simulate alternative dose regimens, allowing for informed assessment of dose regimens before study conduct. This paper is the second in a three-part series, providing an introduction into methods for developing and evaluating population pharmacokinetic models. Example model files are available in the Supplementary Data online. CPT: Pharmacometrics & Systems Pharmacology (2013) 2, e38; doi:10.1038/psp.2013.14; advance online publication 17 April 2013","accessed":{"date-parts":[["2022",6,4]]},"author":[{"family":"Mould","given":"Dr"},{"family":"Upton","given":"Rn"}],"citation-key":"mouldBasicConceptsPopulation2013","container-title":"CPT: Pharmacometrics & Systems Pharmacology","DOI":"10.1038/psp.2013.14","ISSN":"2163-8306","issue":"4","issued":{"date-parts":[["2013"]]},"language":"en","page":"38","source":"Wiley Online Library","title":"Basic Concepts in Population Modeling, Simulation, and Model-Based Drug Development—Part 2: Introduction to Pharmacokinetic Modeling Methods","title-short":"Basic Concepts in Population Modeling, Simulation, and Model-Based Drug Development—Part 2","type":"article-journal","URL":"https://onlinelibrary.wiley.com/doi/abs/10.1038/psp.2013.14","volume":"2"},
  {"id":"muffMarginalConditionalRegression2016","abstract":"Correlated data are ubiquitous in ecological and evolutionary research, and appropriate statistical analysis requires that these correlations are taken into account. For regressions with correlated, non-normal outcomes, two main approaches are used: conditional and marginal modelling. The former leads to generalized linear mixed models (GLMMs), while the latter are estimated using generalized estimating equations (GEEs), or marginalized multilevel regression models. Differences, advantages and drawbacks of conditional and marginal models have been discussed extensively in the statistical and applied literature, and there is some agreement that the choice of the model must depend on the question under study. Yet, there still appears to be a lot of confusion and disagreement over when to choose which model. We start with a review of conditional and marginal models, and the differences in the interpretation of the resulting parameter estimates. We highlight that the two types of models propagate different linear relations between the covariates and the response. Moreover, while conditional models explicitly account for heterogeneity among clustered observations, marginal models yield averages over such heterogeneities and are therefore often interpreted as population-averaged models. We point out theoretically and with an example that when modelling non-normal outcomes no unambiguous definition of a marginal model generally exists. Instead, marginal model parameters are marginal only with respect to unaccounted differences among clusters and thus depend on the fixed effects in the model. Therefore, marginal model parameters should not be loosely interpreted as population-averaged parameters. In addition, we explain how marginal modelling is mathematically analogous to deliberately omitting covariates with explanatory power, and to deliberately introducing a Berkson measurement error into covariates. We also reiterate that marginal modelling is related to a well-known statistical phenomenon, the Simpson's paradox. In most cases, therefore, we regard the conditional model as the more powerful choice to explain how covariates are associated with a non-normal response. Still, marginal models can be useful, given that the scientific question explicitly requires such a model formulation.","accessed":{"date-parts":[["2022",5,18]]},"author":[{"family":"Muff","given":"Stefanie"},{"family":"Held","given":"Leonhard"},{"family":"Keller","given":"Lukas F."}],"citation-key":"muffMarginalConditionalRegression2016","container-title":"Methods in Ecology and Evolution","DOI":"10.1111/2041-210X.12623","ISSN":"2041-210X","issue":"12","issued":{"date-parts":[["2016"]]},"language":"en","page":"1514-1524","source":"Wiley Online Library","title":"Marginal or conditional regression models for correlated non-normal data?","type":"article-journal","URL":"https://onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.12623","volume":"7"},
  {"id":"murphyOptimalDynamicTreatment2003","abstract":"Summary. A dynamic treatment regime is a list of decision rules, one per time interval, for how the level of treatment will be tailored through time to an individual's changing status. The goal of this paper is to use experimental or observational data to estimate decision regimes that result in a maximal mean response. To explicate our objective and to state the assumptions, we use the potential outcomes model. The method proposed makes smooth parametric assumptions only on quantities that are directly relevant to the goal of estimating the optimal rules. We illustrate the methodology proposed via a small simulation.","accessed":{"date-parts":[["2022",5,22]]},"author":[{"family":"Murphy","given":"S. A."}],"citation-key":"murphyOptimalDynamicTreatment2003","container-title":"Journal of the Royal Statistical Society: Series B (Statistical Methodology)","DOI":"10.1111/1467-9868.00389","ISSN":"1467-9868","issue":"2","issued":{"date-parts":[["2003"]]},"language":"en","page":"331-355","source":"Wiley Online Library","title":"Optimal dynamic treatment regimes","type":"article-journal","URL":"https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-9868.00389","volume":"65"},
  {"id":"nealMCMCUsingHamiltonian2011","abstract":"Markov chain Monte Carlo (MCMC) originated with the classic paper of Metropolis et al.\n(1953), where it was used to simulate the distribution of states for a system of idealized\nmolecules. Not long after, another approach tomolecular simulationwas introduced (Alder\nand Wainwright, 1959), in which the motion of the molecules was deterministic, following\nNewton’s laws ofmotion,which have an elegant formalization asHamiltonian dynamics. For\nﬁnding the properties of bulk materials, these approaches are asymptotically equivalent,\nsince even in a deterministic simulation, each local region of the material experiences\neffectively random inﬂuences from distant regions. Despite the large overlap in their application areas, the MCMC and molecular dynamics approaches have continued to coexist in\nthe following decades (see Frenkel and Smit, 1996).\nIn 1987, a landmark paper by Duane, Kennedy, Pendleton, and Roweth united theMCMC and molecular dynamics approaches. They called their method “hybrid Monte\nCarlo,” which abbreviates to “HMC,” but the phrase “Hamiltonian Monte Carlo,” retaining the abbreviation, is more speciﬁc and descriptive, and I will use it here. Duane et al.\napplied HMC not to molecular simulation, but to lattice ﬁeld theory simulations of quantum chromodynamics. Statistical applications of HMC began with my use of it for neural\nnetwork models (Neal, 1996a). I also provided a statistically-oriented tutorial on HMC in a\nreview of MCMC methods (Neal, 1993, Chapter 5). There have been other applications\nof HMC to statistical problems (e.g. Ishwaran, 1999; Schmidt, 2009) and statisticallyoriented reviews (e.g. Liu, 2001, Chapter 9), but HMC still seems to be underappreciated\nby statisticians, and perhaps also by physicists outside the lattice ﬁeld theory community.\nThis review begins by describing Hamiltonian dynamics. Despite terminology that maybe unfamiliar outside physics, the features of Hamiltonian dynamics that are needed for\nHMC are elementary. The differential equations of Hamiltonian dynamics must be discretized for computer implementation. The “leapfrog” scheme that is typically used is\nquite simple.\nFollowing this introduction to Hamiltonian dynamics, I describe how to use it to con-struct an MCMCmethod. The ﬁrst step is to deﬁne a Hamiltonian function in terms of the\nprobability distribution we wish to sample from. In addition to the variables we are interested in (the “position” variables), we must introduce auxiliary “momentum” variables,\nwhich typically have independent Gaussian distributions. The HMC method alternates\nsimple updates for these momentum variables with Metropolis updates in which a new\nstate is proposed by computing a trajectory according to Hamiltonian dynamics, implemented with the leapfrog method. A state proposed in this way can be distant from thethe \nexploration of the state space that occurs whenMetropolis updates are done using a simple\nrandom-walkproposal distribution. (Analternativewayof avoiding randomwalks is touse\nshort trajectories but only partially replace the momentum variables between trajectories,\nso that successive trajectories tend to move in the same direction.)\nAfter presenting the basic HMCmethod, I discuss practical issues of tuning the leapfrogstepsize and number of leapfrog steps, as well as theoretical results on the scaling of HMC\nwith dimensionality. I then present a number of variations on HMC. The acceptance rate\nfor HMC can be increased for many problems by looking at “windows” of states at the\nbeginning and end of the trajectory. For many statistical problems, approximate computation of trajectories (e.g. using subsets of the data) may be beneﬁcial. Tuning of HMC can\nbe made easier using a “short-cut” in which trajectories computed with a bad choice of\nstepsize take little computation time. Finally, “tempering” methods may be useful when\nmultiple isolated modes exist.","author":[{"family":"Neal","given":"Radford M."}],"citation-key":"nealMCMCUsingHamiltonian2011","container-title":"Handbook of Markov Chain Monte Carlo","ISBN":"978-0-429-13850-8","issued":{"date-parts":[["2011"]]},"number-of-pages":"50","publisher":"Chapman and Hall/CRC","title":"MCMC Using Hamiltonian Dynamics","type":"chapter"},
  {"id":"negiRevisitingRegressionAdjustment2021","abstract":"In the context of random sampling, we show that linear full (separate) regression adjustment (FRA) on the control and treatment groups is, asymptotically, no less efficient than both the simple difference-in-means estimator and the pooled regression adjustment estimator; with heterogeneous treatment effects, FRA is usually strictly more efficient. We also propose a class of nonlinear regression adjustment estimators where consistency is ensured despite arbitrary misspecification of the conditional mean function. A simulation study confirms that nontrivial efficiency gains are possible with linear FRA, and that further gains are possible, even under severe mean misspecification, using nonlinear FRA.","accessed":{"date-parts":[["2025",3,13]]},"author":[{"family":"Negi","given":"Akanksha"},{"family":"Wooldridge","given":"Jeffrey M."}],"citation-key":"negiRevisitingRegressionAdjustment2021","container-title":"Econometric Reviews","DOI":"10.1080/07474938.2020.1824732","ISSN":"0747-4938","issue":"5","issued":{"date-parts":[["2021",4,26]]},"page":"504-534","publisher":"Taylor & Francis","source":"Taylor and Francis+NEJM","title":"Revisiting regression adjustment in experiments with heterogeneous treatment effects","type":"article-journal","URL":"https://doi.org/10.1080/07474938.2020.1824732","volume":"40"},
  {"id":"negiRevisitingRegressionAdjustment2021a","abstract":"In the context of random sampling, we show that linear full (separate) regression adjustment (FRA) on the control and treatment groups is, asymptotically, no less efficient than both the simple difference-in-means estimator and the pooled regression adjustment estimator; with heterogeneous treatment effects, FRA is usually strictly more efficient. We also propose a class of nonlinear regression adjustment estimators where consistency is ensured despite arbitrary misspecification of the conditional mean function. A simulation study confirms that nontrivial efficiency gains are possible with linear FRA, and that further gains are possible, even under severe mean misspecification, using nonlinear FRA.","accessed":{"date-parts":[["2025",3,13]]},"author":[{"family":"Negi","given":"Akanksha"},{"family":"Wooldridge","given":"Jeffrey M."}],"citation-key":"negiRevisitingRegressionAdjustment2021a","container-title":"Econometric Reviews","DOI":"10.1080/07474938.2020.1824732","ISSN":"0747-4938","issue":"5","issued":{"date-parts":[["2021",4,26]]},"page":"504-534","publisher":"Taylor & Francis","source":"Taylor and Francis+NEJM","title":"Revisiting regression adjustment in experiments with heterogeneous treatment effects","type":"article-journal","URL":"https://doi.org/10.1080/07474938.2020.1824732","volume":"40"},
  {"id":"niBayesianHierarchicalVaryingSparsity2019","abstract":"Identifying patient-specific prognostic biomarkers is of critical importance in developing personalized treatment for clinically and molecularly heterogeneous diseases such as cancer. In this article, we propose a novel regression framework, Bayesian hierarchical varying-sparsity regression (BEHAVIOR) models to select clinically relevant disease markers by integrating proteogenomic (proteomic+genomic) and clinical data. Our methods allow flexible modeling of protein–gene relationships as well as induces sparsity in both protein–gene and protein–survival relationships, to select genomically driven prognostic protein markers at the patient-level. Simulation studies demonstrate the superior performance of BEHAVIOR against competing method in terms of both protein marker selection and survival prediction. We apply BEHAVIOR to The Cancer Genome Atlas (TCGA) proteogenomic pan-cancer data and find several interesting prognostic proteins and pathways that are shared across multiple cancers and some that exclusively pertain to specific cancers. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available online.","accessed":{"date-parts":[["2022",5,28]]},"author":[{"family":"Ni","given":"Yang"},{"family":"Stingo","given":"Francesco C."},{"family":"Ha","given":"Min Jin"},{"family":"Akbani","given":"Rehan"},{"family":"Baladandayuthapani","given":"Veerabhadran"}],"citation-key":"niBayesianHierarchicalVaryingSparsity2019","container-title":"Journal of the American Statistical Association","DOI":"10.1080/01621459.2018.1434529","ISSN":"0162-1459","issue":"525","issued":{"date-parts":[["2019",1,2]]},"page":"48-60","PMID":"31178611","publisher":"Taylor & Francis","source":"Taylor and Francis+NEJM","title":"Bayesian Hierarchical Varying-Sparsity Regression Models with Application to Cancer Proteogenomics","type":"article-journal","URL":"https://doi.org/10.1080/01621459.2018.1434529","volume":"114"},
  {"id":"NONMEMTutorialPart","accessed":{"date-parts":[["2022",5,18]]},"citation-key":"NONMEMTutorialPart","title":"NONMEM Tutorial Part II: Estimation Methods and Advanced Examples","type":"webpage","URL":"https://ascpt.onlinelibrary.wiley.com/doi/epdf/10.1002/psp4.12422"},
  {"id":"nybergMethodsSoftwareTools2015","abstract":"Population pharmacokinetic (PK)–pharmacodynamic (PKPD) models are increasingly used in drug development and in academic research; hence, designing efficient studies is an important task. Following the first theoretical work on optimal design for nonlinear mixed-effects models, this research theme has grown rapidly. There are now several different software tools that implement an evaluation of the Fisher information matrix for population PKPD. We compared and evaluated the following five software tools: PFIM, PkStaMp, PopDes, PopED and POPT. The comparisons were performed using two models, a simple-one compartment warfarin PK model and a more complex PKPD model for pegylated interferon, with data on both concentration and response of viral load of hepatitis C virus. The results of the software were compared in terms of the standard error (SE) values of the parameters predicted from the software and the empirical SE values obtained via replicated clinical trial simulation and estimation. For the warfarin PK model and the pegylated interferon PKPD model, all software gave similar results. Interestingly, it was seen, for all software, that the simpler approximation to the Fisher information matrix, using the block diagonal matrix, provided predicted SE values that were closer to the empirical SE values than when the more complicated approximation was used (the full matrix). For most PKPD models, using any of the available software tools will provide meaningful results, avoiding cumbersome simulation and allowing design optimization.","accessed":{"date-parts":[["2022",5,18]]},"author":[{"family":"Nyberg","given":"Joakim"},{"family":"Bazzoli","given":"Caroline"},{"family":"Ogungbenro","given":"Kay"},{"family":"Aliev","given":"Alexander"},{"family":"Leonov","given":"Sergei"},{"family":"Duffull","given":"Stephen"},{"family":"Hooker","given":"Andrew C."},{"family":"Mentré","given":"France"}],"citation-key":"nybergMethodsSoftwareTools2015","container-title":"British Journal of Clinical Pharmacology","DOI":"10.1111/bcp.12352","ISSN":"1365-2125","issue":"1","issued":{"date-parts":[["2015"]]},"language":"en","page":"6-17","source":"Wiley Online Library","title":"Methods and software tools for design evaluation in population pharmacokinetics–pharmacodynamics studies","type":"article-journal","URL":"https://onlinelibrary.wiley.com/doi/abs/10.1111/bcp.12352","volume":"79"},
  {"id":"parkSingleIndexModelSurfaceLink2021","abstract":"This article focuses on the problem of modeling and estimating interaction effects between covariates and a continuous treatment variable on an outcome, using a single-index regression. The primary motivation is to estimate an optimal individualized dose rule and individualized treatment effects. To model possibly nonlinear interaction effects between the patients’ covariates and a continuous treatment variable, we employ a two-dimensional penalized spline regression on an index-treatment domain, where the index is defined as a linear projection of the covariates. The method is illustrated using two applications as well as simulation experiments. A unique contribution of this work is in the parsimonious (single-index) parameterization specifically defined for the interaction effect term, that can be used to assess the treatment benefit. Supplemental materials for this article are available online.","accessed":{"date-parts":[["2022",5,19]]},"author":[{"family":"Park","given":"Hyung"},{"family":"Petkova","given":"Eva"},{"family":"Tarpey","given":"Thaddeus"},{"family":"Ogden","given":"R. Todd"}],"citation-key":"parkSingleIndexModelSurfaceLink2021","container-title":"Journal of Computational and Graphical Statistics","DOI":"10.1080/10618600.2021.1923521","ISSN":"1061-8600","issue":"0","issued":{"date-parts":[["2021",5,3]]},"page":"1-10","publisher":"Taylor & Francis","source":"Taylor and Francis+NEJM","title":"A Single-Index Model With a Surface-Link for Optimizing Individualized Dose Rules","type":"article-journal","URL":"https://doi.org/10.1080/10618600.2021.1923521","volume":"0"},
  {"id":"PDFExistenceNormal2024","abstract":"PDF | The distribution of the ratio of two independent normal random variables X and Y is heavy tailed and has no moments. The shape of its density can... | Find, read and cite all the research you need on ResearchGate","accessed":{"date-parts":[["2025",2,19]]},"citation-key":"PDFExistenceNormal2024","container-title":"ResearchGate","DOI":"10.1007/s00362-012-0429-2","issued":{"date-parts":[["2024",10,22]]},"language":"en","source":"www.researchgate.net","title":"(PDF) On the existence of a normal approximation to the distribution of the ratio of two independent normal random variables","type":"article-journal","URL":"https://www.researchgate.net/publication/257406150_On_the_existence_of_a_normal_approximation_to_the_distribution_of_the_ratio_of_two_independent_normal_random_variables"},
  {"id":"PDFRevisitingRegression2024","abstract":"PDF | In the context of random sampling, we show that linear full (separate) regression adjustment (FRA) on the control and treatment groups is,... | Find, read and cite all the research you need on ResearchGate","accessed":{"date-parts":[["2025",3,13]]},"citation-key":"PDFRevisitingRegression2024","container-title":"ResearchGate","ISSN":"0747-4938","issued":{"date-parts":[["2024",10,22]]},"language":"en","source":"www.researchgate.net","title":"(PDF) Revisiting regression adjustment in experiments with heterogeneous treatment effects","type":"article-journal","URL":"https://www.researchgate.net/publication/344475499_Revisiting_regression_adjustment_in_experiments_with_heterogeneous_treatment_effects"},
  {"id":"pearlInvitedCommentaryUnderstanding2011","abstract":"In choosing covariates for adjustment or inclusion in propensity score analysis, researchers must weigh the benefit of reducing confounding bias carried by those covariates against the risk of amplifying residual bias carried by unmeasured confounders. The latter is characteristic of covariates that act like instrumental variables—that is, variables that are more strongly associated with the exposure than with the outcome. In this issue of the Journal (Am J Epidemiol. 2011;174(11):1213–1222), Myers et al. compare the bias amplification of a near-instrumental variable with its bias-reducing potential and suggest that, in practice, the latter outweighs the former. The author of this commentary sheds broader light on this comparison by considering the cumulative effects of conditioning on multiple covariates and showing that bias amplification may build up at a faster rate than bias reduction. The author further derives a partial order on sets of covariates which reveals preference for conditioning on outcome-related, rather than exposure-related, confounders.","accessed":{"date-parts":[["2024",8,21]]},"author":[{"family":"Pearl","given":"Judea"}],"citation-key":"pearlInvitedCommentaryUnderstanding2011","container-title":"American Journal of Epidemiology","container-title-short":"American Journal of Epidemiology","DOI":"10.1093/aje/kwr352","ISSN":"0002-9262","issue":"11","issued":{"date-parts":[["2011",12,1]]},"page":"1223-1227","source":"Silverchair","title":"Invited Commentary: Understanding Bias Amplification","title-short":"Invited Commentary","type":"article-journal","URL":"https://doi.org/10.1093/aje/kwr352","volume":"174"},
  {"id":"peretzPharmacokineticsOncedailyTacrolimus2021","abstract":"Patient ethnicity may influence the pharmacokinetics (PK) of tacrolimus. Because the Canadian First Nations (FN) constitute a large and increasing segment of the liver transplant population, we undertook to determine whether PK differences exist for a once-daily, extended release formulation of tacrolimus (Advagraf) in FN compared to Caucasian (CAUC) liver transplant recipients. Following achievement of a steady state with Advagraf, blood samples were drawn at 0, 1, 2, 4, 6, 8 and 24 hours for whole blood tacrolimus levels by commercial immunoassay and CYP3A4 and CYP3A5 allele analyses were performed by polymerase chain reactions. Nineteen subjects participated in the study (7 FN and 12 CAUC). The FN cohort had significantly higher AUC (214 ± 48 versus 168 ± 25, P < 0.05), Cmax (16.7 ± 4.4 ng/ml versus 11.3 ± 1.7 ng/ml, P < 0.05), Cmin (6.1 ± 1.0 ng/ml versus 4.7 ± 0.5 ng/ml, P < 0.05) and shorter Tmax (1.6 ± 0.2 hours versus 2.8 ± 0.3 hours, P < 0.05) values than CAUCs. CYP3A4 genotypes were C/C in both cohorts, while the CYP3A5 *1/*3 allele was present in 2/5 FN and 0/9 CAUC. The results of this study indicate that once-daily, extended release Advagraf results in higher blood tacrolimus levels and shorter times to Cmax in FN compared to CAUC liver transplant recipients.","accessed":{"date-parts":[["2022",5,28]]},"author":[{"family":"Peretz","given":"David"},{"family":"On","given":"Ngoc H."},{"family":"Miller","given":"Donald"},{"family":"Kim","given":"Richard"},{"family":"Franklin","given":"Carla"},{"family":"Dascal","given":"Roman"},{"family":"Knowles","given":"Cori"},{"family":"Minuk","given":"Gerald Y."}],"citation-key":"peretzPharmacokineticsOncedailyTacrolimus2021","container-title":"Transplant International","DOI":"10.1111/tri.13997","ISSN":"1432-2277","issue":"11","issued":{"date-parts":[["2021"]]},"language":"en","page":"2266-2273","source":"Wiley Online Library","title":"Pharmacokinetics of a once-daily tacrolimus formulation in first nations and caucasian liver transplant recipients","type":"article-journal","URL":"http://onlinelibrary.wiley.com/doi/abs/10.1111/tri.13997","volume":"34"},
  {"id":"PersonalizedMedicineMotivation","accessed":{"date-parts":[["2022",6,9]]},"citation-key":"PersonalizedMedicineMotivation","DOI":"10.1016/j.fertnstert.2018.05.006","language":"en","title":"Personalized medicine: motivation, challenges, and progress | Elsevier Enhanced Reader","title-short":"Personalized medicine","type":"webpage","URL":"https://reader.elsevier.com/reader/sd/pii/S0015028218304072?token=413F5D8E330F9A314D8CCFA9D8456797B6B22A3CC1F557845347FA57CA9B59EFDB542FD9ABF9090CC7DDEFC460EC3B37&originRegion=us-east-1&originCreation=20220609221806"},
  {"id":"pinkneyImprovedExtendedBayesian2021","abstract":"An improved and extended Bayesian synthetic control model is presented, expanding upon the latent factor model in Tuomaala 2019. The changes we make include 1) standardization of the data prior to model fit - which improves efficiency and generalization across different data sets; 2) adding time varying covariates; 3) adding the ability to have multiple treated units; 4) fitting the latent factors within the Bayesian model; and, 5) a sparsity inducing prior to automatically tune the number of latent factors. We demonstrate the similarity of estimates to two traditional synthetic control studies in Abadie, Diamond, and Hainmueller 2010 and Abadie, Diamond, and Hainmueller 2015 and extend to multiple target series with a new example of estimating digital website visitation from changes in data collection due to digital privacy laws.","accessed":{"date-parts":[["2024",11,10]]},"author":[{"family":"Pinkney","given":"Sean"}],"citation-key":"pinkneyImprovedExtendedBayesian2021","issued":{"date-parts":[["2021",3,30]]},"number":"arXiv:2103.16244","publisher":"arXiv","source":"arXiv.org","title":"An Improved and Extended Bayesian Synthetic Control","type":"article","URL":"http://arxiv.org/abs/2103.16244"},
  {"id":"raineyEstimatingLogitModels2021","abstract":"In small samples, maximum likelihood (ML) estimates of logit model coefficients have substantial bias away from zero. As a solution, we remind political scientists of Firth's (1993, Biometrika, 80, 27–38) penalized maximum likelihood (PML) estimator. Prior research has described and used PML, especially in the context of separation, but its small sample properties remain under-appreciated. The PML estimator eliminates most of the bias and, perhaps more importantly, greatly reduces the variance of the usual ML estimator. Thus, researchers do not face a bias-variance tradeoff when choosing between the ML and PML estimators—the PML estimator has a smaller bias and a smaller variance. We use Monte Carlo simulations and a re-analysis of George and Epstein (1992, American Political Science Review, 86, 323–337) to show that the PML estimator offers a substantial improvement in small samples (e.g., 50 observations) and noticeable improvement even in larger samples (e.g., 1000 observations).","accessed":{"date-parts":[["2023",8,2]]},"author":[{"family":"Rainey","given":"Carlisle"},{"family":"McCaskey","given":"Kelly"}],"citation-key":"raineyEstimatingLogitModels2021","container-title":"Political Science Research and Methods","DOI":"10.1017/psrm.2021.9","ISSN":"2049-8470, 2049-8489","issue":"3","issued":{"date-parts":[["2021",7]]},"page":"549-564","source":"Cambridge University Press","title":"Estimating logit models with small samples","type":"article-journal","URL":"https://www.cambridge.org/core/journals/political-science-research-and-methods/article/estimating-logit-models-with-small-samples/712F6039372475B650B123E9A1FE60E6","volume":"9"},
  {"id":"RegretRegressionOptimalDynamic","accessed":{"date-parts":[["2022",5,21]]},"citation-key":"RegretRegressionOptimalDynamic","title":"Regret‐Regression for Optimal Dynamic Treatment Regimes - Henderson - 2010 - Biometrics - Wiley Online Library","type":"webpage","URL":"https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1541-0420.2009.01368.x"},
  {"id":"RevisitingRegressionAdjustment2021","abstract":"Abstract In the context of random sampling, we show that linear full (separate) regression adjustment (FRA) on the control and treatment groups is, asymptotically, no less efficient than both the simple difference-in-means estimator and the pooled regression adjustment estimator; with heterogeneous treatment effects, FRA is usually strictly more efficient. We also propose a class of nonlinear regression adjustment estimators where consistency is ensured despite arbitrary misspecification of the conditional mean function. A simulation study confirms that nontrivial efficiency gains are possible with linear FRA, and that further gains are possible, even under severe mean misspecification, using nonlinear FRA.","accessed":{"date-parts":[["2024",3,24]]},"citation-key":"RevisitingRegressionAdjustment2021","container-title":"Econometric Reviews","container-title-short":"Econometric Reviews","DOI":"10.1080/07474938.2020.1824732","ISSN":"0747-4938","issue":"5","issued":{"date-parts":[["2021"]]},"page":"504-534","publisher":"Taylor & Francis","source":"Scholars Portal Journals","title":"Revisiting regression adjustment in experiments with heterogeneous treatment effects","type":"article-journal","URL":"https://journals.scholarsportal.info/details/07474938/v40i0005/504_rraiewhte.xml","volume":"40"},
  {"id":"richOptimalIndividualizedDosing2016","abstract":"There have been considerable advances in the methodology for estimating dynamic treatment regimens, and for the design of sequential trials that can be used to collect unconfounded data to inform such regimens. However, relatively little attention has been paid to how such methodology could be used to advance understanding of optimal treatment strategies in a continuous dose setting, even though it is often the case that considerable patient heterogeneity in drug response along with a narrow therapeutic window may necessitate the tailoring of dosing over time. Such is the case with warfarin, a common oral anticoagulant. We propose novel, realistic simulation models based on pharmacokinetic-pharmacodynamic properties of the drug that can be used to evaluate potentially optimal dosing strategies. Our results suggest that this methodology can lead to a dosing strategy that performs well both within and across populations with different pharmacokinetic characteristics, and may assist in the design of randomized trials by narrowing the list of potential dosing strategies to those which are most promising.","accessed":{"date-parts":[["2022",5,19]]},"author":[{"family":"Rich","given":"Benjamin"},{"family":"Moodie","given":"Erica E. M."},{"family":"Stephens","given":"David A."}],"citation-key":"richOptimalIndividualizedDosing2016","container-title":"Biometrical Journal","DOI":"10.1002/bimj.201400244","ISSN":"1521-4036","issue":"3","issued":{"date-parts":[["2016"]]},"language":"en","page":"502-517","source":"Wiley Online Library","title":"Optimal individualized dosing strategies: A pharmacologic approach to developing dynamic treatment regimens for continuous-valued treatments","title-short":"Optimal individualized dosing strategies","type":"article-journal","URL":"http://onlinelibrary.wiley.com/doi/abs/10.1002/bimj.201400244","volume":"58"},
  {"id":"richSimulatingSequentialMultiple2014","abstract":"BackgroundDue to the cost and complexity of conducting a sequential multiple assignment randomized trial (SMART), it is desirable to pre-define a small number of personalized regimes to study.PurposeWe proposed a simulation-based approach to studying personalized dosing strategies in contexts for which a therapeutic agent?s pharmacokinetic and pharmacodynamics properties are well understood. We take dosing of warfarin as a case study, as its properties are well understood. We consider a SMART in which there are five intervention points in which dosing may be modified, following a loading phase of treatment.MethodsRealistic SMARTs are simulated, and two methods of analysis, G-estimation and Q-learning, are used to assess potential personalized dosing strategies.ResultsIn settings where outcome modelling may be complex due to the highly non-linear nature of the pharmacokinetic and pharmacodynamics mechanisms of the therapeutic agent, G-estimation provides for which the more promising method of estimating an optimal dosing strategy. Used in combination with the simulated SMARTs, we were able to improve simulated patient outcomes and suggest which patient characteristics were needed to best individually tailor dosing. In particular, our simulations suggest that current dosing should be determined by an individual?s current coagulation time as measured by the international normalized ratio (INR), their last measured INR, and their last dose. Tailoring treatment only based on current INR and last warfarin dose provided inferior control of INR over the course of the trial.LimitationsThe ability of the simulated SMARTs to suggest optimal personalized dosing strategies relies on the pharmacokinetic and pharmacodynamic models used to generate the hypothetical patient profiles. This approach is best suited to therapeutic agents whose effects are well studied.ConclusionPrior to investing in a complex randomized trial that involves sequential treatment allocations, simulations should be used where possible in order to guide which dosing strategies to evaluate.","accessed":{"date-parts":[["2022",5,19]]},"author":[{"family":"Rich","given":"Benjamin"},{"family":"Moodie","given":"Erica EM"},{"family":"Stephens","given":"David A"}],"citation-key":"richSimulatingSequentialMultiple2014","container-title":"Clinical Trials","container-title-short":"Clinical Trials","DOI":"10.1177/1740774513517063","ISSN":"1740-7745","issue":"4","issued":{"date-parts":[["2014",8,1]]},"language":"en","page":"435-444","publisher":"SAGE Publications","source":"SAGE Journals","title":"Simulating sequential multiple assignment randomized trials to generate optimal personalized warfarin dosing strategies","type":"article-journal","URL":"https://doi.org/10.1177/1740774513517063","volume":"11"},
  {"id":"rileyCalculatingSampleSize2020","abstract":"<p>Clinical prediction models aim to predict outcomes in individuals, to inform diagnosis or prognosis in healthcare. Hundreds of prediction models are published in the medical literature each year, yet many are developed using a dataset that is too small for the total number of participants or outcome events. This leads to inaccurate predictions and consequently incorrect healthcare decisions for some individuals. In this article, the authors provide guidance on how to calculate the sample size required to develop a clinical prediction model.</p>","accessed":{"date-parts":[["2022",9,26]]},"author":[{"family":"Riley","given":"Richard D."},{"family":"Ensor","given":"Joie"},{"family":"Snell","given":"Kym I. E."},{"family":"Harrell","given":"Frank E."},{"family":"Martin","given":"Glen P."},{"family":"Reitsma","given":"Johannes B."},{"family":"Moons","given":"Karel G. M."},{"family":"Collins","given":"Gary"},{"family":"Smeden","given":"Maarten","dropping-particle":"van"}],"citation-key":"rileyCalculatingSampleSize2020","container-title":"BMJ","container-title-short":"BMJ","DOI":"10.1136/bmj.m441","ISSN":"1756-1833","issued":{"date-parts":[["2020",3,18]]},"language":"en","license":"Published by the BMJ Publishing Group Limited. For permission to use (where not already granted under a licence) please go to http://group.bmj.com/group/rights-licensing/permissions","page":"m441","PMID":"32188600","publisher":"British Medical Journal Publishing Group","section":"Research Methods &amp; Reporting","source":"www.bmj.com","title":"Calculating the sample size required for developing a clinical prediction model","type":"article-journal","URL":"https://www.bmj.com/content/368/bmj.m441","volume":"368"},
  {"id":"rileyMinimumSampleSize2019","abstract":"When designing a study to develop a new prediction model with binary or time-to-event outcomes, researchers should ensure their sample size is adequate in terms of the number of participants (n) and outcome events (E) relative to the number of predictor parameters (p) considered for inclusion. We propose that the minimum values of n and E (and subsequently the minimum number of events per predictor parameter, EPP) should be calculated to meet the following three criteria: (i) small optimism in predictor effect estimates as defined by a global shrinkage factor of ≥0.9, (ii) small absolute difference of ≤ 0.05 in the model's apparent and adjusted Nagelkerke's R2, and (iii) precise estimation of the overall risk in the population. Criteria (i) and (ii) aim to reduce overfitting conditional on a chosen p, and require prespecification of the model's anticipated Cox-Snell R2, which we show can be obtained from previous studies. The values of n and E that meet all three criteria provides the minimum sample size required for model development. Upon application of our approach, a new diagnostic model for Chagas disease requires an EPP of at least 4.8 and a new prognostic model for recurrent venous thromboembolism requires an EPP of at least 23. This reinforces why rules of thumb (eg, 10 EPP) should be avoided. Researchers might additionally ensure the sample size gives precise estimates of key predictor effects; this is especially important when key categorical predictors have few events in some categories, as this may substantially increase the numbers required.","accessed":{"date-parts":[["2022",9,27]]},"author":[{"family":"Riley","given":"Richard D"},{"family":"Snell","given":"Kym IE"},{"family":"Ensor","given":"Joie"},{"family":"Burke","given":"Danielle L"},{"family":"Harrell Jr","given":"Frank E"},{"family":"Moons","given":"Karel GM"},{"family":"Collins","given":"Gary S"}],"citation-key":"rileyMinimumSampleSize2019","container-title":"Statistics in Medicine","DOI":"10.1002/sim.7992","ISSN":"1097-0258","issue":"7","issued":{"date-parts":[["2019"]]},"language":"en","license":"© 2018 The Authors. Statistics in Medicine Published by John Wiley & Sons Ltd.","page":"1276-1296","source":"Wiley Online Library","title":"Minimum sample size for developing a multivariable prediction model: PART II - binary and time-to-event outcomes","title-short":"Minimum sample size for developing a multivariable prediction model","type":"article-journal","URL":"https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.7992","volume":"38"},
  {"id":"ritovBayesianAnalysisComplex2014","abstract":"We consider the Bayesian analysis of a few complex, high-dimensional models and show that intuitive priors, which are not tailored to the fine details of the model and the estimated parameters, produce estimators which perform poorly in situations in which good, simple frequentist estimators exist. The models we consider are: stratified sampling, the partial linear model, linear and quadratic functionals of white noise and estimation with stopping times. We present a strong version of Doob’s consistency theorem which demonstrates that the existence of a uniformly $\\sqrt{n}$-consistent estimator ensures that the Bayes posterior is $\\sqrt{n}$-consistent for values of the parameter in subsets of prior probability 1. We also demonstrate that it is, at least, in principle, possible to construct Bayes priors giving both global and local minimax rates, using a suitable combination of loss functions. We argue that there is no contradiction in these apparently conflicting findings.","accessed":{"date-parts":[["2025",3,2]]},"author":[{"family":"Ritov","given":"Y."},{"family":"Bickel","given":"P. J."},{"family":"Gamst","given":"A. C."},{"family":"Kleijn","given":"B. J. K."}],"citation-key":"ritovBayesianAnalysisComplex2014","container-title":"Statistical Science","DOI":"10.1214/14-STS483","ISSN":"0883-4237, 2168-8745","issue":"4","issued":{"date-parts":[["2014",11]]},"page":"619-639","publisher":"Institute of Mathematical Statistics","source":"Project Euclid","title":"The Bayesian Analysis of Complex, High-Dimensional Models: Can It Be CODA?","title-short":"The Bayesian Analysis of Complex, High-Dimensional Models","type":"article-journal","URL":"https://projecteuclid.org/journals/statistical-science/volume-29/issue-4/The-Bayesian-Analysis-of-Complex-High-Dimensional-Models--Can/10.1214/14-STS483.full","volume":"29"},
  {"id":"robinsonBayesianTestingImmune2015","abstract":"Since I joined Stack Exchange as a Data Scientist in June, one of my first projects has been reconsidering the A/B testing system used to evaluate new features and changes to the site. Our current approach relies on computing a p-value to measure our confidence in a new feature.\n\nUnfortunately, this leads to a common pitfall in performing A/B testing, which is the habit of looking at a test while it’s running, then stopping the test as soon as the p-value reaches a particular threshold- say, .05. This seems reasonable, but in doing so, you’re making the p-value no longer trustworthy, and making it substantially more likely you’ll implement features that offer no improvement. How Not To Run an A/B Test gives a good explanation of this problem.\n\nOne solution is to pre-commit to running your experiment for a particular amount of time, never stopping early or extending it farther. But this is impractical in a business setting, where you might want to stop a test early once you see a positive change, or keep a not-yet-significant test running longer than you’d planned. (For more on this, see A/B Testing Rigorously (without losing your job)).","author":[{"family":"Robinson","given":"David"}],"citation-key":"robinsonBayesianTestingImmune2015","container-title":"Variance Explained","issued":{"date-parts":[["2015",8,21]]},"title":"Is Bayesian A/B Testing Immune to Peeking? Not Exactly","type":"post-weblog","URL":"http://varianceexplained.org/r/bayesian-ab-testing/"},
  {"id":"rochfordTwoYearsBayesian2018","author":[{"family":"Rochford","given":"Austin"}],"citation-key":"rochfordTwoYearsBayesian2018","issued":{"date-parts":[["2018",10,18]]},"title":"Two Years of Bayesian Bandits for E-Commerce","type":"speech","URL":"https://austinrochford.com/resources/talks/pydata-nyc-bayes-bandits.slides.html#/"},
  {"id":"rosthojEstimationOptimalDynamic2006","abstract":"A complication of long-term anticoagulation is that the optimal dose level varies not only between patients but over time within patients, in response to short-term changes in lifestyle. Consequently, doseage needs to be adaptive but there are as yet no accepted decision rules. Since anticoagulant use is increasing worldwide there is a need for more objective and routine procedures. In this paper, we describe an analysis of observational longitudinal anticoagulant data, aimed at determining an optimal reactive dose-changing strategy. We use the regret parameterization approach advocated by Murphy (J. R. Stat. Soc. Ser. B 2003; 65:331–366). Practical problems encountered in the implementation of the approach are discussed and illustrated. Copyright © 2006 John Wiley & Sons, Ltd.","accessed":{"date-parts":[["2022",5,21]]},"author":[{"family":"Rosthøj","given":"Susanne"},{"family":"Fullwood","given":"Catherine"},{"family":"Henderson","given":"Robin"},{"family":"Stewart","given":"Syd"}],"citation-key":"rosthojEstimationOptimalDynamic2006","container-title":"Statistics in Medicine","DOI":"10.1002/sim.2694","ISSN":"1097-0258","issue":"24","issued":{"date-parts":[["2006"]]},"language":"en","page":"4197-4215","source":"Wiley Online Library","title":"Estimation of optimal dynamic anticoagulation regimes from observational data: a regret-based approach","title-short":"Estimation of optimal dynamic anticoagulation regimes from observational data","type":"article-journal","URL":"http://onlinelibrary.wiley.com/doi/abs/10.1002/sim.2694","volume":"25"},
  {"id":"roystonRestrictedMeanSurvival2013","abstract":"Designs and analyses of clinical trials with a time-to-event outcome almost invariably rely on the hazard ratio to estimate the treatment effect and implicitly, therefore, on the proportional hazards assumption. However, the results of some recent trials indicate that there is no guarantee that the assumption will hold. Here, we describe the use of the restricted mean survival time as a possible alternative tool in the design and analysis of these trials.","accessed":{"date-parts":[["2023",6,15]]},"author":[{"family":"Royston","given":"Patrick"},{"family":"Parmar","given":"Mahesh KB"}],"citation-key":"roystonRestrictedMeanSurvival2013","container-title":"BMC Medical Research Methodology","container-title-short":"BMC Medical Research Methodology","DOI":"10.1186/1471-2288-13-152","ISSN":"1471-2288","issue":"1","issued":{"date-parts":[["2013",12,7]]},"page":"152","source":"BioMed Central","title":"Restricted mean survival time: an alternative to the hazard ratio for the design and analysis of randomized trials with a time-to-event outcome","title-short":"Restricted mean survival time","type":"article-journal","URL":"https://doi.org/10.1186/1471-2288-13-152","volume":"13"},
  {"id":"saidOptimizingSampleSizes2020","abstract":"In most business decisions, you want to choose a policy that maximizes your benefits minus your costs. In experimentation, the benefit comes from learning information to drive future decisions, and the cost comes from the experiment itself. The optimal sample size will therefore depend on the unique circumstances of your business, not on arbitrary statistical significance thresholds.\n\nIn this three-part blog post, I’ll present a new way of determining optimal sample sizes that completely abandons the notion of statistical significance.","author":[{"family":"Said","given":"Chris"}],"citation-key":"saidOptimizingSampleSizes2020","container-title":"Chris Said","issued":{"date-parts":[["2020",1,10]]},"title":"Optimizing sample sizes in A/B testing, Part I: General summary","type":"post-weblog","URL":"https://chris-said.io/2020/01/10/optimizing-sample-sizes-in-ab-testing-part-I/"},
  {"id":"sennSevenMythsRandomisation2013","abstract":"I consider seven misunderstandings that may be encountered about the nature, purpose and properties of randomisation in clinical trials. Some concern the practical realities of clinical research on patients. Others are to do with the value and purpose of balance. Still others are to do with a confusion about the role of conditioning in valid statistical inference. I consider a simple game of chance involving two dice to illustrate some points about inference and then consider the seven misunderstandings in turn. I conclude that although one should not make a fetish of randomisation, when proposing alternatives to randomisation in clinical trials, one should be very careful to be precise about the exact nature of the alternative being considered if one is to avoid the danger of underestimating the advantages that randomisation can offer. Copyright © 2012 John Wiley & Sons, Ltd.","accessed":{"date-parts":[["2025",3,28]]},"author":[{"family":"Senn","given":"Stephen"}],"citation-key":"sennSevenMythsRandomisation2013","container-title":"Statistics in Medicine","container-title-short":"Statistics in Medicine","DOI":"10.1002/sim.5713","ISSN":"0277-6715","issue":"9","issued":{"date-parts":[["2013"]]},"page":"1439-1450","publisher":"Wiley","source":"Scholars Portal Journals","title":"Seven myths of randomisation in clinical trials","type":"article-journal","URL":"https://journals.scholarsportal.info/details/02776715/v32i0009/1439_smorict.xml","volume":"32"},
  {"id":"sennStratificationPropensityScore2007","abstract":"Stratifying and matching by the propensity score are increasingly popular approaches to deal with confounding in medical studies investigating effects of a treatment or exposure. A more traditional alternative technique is the direct adjustment for confounding in regression models. This paper discusses fundamental differences between the two approaches, with a focus on linear regression and propensity score stratification, and identifies points to be considered for an adequate comparison. The treatment estimators are examined for unbiasedness and efficiency. This is illustrated in an application to real data and supplemented by an investigation on properties of the estimators for a range of underlying linear models. We demonstrate that in specific circumstances the propensity score estimator is identical to the effect estimated from a full linear model, even if it is built on coarser covariate strata than the linear model. As a consequence the coarsening property of the propensity score-adjustment for a one-dimensional confounder instead of a high-dimensional covariate-may be viewed as a way to implement a pre-specified, richly parametrized linear model. We conclude that the propensity score estimator inherits the potential for overfitting and that care should be taken to restrict covariates to those relevant for outcome.","author":[{"family":"Senn","given":"Stephen"},{"family":"Graf","given":"Erika"},{"family":"Caputo","given":"Angelika"}],"citation-key":"sennStratificationPropensityScore2007","container-title":"Statistics in Medicine","container-title-short":"Stat Med","DOI":"10.1002/sim.3133","ISSN":"0277-6715","issue":"30","issued":{"date-parts":[["2007",12,30]]},"language":"eng","page":"5529-5544","PMID":"18058851","source":"PubMed","title":"Stratification for the propensity score compared with linear regression techniques to assess the effect of treatment or exposure","type":"article-journal","volume":"26"},
  {"id":"sennStratificationPropensityScore2007a","abstract":"Stratifying and matching by the propensity score are increasingly popular approaches to deal with confounding in medical studies investigating effects of a treatment or exposure. A more traditional alternative technique is the direct adjustment for confounding in regression models. This paper discusses fundamental differences between the two approaches, with a focus on linear regression and propensity score stratification, and identifies points to be considered for an adequate comparison. The treatment estimators are examined for unbiasedness and efficiency. This is illustrated in an application to real data and supplemented by an investigation on properties of the estimators for a range of underlying linear models. We demonstrate that in specific circumstances the propensity score estimator is identical to the effect estimated from a full linear model, even if it is built on coarser covariate strata than the linear model. As a consequence the coarsening property of the propensity score—adjustment for a one-dimensional confounder instead of a high-dimensional covariate—may be viewed as a way to implement a pre-specified, richly parametrized linear model. We conclude that the propensity score estimator inherits the potential for overfitting and that care should be taken to restrict covariates to those relevant for outcome. Copyright © 2007 John Wiley & Sons, Ltd.","accessed":{"date-parts":[["2024",8,21]]},"author":[{"family":"Senn","given":"Stephen"},{"family":"Graf","given":"Erika"},{"family":"Caputo","given":"Angelika"}],"citation-key":"sennStratificationPropensityScore2007a","container-title":"Statistics in Medicine","DOI":"10.1002/sim.3133","ISSN":"1097-0258","issue":"30","issued":{"date-parts":[["2007"]]},"language":"en","license":"Copyright © 2007 John Wiley & Sons, Ltd.","page":"5529-5544","source":"Wiley Online Library","title":"Stratification for the propensity score compared with linear regression techniques to assess the effect of treatment or exposure","type":"article-journal","URL":"https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.3133","volume":"26"},
  {"id":"sennStratificationPropensityScore2007b","abstract":"Stratifying and matching by the propensity score are increasingly popular approaches to deal with confounding in medical studies investigating effects of a treatment or exposure. A more traditional alternative technique is the direct adjustment for confounding in regression models.This paper discusses fundamental differences between the two approaches, with a focus on linear regression and propensity score stratification, and identifies points to be considered for an adequate comparison. The treatment estimators are examined for unbiasedness and efficiency. This is illustrated in an application to real data and supplemented by an investigation on properties of the estimators for a range of underlying linear models. We demonstrate that in specific circumstances the propensity score estimator is identical to the effect estimated from a full linear model, even if it is built on coarser covariate strata than the linear model. As a consequence the coarsening property of the propensity score—adjustment for a one‐dimensional confounder instead of a high‐dimensional covariate—may be viewed as a way to implement a pre‐specified, richly parametrized linear model. We conclude that the propensity score estimator inherits the potential for overfitting and that care should be taken to restrict covariates to those relevant for outcome. Copyright © 2007 John Wiley & Sons, Ltd.","accessed":{"date-parts":[["2024",8,21]]},"author":[{"family":"Senn","given":"Stephen"},{"family":"Graf","given":"Erika"},{"family":"Caputo","given":"Angelika"}],"citation-key":"sennStratificationPropensityScore2007b","container-title":"Statistics in Medicine","container-title-short":"Statistics in Medicine","DOI":"10.1002/sim.3133","ISSN":"0277-6715","issue":"30","issued":{"date-parts":[["2007"]]},"page":"5529-5544","publisher":"John Wiley & Sons, Ltd.","source":"Scholars Portal Journals","title":"Stratification for the propensity score compared with linear regression techniques to assess the effect of treatment or exposure","type":"article-journal","URL":"https://journals.scholarsportal.info/details/02776715/v26i0030/5529_sftpscteotoe.xml","volume":"26"},
  {"id":"sennStratificationRandomisedClinical2024","abstract":"A simple device for balancing for a continuous covariate in clinical trials is to stratify by whether the covariate is above or below some target value, typically the predicted median. This raises an issue as to which model should be used for modelling the effect of treatment on the outcome variable, $Y$. Should one fit, the stratum indicator, $S$, the continuous covariate, $X$, both or neither? This question has been investigated in the literature using simulations targetting the overall effect on inferences about treatment . However, when a covariate is added to a model there are three consequences for inference: 1) The mean square error effect, 2) The variance inflation factor and 3) second order precision. We consider that it is valuable to consider these three factors separately even if, ultimately, it is their joint effect that matters. We present some simple theory, concentrating in particular on the variance inflation factor, that may be used to guide trialists in their choice of model. We also consider the case where the precise form of the relationship between the outcome and the covariate is not known. We conclude by recommending that the continuous coovariate should always be in the model but that, depending on circumstances, there may be some justification in fitting the stratum indicator also.","accessed":{"date-parts":[["2024",8,22]]},"author":[{"family":"Senn","given":"Stephen"},{"family":"König","given":"Franz"},{"family":"Posch","given":"Martin"}],"citation-key":"sennStratificationRandomisedClinical2024","DOI":"10.48550/arXiv.2408.06760","issued":{"date-parts":[["2024",8,13]]},"number":"arXiv:2408.06760","publisher":"arXiv","source":"arXiv.org","title":"Stratification in Randomised Clinical Trials and Analysis of Covariance: Some Simple Theory and Recommendations","title-short":"Stratification in Randomised Clinical Trials and Analysis of Covariance","type":"article","URL":"http://arxiv.org/abs/2408.06760"},
  {"id":"shepherdStatisticalAnalysisObservational2024","abstract":"Observational studies have a critical role in disability research, providing the opportunity to address a range of research questions. Over the past decades, there have been substantial shifts and developments in statistical methods for observational studies, most notably for causal inference. In this review, we provide an overview of modern design and analysis concepts critical for observational studies, drawing examples from the field of disability research and highlighting the challenges in this field, to inform the readership on important statistical considerations for their studies. What this paper adds Descriptive research questions have specific analytical complexities, so careful statistical design before analysis is critical. Prediction research aims to produce a model with good predictive ability and requires thorough statistical design prior to analysis. Causal research requires careful statistical analysis planning, facilitated by modern causal inference concepts and analytical methods. Adopting these approaches will strengthen the quality of observational studies addressing a range of research questions in the disability space.","accessed":{"date-parts":[["2025",7,7]]},"author":[{"family":"Shepherd","given":"Daisy A."},{"family":"Amor","given":"David J."},{"family":"Moreno-Betancur","given":"Margarita"}],"citation-key":"shepherdStatisticalAnalysisObservational2024","container-title":"Developmental Medicine & Child Neurology","DOI":"10.1111/dmcn.15948","ISSN":"1469-8749","issue":"11","issued":{"date-parts":[["2024"]]},"language":"en","page":"1408-1418","source":"Wiley Online Library","title":"Statistical analysis of observational studies in disability research","type":"article-journal","URL":"https://onlinelibrary.wiley.com/doi/abs/10.1111/dmcn.15948","volume":"66"},
  {"id":"standevelopmentteamStanModellingLanguage2012","author":[{"family":"Stan Development Team","given":""}],"citation-key":"standevelopmentteamStanModellingLanguage2012","issued":{"date-parts":[["2012",8,12]]},"title":"Stan Modelling Language Users Guide and Reference Manual Version 1.0.0","type":"article-journal","URL":"https://github.com/stan-dev/stan/releases/tag/v1.0.0"},
  {"id":"sturkenboomPopulationPharmacokineticsBayesian2021","abstract":"Tuberculosis (TB) is still the number one cause of death due to an infectious disease. Pharmacokinetics and pharmacodynamics of anti-TB drugs are key in the optimization of TB treatment and help to prevent slow response to treatment, acquired drug resistance, and adverse drug effects. The aim of this review was to provide an update on the pharmacokinetics and pharmacodynamics of anti-TB drugs and to show how population pharmacokinetics and Bayesian dose adjustment can be used to optimize treatment. We cover aspects on preclinical, clinical, and population pharmacokinetics of different drugs used for drug-susceptible TB and multidrug-resistant TB. Moreover, we include available data to support therapeutic drug monitoring of these drugs and known pharmacokinetic and pharmacodynamic targets that can be used for optimization of therapy. We have identified a wide range of population pharmacokinetic models for first- and second-line drugs used for TB, which included models built on NONMEM, Pmetrics, ADAPT, MWPharm, Monolix, Phoenix, and NPEM2 software. The first population models were built for isoniazid and rifampicin; however, in recent years, more data have emerged for both new anti-TB drugs, but also for defining targets of older anti-TB drugs. Since the introduction of therapeutic drug monitoring for TB over 3 decades ago, further development of therapeutic drug monitoring in TB next steps will again depend on academic and clinical initiatives. We recommend close collaboration between researchers and the World Health Organization to provide important guideline updates regarding therapeutic drug monitoring and pharmacokinetics/pharmacodynamics.","accessed":{"date-parts":[["2022",5,18]]},"author":[{"family":"Sturkenboom","given":"Marieke G. G."},{"family":"Märtson","given":"Anne-Grete"},{"family":"Svensson","given":"Elin M."},{"family":"Sloan","given":"Derek J."},{"family":"Dooley","given":"Kelly E."},{"family":"Elsen","given":"Simone H. J.","non-dropping-particle":"van den"},{"family":"Denti","given":"Paolo"},{"family":"Peloquin","given":"Charles A."},{"family":"Aarnoutse","given":"Rob E."},{"family":"Alffenaar","given":"Jan-Willem C."}],"citation-key":"sturkenboomPopulationPharmacokineticsBayesian2021","container-title":"Clinical Pharmacokinetics","container-title-short":"Clin Pharmacokinet","DOI":"10.1007/s40262-021-00997-0","ISSN":"0312-5963, 1179-1926","issue":"6","issued":{"date-parts":[["2021",6]]},"language":"en","page":"685-710","source":"DOI.org (Crossref)","title":"Population Pharmacokinetics and Bayesian Dose Adjustment to Advance TDM of Anti-TB Drugs","type":"article-journal","URL":"https://link.springer.com/10.1007/s40262-021-00997-0","volume":"60"},
  {"id":"sullivanShouldMultipleImputation2018","abstract":"The use of multiple imputation has increased markedly in recent years, and journal reviewers may expect to see multiple imputation used to handle missing data. However in randomized trials, where treatment group is always observed and independent of baseline covariates, other approaches may be preferable. Using data simulation we evaluated multiple imputation, performed both overall and separately by randomized group, across a range of commonly encountered scenarios. We considered both missing outcome and missing baseline data, with missing outcome data induced under missing at random mechanisms. Provided the analysis model was correctly specified, multiple imputation produced unbiased treatment effect estimates, but alternative unbiased approaches were often more efficient. When the analysis model overlooked an interaction effect involving randomized group, multiple imputation produced biased estimates of the average treatment effect when applied to missing outcome data, unless imputation was performed separately by randomized group. Based on these results, we conclude that multiple imputation should not be seen as the only acceptable way to handle missing data in randomized trials. In settings where multiple imputation is adopted, we recommend that imputation is carried out separately by randomized group.","accessed":{"date-parts":[["2025",6,25]]},"author":[{"family":"Sullivan","given":"Thomas R"},{"family":"White","given":"Ian R"},{"family":"Salter","given":"Amy B"},{"family":"Ryan","given":"Philip"},{"family":"Lee","given":"Katherine J"}],"citation-key":"sullivanShouldMultipleImputation2018","container-title":"Statistical Methods in Medical Research","container-title-short":"Stat Methods Med Res","DOI":"10.1177/0962280216683570","ISSN":"0962-2802","issue":"9","issued":{"date-parts":[["2018",9,1]]},"language":"EN","page":"2610-2626","publisher":"SAGE Publications Ltd STM","source":"SAGE Journals","title":"Should multiple imputation be the method of choice for handling missing data in randomized trials?","type":"article-journal","URL":"https://doi.org/10.1177/0962280216683570","volume":"27"},
  {"id":"sureshSurvivalPredictionModels2022","abstract":"Prediction models for time-to-event outcomes are commonly used in biomedical research to obtain subject-specific probabilities that aid in making important clinical care decisions. There are several regression and machine learning methods for building these models that have been designed or modified to account for the censoring that occurs in time-to-event data. Discrete-time survival models, which have often been overlooked in the literature, provide an alternative approach for predictive modeling in the presence of censoring with limited loss in predictive accuracy. These models can take advantage of the range of nonparametric machine learning classification algorithms and their available software to predict survival outcomes.","accessed":{"date-parts":[["2024",1,22]]},"author":[{"family":"Suresh","given":"Krithika"},{"family":"Severn","given":"Cameron"},{"family":"Ghosh","given":"Debashis"}],"citation-key":"sureshSurvivalPredictionModels2022","container-title":"BMC Medical Research Methodology","container-title-short":"BMC Medical Research Methodology","DOI":"10.1186/s12874-022-01679-6","ISSN":"1471-2288","issue":"1","issued":{"date-parts":[["2022",7,26]]},"page":"207","source":"BioMed Central","title":"Survival prediction models: an introduction to discrete-time modeling","title-short":"Survival prediction models","type":"article-journal","URL":"https://doi.org/10.1186/s12874-022-01679-6","volume":"22"},
  {"id":"tangBusinessPolicyExperiments2023","abstract":"This paper investigates an approach to both speed up business decision-making and lower the cost of learning through experimentation by factorizing business policies and employing fractional factorial experimental designs for their evaluation. We illustrate how this method integrates with advances in the estimation of heterogeneous treatment effects, elaborating on its advantages and foundational assumptions. We empirically demonstrate the implementation and benefits of our approach and assess its validity in evaluating consumer promotion policies at DoorDash, which is one of the largest delivery platforms in the US. Our approach discovers a policy with 5% incremental profit at 67% lower implementation cost.","accessed":{"date-parts":[["2025",9,15]]},"author":[{"family":"Tang","given":"Yixin"},{"family":"Lin","given":"Yicong"},{"family":"Sahni","given":"Navdeep S."}],"citation-key":"tangBusinessPolicyExperiments2023","DOI":"10.48550/arXiv.2311.14698","issued":{"date-parts":[["2023",11,29]]},"number":"arXiv:2311.14698","publisher":"arXiv","source":"arXiv.org","title":"Business Policy Experiments using Fractional Factorial Designs: Consumer Retention on DoorDash","title-short":"Business Policy Experiments using Fractional Factorial Designs","type":"article","URL":"http://arxiv.org/abs/2311.14698"},
  {"id":"timonenImportanceSamplingApproach2022","abstract":"Statistical models can involve implicitly defined quantities, such as solutions to nonlinear ordinary differential equations (ODEs), that unavoidably need to be numerically approximated in order to evaluate the model. The approximation error inherently biases statistical inference results, but the amount of this bias is generally unknown and often ignored in Bayesian parameter inference. We propose a computationally efficient method for verifying the reliability of posterior inference for such models, when the inference is performed using Markov chain Monte Carlo methods. We validate the efficiency and reliability of our workflow in experiments using simulated and real data, and different ODE solvers. We highlight problems that arise with commonly used adaptive ODE solvers, and propose robust and effective alternatives which, accompanied by our workflow, can now be taken into use without losing reliability of the inferences.","accessed":{"date-parts":[["2022",5,19]]},"author":[{"family":"Timonen","given":"Juho"},{"family":"Siccha","given":"Nikolas"},{"family":"Bales","given":"Ben"},{"family":"Lähdesmäki","given":"Harri"},{"family":"Vehtari","given":"Aki"}],"citation-key":"timonenImportanceSamplingApproach2022","DOI":"10.48550/arXiv.2205.09059","issued":{"date-parts":[["2022",5,18]]},"number":"arXiv:2205.09059","publisher":"arXiv","source":"arXiv.org","title":"An importance sampling approach for reliable and efficient inference in Bayesian ordinary differential equation models","type":"article","URL":"http://arxiv.org/abs/2205.09059"},
  {"id":"ueshimaPopulationPharmacokineticsPharmacogenomics2018","abstract":"Aims This study aimed to analyse the effects of genetic polymorphisms in drug transporters and metabolizing enzymes, and clinical laboratory data on the pharmacokinetic parameters of apixaban. Methods Data were collected from 81 Japanese patients with atrial fibrillation. Pharmacogenomic data were stratified by ABCB1, ABCG2 and CYP3A5 polymorphisms. The pharmacokinetic profile of apixaban was described by a one-compartment model with first-order absorption. Population pharmacokinetic analysis was conducted using a nonlinear mixed effect modelling (NONMEM™) program. Results The nonlinear relationship between oral clearance (CL/F) of apixaban and creatinine clearance (Ccr) was observed. The population mean of CL/F for a typical patient (Ccr value of 70 ml min−1) with the CYP3A5*1/*1 and ABCG2 421C/C or C/A genotypes was estimated to be 3.06 l h−1. When Ccr values were set to the typical value, the population mean of CL/F was 1.52 times higher in patients with the CYP3A5*1/*1 genotype compared with patients with the CYP3A5*1/*3 or *3/*3 genotype, while the population mean of CL/F was 1.49 times higher in patients with the ABCG2 421C/C or C/A genotype compared with patients with the ABCG2 421A/A genotype. However, no covariates affected the population mean of the apparent volume of distribution (Vd/F) of apixaban. The population mean of Vd/F was estimated to be 24.7 l. Conclusion The present study suggests that the ABCG2 421A/A and CYP3A5*3 genotypes and renal function are intrinsic factors affecting apixaban pharmacokinetics. These findings may provide useful information for precision medicine using apixaban, to avoid the risk of adverse reactions.","accessed":{"date-parts":[["2022",5,28]]},"author":[{"family":"Ueshima","given":"Satoshi"},{"family":"Hira","given":"Daiki"},{"family":"Kimura","given":"Yuuma"},{"family":"Fujii","given":"Ryo"},{"family":"Tomitsuka","given":"Chiho"},{"family":"Yamane","given":"Takuya"},{"family":"Tabuchi","given":"Yohei"},{"family":"Ozawa","given":"Tomoya"},{"family":"Itoh","given":"Hideki"},{"family":"Ohno","given":"Seiko"},{"family":"Horie","given":"Minoru"},{"family":"Terada","given":"Tomohiro"},{"family":"Katsura","given":"Toshiya"}],"citation-key":"ueshimaPopulationPharmacokineticsPharmacogenomics2018","container-title":"British Journal of Clinical Pharmacology","DOI":"10.1111/bcp.13561","ISSN":"1365-2125","issue":"6","issued":{"date-parts":[["2018"]]},"language":"en","page":"1301-1312","source":"Wiley Online Library","title":"Population pharmacokinetics and pharmacogenomics of apixaban in Japanese adult patients with atrial fibrillation","type":"article-journal","URL":"https://onlinelibrary.wiley.com/doi/abs/10.1111/bcp.13561","volume":"84"},
  {"id":"upretiEffectExtremesBody2013","abstract":"Aim Apixaban is an oral, direct, factor-Xa inhibitor approved for thromboprophylaxis in patients who have undergone elective hip or knee replacement surgery and for prevention of stroke and systemic embolism in patients with non-valvular atrial fibrillation. This open label, parallel group study investigated effects of extremes of body weight on apixaban pharmacokinetics, pharmacodynamics, safety and tolerability. Method Fifty-four healthy subjects were enrolled [18 each into low (≤50 kg), reference (65–85 kg) and high (≥120 kg) body weight groups]. Following administration of a single oral dose of 10 mg apixaban, plasma and urine samples were collected for determination of apixaban pharmacokinetics and anti-factor Xa activity. Adverse events, vital signs and laboratory assessments were monitored. Results Compared with the reference body weight group, low body weight had approximately 27% [90% confidence interval (CI): 8–51%] and 20% (90% CI: 11–42%) higher apixaban maximum observed plasma concentration (Cmax) and area under the concentration–time curve extrapolated to infinity (AUC(0,∞)), respectively, and high body weight had approximately 31% (90% CI: 18–41%) and 23% (90% CI: 9–35%) lower apixaban Cmax and AUC(0,∞), respectively. Apixaban renal clearance was similar across the weight groups. Plasma anti-factor Xa activity showed a direct, linear relationship with apixaban plasma concentration, regardless of body weight group. Apixaban was well tolerated in this study. Conclusion The modest change in apixaban exposure is unlikely to require dose adjustment for apixaban based on body weight alone. However, caution is warranted in the presence of additional factors (such as severe renal impairment) that could increase apixaban exposure.","accessed":{"date-parts":[["2022",5,18]]},"author":[{"family":"Upreti","given":"Vijay V."},{"family":"Wang","given":"Jessie"},{"family":"Barrett","given":"Yu Chen"},{"family":"Byon","given":"Wonkyung"},{"family":"Boyd","given":"Rebecca A."},{"family":"Pursley","given":"Janice"},{"family":"LaCreta","given":"Frank P."},{"family":"Frost","given":"Charles E."}],"citation-key":"upretiEffectExtremesBody2013","container-title":"British Journal of Clinical Pharmacology","DOI":"10.1111/bcp.12114","ISSN":"1365-2125","issue":"6","issued":{"date-parts":[["2013"]]},"language":"en","page":"908-916","source":"Wiley Online Library","title":"Effect of extremes of body weight on the pharmacokinetics, pharmacodynamics, safety and tolerability of apixaban in healthy subjects","type":"article-journal","URL":"https://onlinelibrary.wiley.com/doi/abs/10.1111/bcp.12114","volume":"76"},
  {"id":"vancalsterCalibrationAchillesHeel2019","abstract":"The assessment of calibration performance of risk prediction models based on regression or more flexible machine learning algorithms receives little attention.","accessed":{"date-parts":[["2024",4,25]]},"author":[{"family":"Van Calster","given":"Ben"},{"family":"McLernon","given":"David J."},{"family":"Smeden","given":"Maarten","non-dropping-particle":"van"},{"family":"Wynants","given":"Laure"},{"family":"Steyerberg","given":"Ewout W."},{"family":"Bossuyt","given":"Patrick"},{"family":"Collins","given":"Gary S."},{"family":"Macaskill","given":"Petra"},{"family":"McLernon","given":"David J."},{"family":"Moons","given":"Karel G. M."},{"family":"Steyerberg","given":"Ewout W."},{"family":"Van Calster","given":"Ben"},{"family":"van Smeden","given":"Maarten"},{"family":"Vickers","given":"Andrew J."},{"literal":"On behalf of Topic Group ‘Evaluating diagnostic tests and prediction models’ of the STRATOS initiative"}],"citation-key":"vancalsterCalibrationAchillesHeel2019","container-title":"BMC Medicine","container-title-short":"BMC Medicine","DOI":"10.1186/s12916-019-1466-7","ISSN":"1741-7015","issue":"1","issued":{"date-parts":[["2019",12,16]]},"page":"230","source":"BioMed Central","title":"Calibration: the Achilles heel of predictive analytics","title-short":"Calibration","type":"article-journal","URL":"https://doi.org/10.1186/s12916-019-1466-7","volume":"17"},
  {"id":"vandenbrouckeStrengtheningReportingObservational2007","abstract":"Much medical research is observational. The reporting of observational studies is often of insufficient quality. Poor reporting hampers the assessment of the strengths and weaknesses of a study and the generalizability of its results. Taking into account empirical evidence and theoretical considerations, a group of methodologists, researchers, and editors developed the Strengthening the Reporting of Observational Studies in Epidemiology (STROBE) recommendations to improve the quality of reporting of observational studies.\n          The STROBE Statement consists of a checklist of 22 items, which relate to the title, abstract, introduction, methods, results and discussion sections of articles. Eighteen items are common to cohort studies, case-control studies and cross-sectional studies and four are specific to each of the three study designs. The STROBE Statement provides guidance to authors about how to improve the reporting of observational studies and facilitates critical appraisal and interpretation of studies by reviewers, journal editors and readers.\n          This explanatory and elaboration document is intended to enhance the use, understanding, and dissemination of the STROBE Statement. The meaning and rationale for each checklist item are presented. For each item, one or several published examples and, where possible, references to relevant empirical studies and methodological literature are provided. Examples of useful flow diagrams are also included. The STROBE Statement, this document, and the associated web site (http://www.strobe-statement.org) should be helpful resources to improve reporting of observational research.","accessed":{"date-parts":[["2025",9,28]]},"author":[{"family":"Vandenbroucke","given":"Jan P."},{"family":"Elm","given":"Erik","non-dropping-particle":"von"},{"family":"Altman","given":"Douglas G."},{"family":"Gøtzsche","given":"Peter C."},{"family":"Mulrow","given":"Cynthia D."},{"family":"Pocock","given":"Stuart J."},{"family":"Poole","given":"Charles"},{"family":"Schlesselman","given":"James J."},{"family":"Egger","given":"Matthias"},{"family":"Initiative","given":"for the STROBE"}],"citation-key":"vandenbrouckeStrengtheningReportingObservational2007","container-title":"Epidemiology","DOI":"10.1097/EDE.0b013e3181577511","ISSN":"1044-3983","issue":"6","issued":{"date-parts":[["2007",11]]},"language":"en-US","page":"805","source":"journals.lww.com","title":"Strengthening the Reporting of Observational Studies in Epidemiology (STROBE): Explanation and Elaboration","title-short":"Strengthening the Reporting of Observational Studies in Epidemiology (STROBE)","type":"article-journal","URL":"https://journals.lww.com/epidem/fulltext/2007/11000/strengthening_the_reporting_of_observational.28.aspx","volume":"18"},
  {"id":"vandenbrouckeStrengtheningReportingObservational2007a","abstract":"Much medical research is observational. The reporting of observational studies is often of insufficient quality. Poor reporting hampers the assessment of the strengths and weaknesses of a study and the generalisability of its results. Taking into account empirical evidence and theoretical considerations, a group of methodologists, researchers, and editors developed the Strengthening the Reporting of Observational Studies in Epidemiology (STROBE) recommendations to improve the quality of reporting of observational studies. The STROBE Statement consists of a checklist of 22 items, which relate to the title, abstract, introduction, methods, results and discussion sections of articles. Eighteen items are common to cohort studies, casecontrol studies and cross-sectional studies and four are specific to each of the three study designs. The STROBE Statement provides guidance to authors about how to improve the reporting of observational studies and facilitates critical appraisal and interpretation of studies by reviewers, journal editors and readers. This explanatory and elaboration document is intended to enhance the use, understanding, and dissemination of the STROBE Statement. The meaning and rationale for each checklist item are presented. For each item, one or several published examples and, where possible, references to relevant empirical studies and methodological literature are provided. Examples of useful flow diagrams are also included. The STROBE Statement, this document, and the associated Web site (http://www. strobe-statement.org/) should be helpful resources to improve reporting of observational research.","author":[{"family":"Vandenbroucke","given":"Jan P"},{"family":"Poole","given":"Charles"},{"family":"Schlesselman","given":"James J"},{"family":"Egger","given":"Matthias"}],"citation-key":"vandenbrouckeStrengtheningReportingObservational2007a","container-title":"PLoS Medicine","issue":"10","issued":{"date-parts":[["2007"]]},"language":"en","source":"Zotero","title":"Strengthening the Reporting of Observational Studies in Epidemiology (STROBE): Explanation and Elaboration","type":"article-journal","volume":"4"},
  {"id":"vandengoorberghHarmClassImbalance2022","abstract":"Methods to correct class imbalance (imbalance between the frequency of outcome events and nonevents) are receiving increasing interest for developing prediction models. We examined the effect of imbalance correction on the performance of logistic regression models.Prediction models were developed using standard and penalized (ridge) logistic regression under 4 methods to address class imbalance: no correction, random undersampling, random oversampling, and SMOTE. Model performance was evaluated in terms of discrimination, calibration, and classification. Using Monte Carlo simulations, we studied the impact of training set size, number of predictors, and the outcome event fraction. A case study on prediction modeling for ovarian cancer diagnosis is presented.The use of random undersampling, random oversampling, or SMOTE yielded poorly calibrated models: the probability to belong to the minority class was strongly overestimated. These methods did not result in higher areas under the ROC curve when compared with models developed without correction for class imbalance. Although imbalance correction improved the balance between sensitivity and specificity, similar results were obtained by shifting the probability threshold instead.Imbalance correction led to models with strong miscalibration without better ability to distinguish between patients with and without the outcome event. The inaccurate probability estimates reduce the clinical utility of the model, because decisions about treatment are ill-informed.Outcome imbalance is not a problem in itself, imbalance correction may even worsen model performance.","accessed":{"date-parts":[["2024",4,25]]},"author":[{"family":"Goorbergh","given":"Ruben","non-dropping-particle":"van den"},{"family":"Smeden","given":"Maarten","non-dropping-particle":"van"},{"family":"Timmerman","given":"Dirk"},{"family":"Van Calster","given":"Ben"}],"citation-key":"vandengoorberghHarmClassImbalance2022","container-title":"Journal of the American Medical Informatics Association","container-title-short":"Journal of the American Medical Informatics Association","DOI":"10.1093/jamia/ocac093","ISSN":"1527-974X","issue":"9","issued":{"date-parts":[["2022",9,1]]},"page":"1525-1534","source":"Silverchair","title":"The harm of class imbalance corrections for risk prediction models: illustration and simulation using logistic regression","title-short":"The harm of class imbalance corrections for risk prediction models","type":"article-journal","URL":"https://doi.org/10.1093/jamia/ocac093","volume":"29"},
  {"id":"vanzwetNewLookValues2023","accessed":{"date-parts":[["2023",12,31]]},"author":[{"family":"Zwet","given":"Erik","non-dropping-particle":"van"},{"family":"Gelman","given":"Andrew"},{"family":"Greenland","given":"Sander"},{"family":"Imbens","given":"Guido"},{"family":"Schwab","given":"Simon"},{"family":"Goodman","given":"Steven N."}],"citation-key":"vanzwetNewLookValues2023","container-title":"NEJM Evidence","DOI":"10.1056/EVIDoa2300003","issue":"1","issued":{"date-parts":[["2023",12,26]]},"page":"EVIDoa2300003","publisher":"Massachusetts Medical Society","source":"evidence.nejm.org (Atypon)","title":"A New Look at P Values for Randomized Clinical Trials","type":"article-journal","URL":"https://evidence.nejm.org/doi/full/10.1056/EVIDoa2300003","volume":"3"},
  {"id":"vanzwetStatisticalPropertiesRCTs2021","abstract":"We abstract the concept of a randomized controlled trial as a triple (β,b,s), where β is the primary efficacy parameter, b the estimate, and s the standard error (s>0). If the parameter β is either a difference of means, a log odds ratio or a log hazard ratio, then it is reasonable to assume that b is unbiased and normally distributed. This then allows us to estimate the joint distribution of the z-value z=b/s and the signal-to-noise ratio SNR=β/s from a sample of pairs (bi,si). We have collected 23 551 such pairs from the Cochrane database. We note that there are many statistical quantities that depend on (β,b,s) only through the pair (z,SNR). We start by determining the estimated distribution of the achieved power. In particular, we estimate the median achieved power to be only 13%. We also consider the exaggeration ratio which is the factor by which the magnitude of β is overestimated. We find that if the estimate is just significant at the 5% level, we would expect it to overestimate the true effect by a factor of 1.7. This exaggeration is sometimes referred to as the winner's curse and it is undoubtedly to a considerable extent responsible for disappointing replication results. For this reason, we believe it is important to shrink the unbiased estimator, and we propose a method for doing so. We show that our shrinkage estimator successfully addresses the exaggeration. As an example, we re-analyze the ANDROMEDA-SHOCK trial.","accessed":{"date-parts":[["2023",5,12]]},"author":[{"family":"Zwet","given":"Erik","non-dropping-particle":"van"},{"family":"Schwab","given":"Simon"},{"family":"Senn","given":"Stephen"}],"citation-key":"vanzwetStatisticalPropertiesRCTs2021","container-title":"Statistics in Medicine","DOI":"10.1002/sim.9173","ISSN":"1097-0258","issue":"27","issued":{"date-parts":[["2021"]]},"language":"en","page":"6107-6117","source":"Wiley Online Library","title":"The statistical properties of RCTs and a proposal for shrinkage","type":"article-journal","URL":"https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.9173","volume":"40"},
  {"id":"vanzwetStatisticalPropertiesRCTs2021a","abstract":"We abstract the concept of a randomized controlled trial as a triple (β,b,s), where β is the primary efficacy parameter, b the estimate, and s the standard error (s>0). If the parameter β is either a difference of means, a log odds ratio or a log hazard ratio, then it is reasonable to assume that b is unbiased and normally distributed. This then allows us to estimate the joint distribution of the z-value z=b/s and the signal-to-noise ratio SNR=β/s from a sample of pairs (bi,si). We have collected 23 551 such pairs from the Cochrane database. We note that there are many statistical quantities that depend on (β,b,s) only through the pair (z,SNR). We start by determining the estimated distribution of the achieved power. In particular, we estimate the median achieved power to be only 13%. We also consider the exaggeration ratio which is the factor by which the magnitude of β is overestimated. We find that if the estimate is just significant at the 5% level, we would expect it to overestimate the true effect by a factor of 1.7. This exaggeration is sometimes referred to as the winner's curse and it is undoubtedly to a considerable extent responsible for disappointing replication results. For this reason, we believe it is important to shrink the unbiased estimator, and we propose a method for doing so. We show that our shrinkage estimator successfully addresses the exaggeration. As an example, we re-analyze the ANDROMEDA-SHOCK trial.","accessed":{"date-parts":[["2025",8,9]]},"author":[{"family":"Zwet","given":"Erik","non-dropping-particle":"van"},{"family":"Schwab","given":"Simon"},{"family":"Senn","given":"Stephen"}],"citation-key":"vanzwetStatisticalPropertiesRCTs2021a","container-title":"Statistics in Medicine","DOI":"10.1002/sim.9173","ISSN":"1097-0258","issue":"27","issued":{"date-parts":[["2021"]]},"language":"en","license":"© 2021 The Authors. Statistics in Medicine published by John Wiley & Sons Ltd.","page":"6107-6117","source":"Wiley Online Library","title":"The statistical properties of RCTs and a proposal for shrinkage","type":"article-journal","URL":"https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.9173","volume":"40"},
  {"id":"vickersAnalysingControlledTrials2001","abstract":"In many randomised trials researchers measure a continuous variable at baseline and again as an outcome assessed at follow up. Baseline measurements are common in trials of chronic conditions where researchers want to see whether a treatment can reduce pre-existing levels of pain, anxiety, hypertension, and the like.\n\nStatistical comparisons in such trials can be made in several ways. Comparison of follow up (post-treatment) scores will give a result such as “at the end of the trial, mean pain scores were 15 mm (95% confidence interval 10 to 20 mm) lower in the treatment group.” Alternatively a change score can be calculated by subtracting the follow up score from the baseline score, leading to a statement such as “pain reductions were 20 mm (16 to 24 mm) greater on treatment than control.” If the average baseline scores are the same in each group the estimated treatment effect will be the same using these two simple approaches. If the treatment is effective the statistical significance of the treatment effect by the two methods will depend on the correlation between baseline and follow up scores. If the correlation is low using the change score will …","accessed":{"date-parts":[["2023",8,4]]},"author":[{"family":"Vickers","given":"Andrew J."},{"family":"Altman","given":"Douglas G."}],"citation-key":"vickersAnalysingControlledTrials2001","container-title":"BMJ","container-title-short":"BMJ","DOI":"10.1136/bmj.323.7321.1123","ISSN":"0959-8138, 1468-5833","issue":"7321","issued":{"date-parts":[["2001",11,10]]},"language":"en","license":"© 2001 BMJ Publishing Group Ltd.","page":"1123-1124","PMID":"11701584","publisher":"British Medical Journal Publishing Group","section":"Education and debate","source":"www.bmj.com","title":"Analysing controlled trials with baseline and follow up measurements","type":"article-journal","URL":"https://www.bmj.com/content/323/7321/1123","volume":"323"},
  {"id":"vuWhyAreReplication2024","abstract":"Many explanations have been offered for why replication rates are low in the social sciences, including selective publication, p-hacking, and treatment effect heterogeneity. This article emphasizes that issues with the most commonly used approach for setting sample sizes in replication studies may also play an important role. Theoretically, I show in a simple model of the publication process that we should expect the replication rate to fall below its nominal target, even when original studies are unbiased. The main mechanism is that the most commonly used approach for setting the replication sample size does not properly account for the fact that original effect sizes are estimated. Specifically, it sets the replication sample size to achieve a nominal power target under the assumption that estimated effect sizes correspond to fixed true effects. However, since there are non-linearities in the replication power function linking original effect sizes to power, ignoring the fact that effect sizes are estimated leads to systematically lower replication rates than intended. Empirically, I find that a parsimonious model accounting only for these issues can fully explain observed replication rates in experimental economics and social science, and two-thirds of the replication gap in psychology. I conclude with practical recommendations for replicators.","accessed":{"date-parts":[["2025",8,8]]},"author":[{"family":"Vu","given":"Patrick"}],"citation-key":"vuWhyAreReplication2024","container-title":"Journal of Econometrics","container-title-short":"Journal of Econometrics","DOI":"10.1016/j.jeconom.2024.105868","ISSN":"0304-4076","issue":"1","issued":{"date-parts":[["2024",10,1]]},"page":"105868","source":"ScienceDirect","title":"Why are replication rates so low?","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S0304407624002136","volume":"245"},
  {"id":"wangDerivationVariousNONMEM2007","abstract":"Various estimation methods and the lack of a systematic derivation of the core objective function implemented in NONMEM for nonlinear mixed effect modeling has caused consistent confusion and inquiry among scientists who routinely use NONMEM for data analysis. This paper provides a detailed derivation of the objective functions for the most commonly used estimation methods in NONMEM, such as the Laplacian method, the first-order conditional estimation method (FOCE) with or without interaction, and the first-order method (FO). In addition, models with homogenous or heterogeneous residual error were used to demonstrate the relationship between the objective functions derived from two different types of approximation, namely Laplacian approximation of log-likelihood and linearized model approximation. The relationship between these estimation methods and those implemented in SAS and Splus is discussed.","accessed":{"date-parts":[["2022",5,28]]},"author":[{"family":"Wang","given":"Yaning"}],"citation-key":"wangDerivationVariousNONMEM2007","container-title":"Journal of Pharmacokinetics and Pharmacodynamics","container-title-short":"J Pharmacokinet Pharmacodyn","DOI":"10.1007/s10928-007-9060-6","ISSN":"1573-8744","issue":"5","issued":{"date-parts":[["2007",10,1]]},"language":"en","page":"575-593","source":"Springer Link","title":"Derivation of various NONMEM estimation methods","type":"article-journal","URL":"https://doi.org/10.1007/s10928-007-9060-6","volume":"34"},
  {"id":"wangQuantileOptimalTreatmentRegimes2018","abstract":"Finding the optimal treatment regime (or a series of sequential treatment regimes) based on individual characteristics has important applications in areas such as precision medicine, government policies, and active labor market interventions. In the current literature, the optimal treatment regime is usually defined as the one that maximizes the average benefit in the potential population. This article studies a general framework for estimating the quantile-optimal treatment regime, which is of importance in many real-world applications. Given a collection of treatment regimes, we consider robust estimation of the quantile-optimal treatment regime, which does not require the analyst to specify an outcome regression model. We propose an alternative formulation of the estimator as a solution of an optimization problem with an estimated nuisance parameter. This novel representation allows us to investigate the asymptotic theory of the estimated optimal treatment regime using empirical process techniques. We derive theory involving a nonstandard convergence rate and a nonnormal limiting distribution. The same nonstandard convergence rate would also occur if the mean optimality criterion is applied, but this has not been studied. Thus, our results fill an important theoretical gap for a general class of policy search methods in the literature. The article investigates both static and dynamic treatment regimes. In addition, doubly robust estimation and alternative optimality criterion such as that based on Gini’s mean difference or weighted quantiles are investigated. Numerical simulations demonstrate the performance of the proposed estimator. A data example from a trial in HIV+ patients is used to illustrate the application. Supplementary materials for this article are available online.","accessed":{"date-parts":[["2022",5,19]]},"author":[{"family":"Wang","given":"Lan"},{"family":"Zhou","given":"Yu"},{"family":"Song","given":"Rui"},{"family":"Sherwood","given":"Ben"}],"citation-key":"wangQuantileOptimalTreatmentRegimes2018","container-title":"Journal of the American Statistical Association","container-title-short":"Journal of the American Statistical Association","DOI":"10.1080/01621459.2017.1330204","ISSN":"0162-1459, 1537-274X","issue":"523","issued":{"date-parts":[["2018",7,3]]},"language":"en","page":"1243-1254","source":"DOI.org (Crossref)","title":"Quantile-Optimal Treatment Regimes","type":"article-journal","URL":"https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1330204","volume":"113"},
  {"id":"xiongDataDrivenSwitchbackExperiments2023","abstract":"We study the design and analysis of switchback experiments conducted on a single aggregate unit. The design problem is to partition the continuous time space into intervals and switch treatments between intervals, in order to minimize the estimation error of the treatment effect. We show that the estimation error depends on four factors: carryover effects, periodicity, serially correlated outcomes, and impacts from simultaneous experiments. We derive a rigorous bias-variance decomposition and show the tradeoffs of the estimation error from these factors. The decomposition provides three new insights in choosing a design: First, balancing the periodicity between treated and control intervals reduces the variance; second, switching less frequently reduces the bias from carryover effects while increasing the variance from correlated outcomes, and vice versa; third, randomizing interval start and end points reduces both bias and variance from simultaneous experiments. Combining these insights, we propose a new empirical Bayes design approach. This approach uses prior data and experiments for designing future experiments. We illustrate this approach using real data from a ride-sharing platform, yielding a design that reduces MSE by 33% compared to the status quo design used on the platform.","accessed":{"date-parts":[["2024",12,17]]},"author":[{"family":"Xiong","given":"Ruoxuan"},{"family":"Chin","given":"Alex"},{"family":"Taylor","given":"Sean J."}],"citation-key":"xiongDataDrivenSwitchbackExperiments2023","DOI":"10.2139/ssrn.4626245","event-place":"Rochester, NY","genre":"SSRN Scholarly Paper","issued":{"date-parts":[["2023",11,7]]},"language":"en","number":"4626245","publisher":"Social Science Research Network","publisher-place":"Rochester, NY","source":"papers.ssrn.com","title":"Data-Driven Switchback Experiments: Theoretical Tradeoffs and Empirical Bayes Designs","title-short":"Data-Driven Switchback Experiments","type":"article","URL":"https://papers.ssrn.com/abstract=4626245"},
  {"id":"xuBayesianNonparametricSurvival2019","abstract":"Allogeneic stem cell transplantation is now part of standard care for acute leukaemia. To reduce toxicity of the pretransplant conditioning regimen, intravenous busulfan is usually used as a preparative regimen for acute leukaemia patients undergoing allogeneic stem cell transplantation. Systemic busulfan exposure, characterized by the area under the plasma concentration versus time curve, AUC, is strongly associated with clinical outcome. An AUC that is too high is associated with severe toxicities, whereas an AUC that is too low carries increased risks of recurrence of disease and failure to engraft. Consequently, an optimal AUC-interval needs to be determined for therapeutic use. To address the possibility that busulfan pharmacokinetics and pharmacodynamics vary significantly with patients’ characteristics, we propose a tailored approach to determine optimal covariate-specific AUC-intervals. To estimate these personalized AUC-intervals, we apply a flexible Bayesian non-parametric regression model based on a dependent Dirichlet process and Gaussian process. Our analyses of a data set of 151 patients identified optimal therapeutic intervals for AUC that varied substantively with age and whether the patient was in complete remission or had active disease at transplant. Extensive simulations to evaluate the dependent Dirichlet process–Gaussian process model in similar settings showed that its performance compares favourably with alternative methods. We provide an R package, DDPGPSurv, that implements the dependent Dirichlet process–Gaussian process model for a broad range of survival regression analyses.","accessed":{"date-parts":[["2022",5,19]]},"author":[{"family":"Xu","given":"Yanxun"},{"family":"Thall","given":"Peter F."},{"family":"Hua","given":"William"},{"family":"Andersson","given":"Borje S."}],"citation-key":"xuBayesianNonparametricSurvival2019","container-title":"Journal of the Royal Statistical Society: Series C (Applied Statistics)","DOI":"10.1111/rssc.12331","ISSN":"1467-9876","issue":"3","issued":{"date-parts":[["2019"]]},"language":"en","page":"809-828","source":"Wiley Online Library","title":"Bayesian non-parametric survival regression for optimizing precision dosing of intravenous busulfan in allogeneic stem cell transplantation","type":"article-journal","URL":"https://onlinelibrary.wiley.com/doi/abs/10.1111/rssc.12331","volume":"68"},
  {"id":"yingDeterminingSampleSize2025","abstract":"<p>Pilot trials have a key role in preparing for definitive randomized trials, yet determining their sample size remains a common challenge. This article provides practical guidance, methods, and tools—including step-by-step examples, sample size tables, and statistical code—for calculating and justifying sample sizes in external randomized pilot trials.</p>","accessed":{"date-parts":[["2025",8,18]]},"author":[{"family":"Ying","given":"Xiangji"},{"family":"Freedland","given":"Kenneth E."},{"family":"Powell","given":"Lynda H."},{"family":"Stuart","given":"Elizabeth A."},{"family":"Ehrhardt","given":"Stephan"},{"family":"Mayo-Wilson","given":"Evan"}],"citation-key":"yingDeterminingSampleSize2025","container-title":"BMJ","container-title-short":"BMJ","DOI":"10.1136/bmj-2024-083405","ISSN":"1756-1833","issued":{"date-parts":[["2025",8,8]]},"language":"en","license":"Published by the BMJ Publishing Group Limited. For permission to use (where not already granted under a licence) please go to http://group.bmj.com/group/rights-licensing/permissions","page":"e083405","PMID":"40780848","publisher":"British Medical Journal Publishing Group","section":"Research Methods &amp; Reporting","source":"www.bmj.com","title":"Determining sample size for pilot trials: a tutorial","title-short":"Determining sample size for pilot trials","type":"article-journal","URL":"https://www.bmj.com/content/390/bmj-2024-083405","volume":"390"},
  {"id":"zhouMethodsComparingMeans1997","abstract":"Standard methods of using the t-test and the Wilcoxon test have deficiencies for comparing the means of two skewed log-normal samples. In this paper, we propose two new methods to overcome these deficiencies: (1) a likelihood-based approach and (2) a bootstrap-based approach. Our simulation study shows that the likelihood-based approach is the best in terms of the type I error rate and power when data follow a log-normal distribution.","accessed":{"date-parts":[["2023",5,19]]},"author":[{"family":"Zhou","given":"Xiao-Hua"},{"family":"Gao","given":"Sujuan"},{"family":"Hui","given":"Siu L."}],"citation-key":"zhouMethodsComparingMeans1997","container-title":"Biometrics","DOI":"10.2307/2533570","ISSN":"0006-341X","issue":"3","issued":{"date-parts":[["1997"]]},"page":"1129-1135","publisher":"[Wiley, International Biometric Society]","source":"JSTOR","title":"Methods for Comparing the Means of Two Independent Log-Normal Samples","type":"article-journal","URL":"https://www.jstor.org/stable/2533570","volume":"53"}
]
