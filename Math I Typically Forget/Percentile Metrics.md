Percentile metrics are useful when metrics are related to web performance (e.g. P99 latency).  Suppose we have $n$ i.i.d. observations, $X_1, \cdots, X_n$ generated by a cumulative distribution function $F(x) = P(X\leq x)$.  The theoretical $p^{th}$ quantile for the distribution $F$ is defined as $F^{-1}(p)$.

If we order the observations in ascending order (i.e. $X_{(1)}, \cdots, X_{(n)}$) then the sample quantile $p$ is $X_{(\lfloor np \rfloor)}$ . For economy of thought, I am going to use $X_{(np)}$ , without flooring, but we should understand that the flooring operator is used to make $np$ an integer.

I omit a proof here, but it can be shown that the sampling distribution of the quantile is asymptotically normal

$$ \sqrt{n} \left( X_{np} - F^{-1}(p) \right) \to N\left( 0, \dfrac{p(1-p)}{f(F^{-1}(p))} \right) $$
Note that the sampling variance depends on the density at the true and unknown quantile. This is a pain in the ass and prevents us from directly estimating the variance like we do in other metrics, but we can get around it in two clever ways.

## Outer Confidence Interval With Post-Adjustment 

Let $Y_i = I(X_i \leq F^{-1}(p))$ , and so then $\sum_i Y_i$ is the count of observations smaller than the quantile, which naturally follows a binomial distribution.  Hence, when $n$ is large, $\bar Y$ is asymptotically normal with sampling variance $p(1-p)/n$.  If the quantile is within $[X_{(r)}, X_{(r+1)})$ then $\bar Y = r/n$.  We can invert this equation to obtain a confidence interval for $r/n$, which is $p \pm z_{1-\alpha/2} s/\sqrt{n}$.  Hence, with 95% probability, the true quantile lies between

$$ L, U = X_{(p \pm z_{1-\alpha/2} \sqrt({p(1-p))}/\sqrt{n})} $$

If the data are clustered (which they typically are), we can use what Deng calls the "Outer CI with Post-Adjustment Algorithm"

- Step 1: Compute i.i.d. Percentile Bounds
    
    Calculate the lower ($L_p$) and upper ($U_p$) probability bounds assuming i.i.d. data using the standard normal critical value $z$. Convert these probabilities into integer ranks $L$ and $U$:
    
    $$L_p = p - z \sqrt{\frac{p(1-p)}{n}}, \quad U_p = p + z \sqrt{\frac{p(1-p)}{n}}$$
    
    $$L = \lfloor n \cdot L_p \rfloor, \quad U = \lceil n \cdot U_p \rceil$$
    
- Step 2: Fetch Quantiles
    
    Sort the data and retrieve three specific values: the point estimate ($X_{np}$), the lower bound ($X_L$), and the upper bound ($X_U$) based on the ranks calculated in Step 1.
    
- Step 3: Compute Indicator Variable
    
    Create a binary indicator column $I$ where $I_i = 1$ if the metric value is less than or equal to the point estimate $X_{np}$, and $0$ otherwise.
    
- Step 4: Compute Delta Method Variance
    
    Group by the cluster column to get sum of indicators $S$ and cluster sizes $N$. Calculate the variance of the ratio estimator ($S/N$) using the Delta method formula:
    
    $$\text{Var}(I) = \frac{1}{K \bar{N}^2} \left( \text{Var}(S) - 2 \frac{\bar{S}}{\bar{N}} \text{Cov}(S,N) + \left(\frac{\bar{S}}{\bar{N}}\right)^2 \text{Var}(N) \right)$$
    
- Step 5: Compute Correction Factor
    
    Calculate the correction factor $c$ as the ratio of the clustered standard deviation (derived from Step 4) to the theoretical i.i.d. standard deviation:
    
    $$c = \sqrt{\frac{\text{Var}(I)}{p(1-p)/n}}$$
    
- Step 6: Compute Standard Error
    
    Estimate the standard error of the quantile by scaling the naive interval width (the distance between $X_U$ and $X_L$) by the correction factor $c$, normalized by the critical value width ($2z$):
    
    $$SE = c \times \frac{X_U - X_L}{2z}$$
    
    (Note: The final Confidence Interval is then constructed as $X_{np} \pm 2 \cdot SE$.)



## Code Example

You can execute this script using `uv run --script main.py`.

```python
# /// script
# requires-python = ">=3.14"
# dependencies = [
#     "numpy",
#     "pandas",
#     "scipy",
#     "tqdm",
# ]
# ///
import numpy as np
import pandas as pd
from scipy.stats import norm
import tqdm

def percentile_delta_method(df, metric_col_name, cluster_col_name, p, alpha=0.05):
    """
    Estimate quantile and its confidence interval using the Delta method with clustering.

    This implements "outer confidence interval with post-adjustment" algorithm
    at the end of section 4.2 in Deng, Knoblich, Lu (2018), or equivalently
    Algorithm 1 in Yao, Li, and Lu (2014).
    
    Parameters:
    df: pd.DataFrame - Fact-level DataFrame with columns for cluster ID and the metric of interest
    metric_col_name: str - Name of the column containing the metric of interest (e.g., 'page_load_time')
    cluster_col_name: str - Name of the column containing the cluster ID (e.g., 'user_id')
    p: float - Desired percentile (between 0 and 1)
    alpha: float - Significance level for confidence interval (default 0.05)
    
    Returns:
    tuple - (percentile estimate, standard error)
    """
    n = len(df)
    z_critical = norm.ppf(1 - alpha/2)
    
    # Step 1: Compute lower and upper percentiles for confidence interval
    L_p = p - z_critical * np.sqrt(p*(1-p)/n)
    U_p = p + z_critical * np.sqrt(p*(1-p)/n)

    L = int(n * L_p) # convert to ranks
    U = int(n * U_p) + 1 # take the ceiling

    # Step 2: Fetch 3 percentiles. Here, exact rank because the data is small. In production,
    #         could be replaced with approximate percentiles (but keeping high precision).
    sorted_values = df[metric_col_name].sort_values()
    X_np = sorted_values.iloc[int(n*p)]
    X_L = sorted_values.iloc[L]
    X_U = sorted_values.iloc[U]
    
    # Step 3: Compute indicator variable (0/1 "is this observation below the target percentile?")
    df['I'] = (df[metric_col_name] <= X_np).astype(int)
    
    # Step 4: Compute variance of I using Delta method
    K = df[cluster_col_name].nunique()
    N = df.groupby(cluster_col_name).size()
    S = df.groupby(cluster_col_name)['I'].sum()
    
    S_bar = S.mean()
    N_bar = N.mean()
    
    var_S = S.var()
    var_N = N.var()
    cov_SN = np.cov(S, N)[0, 1]
    
    var_I = (1 / (K * N_bar**2)) * (var_S - 2*(S_bar/N_bar)*cov_SN + (S_bar/N_bar)**2 * var_N)
    
    # Step 5: Compute correction factor
    c = np.sqrt(var_I / (p * (1 - p) / n))
    
    # Step 6: Compute standard error
    se = c * (X_U - X_L) / (2 * z_critical)
    
    return X_np, se

def between(x:float, l:float, u:float):
    return (x<u)&(l<x)


if __name__ == "__main__":

    nsims = 50_000
    covered = 0
    true_ptile = norm().ppf(0.975)

    for i in tqdm.tqdm(range(nsims)):
        df = pd.DataFrame({
            'user_id' : np.random.choice(np.arange(100), size=1000),
            'outcome': np.random.normal(size=1000)
        })

        X_np, se = percentile_delta_method(df, metric_col_name='outcome', cluster_col_name='user_id', p=0.975, alpha=0.05)

        covered += between(true_ptile, X_np-2*se, X_np+2*se)

    print(f"Coverage: {covered/nsims:.2f}%")
```

# Estimate The Density

From the above equations, we can actually estimate the density at the unknown quantile by setting the sampling variance equal to the asymptotic variance of the quantile

$$ \hat f( F^{-1}(p)) \approx \dfrac{2 z_{1-\alpha/2}}{X_U - X_L} \sqrt{\dfrac{p(1-p)}{n}} $$


# Code Example

```python
# /// script
# requires-python = ">=3.14"
# dependencies = [
#     "numpy",
#     "pandas",
#     "scipy",
#     "tqdm",
# ]
# ///
import numpy as np
import pandas as pd
from scipy.stats import norm
import tqdm

def percentile_w_density(df, metric_col_name, cluster_col_name, p, alpha=0.05):
    """
    Estimate quantile and its confidence interval using the Delta method with clustering.

    This implements "outer confidence interval with post-adjustment" algorithm
    at the end of section 4.2 in Deng, Knoblich, Lu (2018), or equivalently
    Algorithm 1 in Yao, Li, and Lu (2014).
    
    Parameters:
    df: pd.DataFrame - Fact-level DataFrame with columns for cluster ID and the metric of interest
    metric_col_name: str - Name of the column containing the metric of interest (e.g., 'page_load_time')
    cluster_col_name: str - Name of the column containing the cluster ID (e.g., 'user_id')
    p: float - Desired percentile (between 0 and 1)
    alpha: float - Significance level for confidence interval (default 0.05)
    
    Returns:
    tuple - (percentile estimate, standard error)
    """
    n = len(df)
    z_critical = norm.ppf(1 - alpha/2)
    
    # Step 1: Compute lower and upper percentiles for confidence interval
    L_p = p - z_critical * np.sqrt(p*(1-p)/n)
    U_p = p + z_critical * np.sqrt(p*(1-p)/n)

    L = int(n * L_p) # convert to ranks
    U = int(n * U_p) + 1 # take the ceiling

    # Step 2: Fetch 3 percentiles. Here, exact rank because the data is small. In production,
    #         could be replaced with approximate percentiles (but keeping high precision).
    sorted_values = df[metric_col_name].sort_values()
    X_np = sorted_values.iloc[int(n*p)]
    X_L = sorted_values.iloc[L]
    X_U = sorted_values.iloc[U]
    
    # Step 3: Compute indicator variable (0/1 "is this observation below the target percentile?")
    df['I'] = (df[metric_col_name] <= X_np).astype(int)
    
    # Step 4: Compute variance of I using Delta method
    K = df[cluster_col_name].nunique()
    N = df.groupby(cluster_col_name).size()
    S = df.groupby(cluster_col_name)['I'].sum()
    
    S_bar = S.mean()
    N_bar = N.mean()
    
    var_S = S.var()
    var_N = N.var()
    cov_SN = np.cov(S, N)[0, 1]
    
    var_I = (1 / (K * N_bar**2)) * (var_S - 2*(S_bar/N_bar)*cov_SN + (S_bar/N_bar)**2 * var_N)

    # Skip the correction factor, just estimate the density directly using X_U and X_L
    # Step 6: Compute standard error
    density = 2*z_critical / (X_U - X_L) * np.sqrt(p*(1-p)/n)
    se = np.sqrt(var_I)/ (density)
    
    return X_np, se

def between(x:float, l, u):
    return (x<u)&(l<x)


if __name__ == "__main__":

    nsims = 50_000
    covered = 0
    true_ptile = norm().ppf(0.975)

    for i in tqdm.tqdm(range(nsims)):
        df = pd.DataFrame({
            'user_id' : np.random.choice(np.arange(100), size=1000),
            'outcome': np.random.normal(size=1000)
        })

        X_np, se = percentile_delta_method(df, metric_col_name='outcome', cluster_col_name='user_id', p=0.975, alpha=0.05)

        covered += between(true_ptile, X_np-2*se, X_np+2*se)

    print(f"Coverage: {covered/nsims:.2f}%")





```